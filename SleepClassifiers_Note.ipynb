{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SleepClassifiers (Documented)",
      "provenance": [],
      "collapsed_sections": [
        "v5nelR4ZFRWq",
        "-O2FJinEFRWq",
        "OR6aqzcJFRWr",
        "LnuvxyiUFRWx",
        "tPnAwxZJFRWy",
        "MaT7QGRBFRWy",
        "6i1-yT6rFRWr",
        "mbjpl6t9FRWs",
        "4dNx65U-FRWt",
        "4hOWh8kLFRWt",
        "NwLNCamhFRWu",
        "wiyyjjPsFRWu",
        "AvMaKW7EFRWx",
        "dLRmQBKAFRWy",
        "YgLfan4-FRW1",
        "rycpu2CkXke1",
        "BqFywwIiFRWy",
        "iYkV1wNrFRWz",
        "IHUn6UJwFRW1",
        "oOmTvaCmvq93",
        "DwcR3G7Ivq94",
        "NMHTwtMpFRW3",
        "CM7Zwy0KIHmh",
        "sg0MmASOFRW3",
        "HXhaWzwQFRW3",
        "YCLB-DdZFRW4",
        "q6xBonnYvq97",
        "KbmfuP47vq97",
        "QA3ekcr8vq98",
        "92eO90a6FRW4",
        "_e49Fkigvq98",
        "QS6XwRmVvq9_",
        "2BG_Jv_Hvq-A",
        "4_0npCiYvq-A",
        "UUCZ_2XPvq-A",
        "YpRXmZzMvq-B",
        "RaXXwHSFvq-B",
        "gm_7u7aQvq-B",
        "4QwnKIAJvq-B",
        "5DlqONBFvq-C",
        "WWN1Yk6nvq-C"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "4133d8d2d3b4b400abacbc5304f0ad3d3c22bd6024d94ca1181c75ab31b71175"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit (conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nothing2say/Sleep-2-Learn/blob/main/SleepClassifiers_Note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wm-rZXtFRWm"
      },
      "source": [
        "# Sleep Stage Classification (Walch et al.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5FD3PILNJWR"
      },
      "source": [
        "Jupyter Notebwook version of \n",
        "**Sleep Classifiers** with Documentation\n",
        "\n",
        "--- \n",
        "Original sourcecode:https://github.com/ojwalch/sleep_classifiers\n",
        "\n",
        "(the datasets used in this code were described in GitHub repo)\n",
        "\n",
        "Paper: https://academic.oup.com/sleep/article/42/12/zsz180/5549536 \n",
        "\n",
        "---\n",
        "Rebuilt & Documented by Nothing2saY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO7BMP3iBQza"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5nelR4ZFRWq"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O2FJinEFRWq"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8M5_dpFRWq",
        "outputId": "0c077ce3-a645-4cb8-cbc2-f5829c0a3672"
      },
      "source": [
        "### Requirements ###\n",
        "\"\"\"\n",
        "! pip install 'cycler == 0.10.0'\n",
        "! pip install 'docx2txt == 0.8'\n",
        "! pip install 'joblib == 0.14.1'\n",
        "! pip install 'kiwisolver == 1.2.0'\n",
        "! pip install 'matplotlib == 3.1.1'\n",
        "! pip install 'numpy == 1.19.2'\n",
        "! pip install 'pandas == 1.1.0'\n",
        "! pip install 'pdfminer == 20191125'\n",
        "! pip install 'Pillow == 8.1.1'\n",
        "! pip install 'pycryptodome == 3.9.7'\n",
        "! pip install 'pyEDFlib == 0.1.17'\n",
        "! pip install 'python-dateutil == 2.8.1'\n",
        "! pip install 'pytz == 2020.1'\n",
        "! pip install 'scikit-learn == 0.22.2.post1'\n",
        "! pip install 'scipy == 1.4.1'\n",
        "! pip install 'seaborn == 0.10.1'\n",
        "! pip install 'six == 1.15.0'\n",
        "! pip install 'sklearn == 0.0'\n",
        "! pip install simplefilter\n",
        "! pip install IPython\n",
        "\"\"\"\n",
        "#import sklearn\n",
        "#print('sklearn version: %s' % sklearn.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n! pip install 'cycler == 0.10.0'\\n! pip install 'docx2txt == 0.8'\\n! pip install 'joblib == 0.14.1'\\n! pip install 'kiwisolver == 1.2.0'\\n! pip install 'matplotlib == 3.1.1'\\n! pip install 'numpy == 1.19.2'\\n! pip install 'pandas == 1.1.0'\\n! pip install 'pdfminer == 20191125'\\n! pip install 'Pillow == 8.1.1'\\n! pip install 'pycryptodome == 3.9.7'\\n! pip install 'pyEDFlib == 0.1.17'\\n! pip install 'python-dateutil == 2.8.1'\\n! pip install 'pytz == 2020.1'\\n! pip install 'scikit-learn == 0.22.2.post1'\\n! pip install 'scipy == 1.4.1'\\n! pip install 'seaborn == 0.10.1'\\n! pip install 'six == 1.15.0'\\n! pip install 'sklearn == 0.0'\\n! pip install simplefilter\\n! pip install IPython\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR6aqzcJFRWr"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR0efODdFRWr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import docx2txt as docx2txt\n",
        "import datetime as dt\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import pyedflib as pyedflib\n",
        "\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "from enum import Enum\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib import font_manager\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from functools import partial\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from numpy.core.multiarray import ndarray\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from xml.dom import minidom\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc, cohen_kappa_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from warnings import simplefilter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuvxyiUFRWx"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZfmk_z6FRWx"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPnAwxZJFRWy"
      },
      "source": [
        "#### Data on Google Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFI_g4-uIqHH"
      },
      "source": [
        "# Uncomment when working on Colab\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaT7QGRBFRWy"
      },
      "source": [
        "#### Data on Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ5M5Tv6FRWy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuzhRy_UFRWr"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i1-yT6rFRWr"
      },
      "source": [
        "#### Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-bG_9B1FRWr"
      },
      "source": [
        "class FeatureType(Enum):\n",
        "    count = \"count\"\n",
        "    motion = \"motion\"\n",
        "    heart_rate = \"heart rate\"\n",
        "    cosine = \"cosine\"\n",
        "    circadian_model = \"circadian model\"\n",
        "    time = \"time\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focEYOw9FRWs"
      },
      "source": [
        "class FeatureSetService(object):\n",
        "\n",
        "    def get_label(feature_set: [FeatureType]):\n",
        "        if set(feature_set) == {FeatureType.count}:\n",
        "            return 'Motion only'\n",
        "        if set(feature_set) == {FeatureType.heart_rate}:\n",
        "            return 'HR only'\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate}:\n",
        "            return 'Motion, HR'\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.circadian_model}:\n",
        "            return 'Motion, HR, and Clock'\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.cosine}:\n",
        "            return 'Motion, HR, and Cosine'\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.time}:\n",
        "            return 'Motion, HR, and Time'\n",
        "\n",
        "    def get_color(feature_set: [FeatureType]):\n",
        "        if set(feature_set) == {FeatureType.count}:\n",
        "            return sns.xkcd_rgb[\"denim blue\"]\n",
        "        if set(feature_set) == {FeatureType.heart_rate}:\n",
        "            return sns.xkcd_rgb[\"yellow orange\"]\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate}:\n",
        "            return sns.xkcd_rgb[\"medium green\"]\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.circadian_model}:\n",
        "            return sns.xkcd_rgb[\"medium pink\"]\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.cosine}:\n",
        "            return sns.xkcd_rgb[\"plum\"]\n",
        "        if set(feature_set) == {FeatureType.count, FeatureType.heart_rate, FeatureType.time}:\n",
        "            return sns.xkcd_rgb[\"greyish\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbjpl6t9FRWs"
      },
      "source": [
        "#### Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfqXQ9i8FRWs"
      },
      "source": [
        "# Sleep Staging Labels\n",
        "class SleepStage(Enum):\n",
        "    wake = 0\n",
        "    n1 = 1\n",
        "    n2 = 2\n",
        "    n3 = 3\n",
        "    n4 = 4\n",
        "    rem = 5\n",
        "    unscored = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYZKwsQuFRWs"
      },
      "source": [
        "class Subject(object):\n",
        "\n",
        "    def __init__(self, subject_id, labeled_sleep, feature_dictionary):\n",
        "        self.subject_id = subject_id\n",
        "        self.labeled_sleep = labeled_sleep\n",
        "        self.feature_dictionary = feature_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72NWOu3WFRWs"
      },
      "source": [
        "class SleepWakeLabel(Enum):\n",
        "    wake = 0\n",
        "    sleep = 1\n",
        "\n",
        "class ThreeClassLabel(Enum):\n",
        "    wake = 0\n",
        "    nrem = 1\n",
        "    rem = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHQQas_1FRWs"
      },
      "source": [
        "class RawPerformance(object):\n",
        "    def __init__(self, true_labels, class_probabilities):\n",
        "        self.true_labels = true_labels\n",
        "        self.class_probabilities = class_probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G9QzcHtFRWt"
      },
      "source": [
        "# Set labels for training model\n",
        "\n",
        "class SleepLabeler(object):\n",
        "\n",
        "    def label_sleep_wake(raw_sleep_wake):\n",
        "        labeled_sleep = []\n",
        "\n",
        "        for value in raw_sleep_wake:\n",
        "            if value > 0:\n",
        "                converted_value = SleepWakeLabel.sleep.value\n",
        "            else:\n",
        "                converted_value = SleepWakeLabel.wake.value\n",
        "            labeled_sleep.append(converted_value)\n",
        "\n",
        "        return np.array(labeled_sleep)\n",
        "\n",
        "    def label_three_class(raw_sleep_wake):\n",
        "        labeled_sleep = []\n",
        "\n",
        "        for value in raw_sleep_wake:\n",
        "            if value == 0:\n",
        "                converted_value = ThreeClassLabel.wake.value\n",
        "            elif value == 5:\n",
        "                converted_value = ThreeClassLabel.rem.value\n",
        "            else:\n",
        "                converted_value = ThreeClassLabel.nrem.value\n",
        "\n",
        "            labeled_sleep.append(converted_value)\n",
        "\n",
        "        return np.array(labeled_sleep)\n",
        "\n",
        "    def label_one_vs_rest(sleep_wake_labels, positive_class):\n",
        "        labeled_sleep = []\n",
        "\n",
        "        for value in sleep_wake_labels:\n",
        "            if value == positive_class:\n",
        "                converted_value = 1\n",
        "            else:\n",
        "                converted_value = 0\n",
        "\n",
        "            labeled_sleep.append(converted_value)\n",
        "\n",
        "        return np.array(labeled_sleep)\n",
        "\n",
        "    def convert_three_class_to_two(raw_performance: RawPerformance):\n",
        "        raw_performance.true_labels = SleepLabeler.label_sleep_wake(raw_performance.true_labels)\n",
        "        number_of_samples = np.shape(raw_performance.class_probabilities)[0]\n",
        "        for index in range(number_of_samples):\n",
        "            raw_performance.class_probabilities[index, 1] = raw_performance.class_probabilities[index, 1] + \\\n",
        "                                                            raw_performance.class_probabilities[index, 2]\n",
        "        raw_performance.class_probabilities = raw_performance.class_probabilities[:, :-1]\n",
        "\n",
        "        return raw_performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dNx65U-FRWt"
      },
      "source": [
        "#### Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf9TwfC8FRWt"
      },
      "source": [
        "class Epoch(object):\n",
        "    DURATION = 30  # seconds\n",
        "\n",
        "    def __init__(self, timestamp, index):\n",
        "        self.timestamp = timestamp\n",
        "        self.index = index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBelgk3Svq9r"
      },
      "source": [
        "class Interval(object):\n",
        "    def __init__(self, start_time, end_time):\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hOWh8kLFRWt"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHFEmRLeFRWu"
      },
      "source": [
        "class AttributedClassifier(object):\n",
        "    def __init__(self, name, classifier):\n",
        "        self.name = name\n",
        "        self.classifier = classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5z7TfsBFRWt"
      },
      "source": [
        "class DataSplit(object):\n",
        "    def __init__(self, training_set, testing_set):\n",
        "        self.training_set = training_set\n",
        "        self.testing_set = testing_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCYDtKvvq9s"
      },
      "source": [
        "class TrainTestSplitter(object):\n",
        "\n",
        "    def leave_one_out(subject_ids):\n",
        "        splits = []\n",
        "\n",
        "        for index in range(len(subject_ids)):\n",
        "            training_set = subject_ids.copy()\n",
        "            testing_set = [training_set.pop(index)]\n",
        "\n",
        "            splits.append(DataSplit(training_set=training_set,\n",
        "                          testing_set=testing_set))\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def by_fraction(subject_ids, test_fraction, number_of_splits):\n",
        "\n",
        "        test_index = int(np.round(test_fraction * len(subject_ids)))\n",
        "\n",
        "        splits = []\n",
        "        for trial in range(number_of_splits):\n",
        "            random.shuffle(subject_ids)\n",
        "\n",
        "            training_set = subject_ids.copy()\n",
        "            testing_set = []\n",
        "            for index in range(test_index):\n",
        "                testing_set.append(training_set.pop(0))\n",
        "\n",
        "            splits.append(DataSplit(training_set=training_set,\n",
        "                          testing_set=testing_set))\n",
        "\n",
        "        return splits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwLNCamhFRWu"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvVWWzWbFRWu"
      },
      "source": [
        "class ClassifierSummary(object):\n",
        "    def __init__(self, attributed_classifier: AttributedClassifier, performance_dictionary):\n",
        "        self.attributed_classifier = attributed_classifier\n",
        "        self.performance_dictionary = performance_dictionary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVnT5gNFRWu"
      },
      "source": [
        "class SleepMetrics(object):\n",
        "    def __init__(self, tst, sol, waso, sleep_efficiency, time_in_rem, time_in_nrem):\n",
        "        self.tst = tst\n",
        "        self.sol = sol\n",
        "        self.waso = waso\n",
        "        self.sleep_efficiency = sleep_efficiency\n",
        "        self.time_in_rem = time_in_rem\n",
        "        self.time_in_nrem = time_in_nrem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAliMbFQFRWu"
      },
      "source": [
        "class RawPerformance(object):\n",
        "    def __init__(self, true_labels, class_probabilities):\n",
        "        self.true_labels = true_labels\n",
        "        self.class_probabilities = class_probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFrjAv0ZFRWu"
      },
      "source": [
        "class SleepWakePerformance(object):\n",
        "    def __init__(self, accuracy, wake_correct, sleep_correct, kappa, auc, sleep_predictive_value,\n",
        "                 wake_predictive_value):\n",
        "        self.accuracy = accuracy\n",
        "        self.wake_correct = wake_correct\n",
        "        self.sleep_correct = sleep_correct\n",
        "        self.kappa = kappa\n",
        "        self.auc = auc\n",
        "        self.wake_predictive_value = wake_predictive_value\n",
        "        self.sleep_predictive_value = sleep_predictive_value\n",
        "\n",
        "class ThreeClassPerformance(object):\n",
        "    def __init__(self, accuracy, wake_correct, rem_correct, nrem_correct, kappa):\n",
        "        self.accuracy = accuracy\n",
        "        self.wake_correct = wake_correct\n",
        "        self.rem_correct = rem_correct\n",
        "        self.nrem_correct = nrem_correct\n",
        "        self.kappa = kappa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFmTa-WTvq9t"
      },
      "source": [
        "class ROCPerformance(object):\n",
        "\n",
        "    def __init__(self, false_positive_rates: ndarray, true_positive_rates: ndarray):\n",
        "        self.false_positive_rates = false_positive_rates\n",
        "        self.true_positive_rates = true_positive_rates\n",
        "\n",
        "class PrecisionRecallPerformance(object):\n",
        "\n",
        "    def __init__(self, recalls: ndarray, precisions: ndarray):\n",
        "        self.recalls = recalls\n",
        "        self.precisions = precisions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiyyjjPsFRWu"
      },
      "source": [
        "## Utilities "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CAWT5tTvq9t"
      },
      "source": [
        "# --- Script to ignore annoying warnings ---\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyT-XGYFFRWu"
      },
      "source": [
        "class Utils(object):\n",
        "    \n",
        "    def get_project_root() -> Path:\n",
        "        #return os.getcwd()  # return path string\n",
        "        return Path(\"__file__\").parent.parent\n",
        "    \n",
        "    def get_classifiers():\n",
        "        return [AttributedClassifier(name='Logistic Regression',\n",
        "                                     classifier=LogisticRegression(penalty='l1',\n",
        "                                                                   solver='liblinear',\n",
        "                                                                   verbose=0,\n",
        "                                                                   multi_class='auto')), \n",
        "                AttributedClassifier(name='Random Forest',\n",
        "                                    classifier=RandomForestClassifier(n_estimators=100,\n",
        "                                                                    max_features=1.0,\n",
        "                                                                    max_depth=10,\n",
        "                                                                    min_samples_split=10,\n",
        "                                                                    min_samples_leaf=32,\n",
        "                                                                    bootstrap=True)),\n",
        "                AttributedClassifier(name='k-Nearest Neighbors',\n",
        "                                    classifier=KNeighborsClassifier(weights='distance')),\n",
        "                AttributedClassifier(name='Neural Net',\n",
        "                                    classifier=MLPClassifier(activation='relu',\n",
        "                                                            hidden_layer_sizes=(\n",
        "                                                                15, 15, 15),\n",
        "                                                            max_iter=2000,\n",
        "                                                            alpha=0.01,\n",
        "                                                            solver='adam',\n",
        "                                                            verbose=False,\n",
        "                                                            n_iter_no_change=20))]\n",
        "\n",
        "    def get_base_feature_sets():\n",
        "        return [[FeatureType.count],\n",
        "                [FeatureType.heart_rate],\n",
        "                [FeatureType.count, FeatureType.heart_rate],\n",
        "                [FeatureType.count, FeatureType.heart_rate, FeatureType.cosine]]\n",
        "\n",
        "    def convert_pdf_to_txt(pdf_path_string, all_texts):\n",
        "        resource_manager = PDFResourceManager()\n",
        "        returned_string = StringIO()\n",
        "        codec = 'utf-8'\n",
        "        layout_parameters = LAParams(all_texts=all_texts)\n",
        "        device = TextConverter(resource_manager, returned_string,\n",
        "                            codec=codec, laparams=layout_parameters)\n",
        "        fp = open(pdf_path_string, 'rb')\n",
        "        interpreter = PDFPageInterpreter(resource_manager, device)\n",
        "        password = \"\"\n",
        "        max_pages = 0\n",
        "        caching = True\n",
        "        page_numbers = set()\n",
        "\n",
        "        for page in PDFPage.get_pages(fp, page_numbers, maxpages=max_pages, password=password, caching=caching,\n",
        "                                    check_extractable=True):\n",
        "            interpreter.process_page(page)\n",
        "\n",
        "        text = returned_string.getvalue()\n",
        "\n",
        "        fp.close()\n",
        "        device.close()\n",
        "        returned_string.close()\n",
        "        return text\n",
        "    \n",
        "    def smooth_gauss(y, box_pts):\n",
        "        box = np.ones(box_pts) / box_pts\n",
        "        mu = int(box_pts / 2.0)\n",
        "        sigma = 50  # seconds\n",
        "\n",
        "        for ind in range(0, box_pts):\n",
        "            box[ind] = np.exp(-1 / 2 * (((ind - mu) / sigma) ** 2))\n",
        "\n",
        "        box = box / np.sum(box)\n",
        "        sum_value = 0\n",
        "        for ind in range(0, box_pts):\n",
        "            sum_value += box[ind] * y[ind]\n",
        "\n",
        "        return sum_value\n",
        "    \n",
        "    def convolve_with_dog(y, box_pts):\n",
        "        y = y - np.mean(y)\n",
        "        box = np.ones(box_pts) / box_pts\n",
        "\n",
        "        mu1 = int(box_pts / 2.0)\n",
        "        sigma1 = 120\n",
        "\n",
        "        mu2 = int(box_pts / 2.0)\n",
        "        sigma2 = 600\n",
        "\n",
        "        scalar = 0.75\n",
        "\n",
        "        for ind in range(0, box_pts):\n",
        "            box[ind] = np.exp(-1 / 2 * (((ind - mu1) / sigma1) ** 2)) - scalar * np.exp(\n",
        "                -1 / 2 * (((ind - mu2) / sigma2) ** 2))\n",
        "\n",
        "        # Pad by repeating boundary conditions\n",
        "        y = np.insert(y, 0, np.flip(y[0:int(box_pts / 2)]))\n",
        "        y = np.insert(y, len(y) - 1, np.flip(y[int(-box_pts / 2):]))\n",
        "        y_smooth = np.convolve(y, box, mode='valid')\n",
        "\n",
        "        return y_smooth\n",
        "        \n",
        "    # To clean duplicate data by removing\n",
        "    def remove_repeats(array):\n",
        "        array_no_repeats = np.unique(array, axis=0)\n",
        "        array_no_repeats = array_no_repeats[np.argsort(array_no_repeats[:, 0])]\n",
        "        return array_no_repeats\n",
        "    \n",
        "    # To clean missing data by removing\n",
        "    def remove_nans(array):\n",
        "        array = array[~np.isnan(array).any(axis=1)]\n",
        "        array = array[~np.isinf(array).any(axis=1)]\n",
        "        return array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JH5U1ieFRWw"
      },
      "source": [
        "class DataPlotBuilder(object):\n",
        "    \n",
        "    def timestamp_to_string(ts):\n",
        "        return time.strftime('%H:%M:%S', time.localtime(ts))\n",
        "\n",
        "    def convert_labels_for_hypnogram(labels):\n",
        "        processed_labels = np.array([])\n",
        "\n",
        "        for epoch in labels:\n",
        "            if epoch == -1:\n",
        "                processed_labels = np.append(processed_labels, 0)\n",
        "            elif epoch == 5:\n",
        "                processed_labels = np.append(processed_labels, 1)\n",
        "            else:\n",
        "                processed_labels = np.append(processed_labels, -1 * epoch)\n",
        "\n",
        "        return processed_labels\n",
        "\n",
        "    def tidy_data_plot(x_min, x_max, dt, ax):\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(True)\n",
        "        ax.spines['left'].set_visible(True)\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "        xticks = np.arange(x_min, x_max, dt)\n",
        "        plt.xticks(xticks)\n",
        "        labels = []\n",
        "        for xt in xticks:\n",
        "            labels.append(DataPlotBuilder.timestamp_to_string(xt))\n",
        "        ax.set_xticklabels(labels)\n",
        "        plt.xlim(x_min, x_max)\n",
        "\n",
        "    def make_data_demo(subject_id=\"16\", snippet=False):\n",
        "        hr_color = [0.8, 0.2, 0.1]\n",
        "        motion_color = [0.3, 0.2, 0.8]\n",
        "        circ_color = [0.9, 0.7, 0]\n",
        "        psg_color = [0.1, 0.7, 0.1]\n",
        "        font_size = 16\n",
        "        font_name = \"Arial\"\n",
        "\n",
        "        data_path = str(Constants.CROPPED_FILE_PATH) + '/'\n",
        "        circadian_data_path = str(Utils.get_project_root().joinpath('data/circadian_predictions/')) + '/'\n",
        "        output_path = str(Constants.FIGURE_FILE_PATH) + '/'\n",
        "\n",
        "        if snippet is False:\n",
        "            fig = plt.figure(figsize=(10, 12))\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(3, 12))\n",
        "\n",
        "        num_v_plots = 5\n",
        "        fig.patch.set_facecolor('white')\n",
        "\n",
        "        if (os.path.isfile(data_path + subject_id + '_cleaned_hr.out') and os.path.isfile(\n",
        "                data_path + subject_id + '_cleaned_motion.out') and os.path.isfile(\n",
        "            data_path + subject_id + '_cleaned_psg.out') and\n",
        "            os.path.isfile(data_path + subject_id + '_cleaned_counts.out') and\n",
        "            os.stat(data_path + subject_id + '_cleaned_motion.out').st_size > 0) and os.path.isfile(\n",
        "            circadian_data_path + subject_id + '_clock_proxy.txt'):\n",
        "\n",
        "            hr = np.genfromtxt(data_path + subject_id + '_cleaned_hr.out', delimiter=' ')\n",
        "            motion = np.genfromtxt(data_path + subject_id + '_cleaned_motion.out', delimiter=' ')\n",
        "            scores = np.genfromtxt(data_path + subject_id + '_cleaned_psg.out', delimiter=' ')\n",
        "            counts = np.genfromtxt(data_path + subject_id + '_cleaned_counts.out', delimiter=',')\n",
        "            circ_model = np.genfromtxt(circadian_data_path + subject_id + '_clock_proxy.txt', delimiter=',')\n",
        "\n",
        "            min_time = min(scores[:, 0])\n",
        "            max_time = max(scores[:, 0])\n",
        "            dt = 60 * 60\n",
        "\n",
        "            sample_point_fraction = 0.92\n",
        "\n",
        "            sample_point = sample_point_fraction * (max_time - min_time) + min_time\n",
        "            window_size = 10\n",
        "            if snippet:\n",
        "                min_time = sample_point\n",
        "                max_time = sample_point + window_size\n",
        "\n",
        "            ax = plt.subplot(num_v_plots, 1, 1)\n",
        "            ax.plot(motion[:, 0], motion[:, 1], color=motion_color)\n",
        "            ax.plot(motion[:, 0], motion[:, 2], color=[0.4, 0.2, 0.7])\n",
        "            ax.plot(motion[:, 0], motion[:, 3], color=[0.5, 0.2, 0.6])\n",
        "            plt.ylabel('Motion (g)', fontsize=font_size, fontname=font_name)\n",
        "            DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "\n",
        "            if snippet:\n",
        "                ax.spines['bottom'].set_visible(True)\n",
        "                ax.spines['left'].set_visible(True)\n",
        "                ax.spines['top'].set_visible(True)\n",
        "                ax.spines['right'].set_visible(True)\n",
        "\n",
        "                ax.yaxis.label.set_visible(False)\n",
        "\n",
        "                inds = np.intersect1d(np.where(motion[:, 0] > sample_point)[0],\n",
        "                                      np.where(motion[:, 0] <= sample_point + window_size)[0])\n",
        "                y_min = np.amin(motion[inds, 1:3])\n",
        "                plt.ylim(y_min - 0.005, y_min + 0.025)\n",
        "\n",
        "                # Get rid of the ticks\n",
        "                ax.set_xticks([])\n",
        "                ax.yaxis.set_ticks_position(\"right\")\n",
        "\n",
        "                plt.ylabel('')\n",
        "                plt.xlabel(str(window_size) + ' sec window', fontsize=font_size, fontname=font_name)\n",
        "            else:\n",
        "                y_min = -3.2\n",
        "                y_max = 2.5\n",
        "                plt.ylim(y_min, y_max)\n",
        "                current_axis = plt.gca()\n",
        "                current_axis.add_patch(\n",
        "                    Rectangle((sample_point, y_min), window_size, y_max - y_min, alpha=0.7, facecolor=\"gray\"))\n",
        "\n",
        "            ax = plt.subplot(num_v_plots, 1, 2)\n",
        "            ax.plot(counts[:, 0], counts[:, 1], color=[0.2, 0.2, 0.7])\n",
        "            DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "            plt.ylabel('Counts', fontsize=font_size, fontname=font_name)\n",
        "\n",
        "            if snippet:\n",
        "                plt.axis('off')\n",
        "                plt.ylim(-1, -1)\n",
        "\n",
        "            ax = plt.subplot(num_v_plots, 1, 3)\n",
        "            ax.plot(hr[:, 0], hr[:, 1], color=hr_color)\n",
        "            plt.ylabel('Heart rate (bpm)', fontsize=font_size, fontname=font_name)\n",
        "            DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "\n",
        "            sample_point = sample_point_fraction * (max_time - min_time) + min_time\n",
        "            window_size = 1200\n",
        "\n",
        "            if snippet:\n",
        "                min_time = sample_point\n",
        "                max_time = sample_point + window_size\n",
        "                DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "\n",
        "                ax.spines['bottom'].set_visible(True)\n",
        "                ax.spines['left'].set_visible(True)\n",
        "                ax.spines['top'].set_visible(True)\n",
        "                ax.spines['right'].set_visible(True)\n",
        "\n",
        "                ax.yaxis.label.set_visible(False)\n",
        "\n",
        "                ax.set_xticks([])\n",
        "                ax.yaxis.set_ticks_position(\"right\")\n",
        "\n",
        "                plt.ylabel('')\n",
        "                plt.xlabel(str(window_size) + ' sec window', fontsize=font_size, fontname=font_name)\n",
        "                plt.ylim(35, 100)\n",
        "\n",
        "            else:\n",
        "                y_min = 40\n",
        "                y_max = 130\n",
        "                plt.ylim(y_min, y_max)\n",
        "                current_axis = plt.gca()\n",
        "                current_axis.add_patch(\n",
        "                    Rectangle((sample_point, y_min), window_size, y_max - y_min, alpha=0.35, facecolor=\"gray\"))\n",
        "                plt.ylim(40, 130)\n",
        "\n",
        "            ax = plt.subplot(num_v_plots, 1, 4)\n",
        "            ax.plot(circ_model[:, 0], -circ_model[:, 1], color=circ_color)\n",
        "            plt.ylabel('Clock Proxy', fontsize=font_size, fontname=font_name)\n",
        "            DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "            if snippet:\n",
        "                plt.axis('off')\n",
        "                plt.ylim(-1, -1)\n",
        "            else:\n",
        "                plt.ylim(.2, 1.2)\n",
        "\n",
        "            ax = plt.subplot(num_v_plots, 1, 5)\n",
        "\n",
        "            relabeled_scores = DataPlotBuilder.convert_labels_for_hypnogram(scores[:, 1])\n",
        "            ax.step(scores[:, 0], relabeled_scores, color=psg_color)\n",
        "            plt.ylabel('Stage', fontsize=font_size, fontname=font_name)\n",
        "            plt.xlabel('Time', fontsize=font_size, fontname=font_name)\n",
        "            DataPlotBuilder.tidy_data_plot(min_time, max_time, dt, ax)\n",
        "            ax.set_yticks([-4, -3, -2, -1, 0, 1])\n",
        "            ax.set_yticklabels(['N4', 'N3', 'N2', 'N1', 'Wake', 'REM'])\n",
        "\n",
        "            if snippet:\n",
        "                plt.axis('off')\n",
        "                plt.ylim(5, 5)\n",
        "            else:\n",
        "                plt.ylim(-5, 2)\n",
        "\n",
        "            if not snippet:\n",
        "                plt.savefig(output_path + 'data_validation_' + subject_id + '.png', bbox_inches='tight', pad_inches=0.1,\n",
        "                            dpi=300)\n",
        "            else:\n",
        "                plt.savefig(output_path + 'data_validation_zoom_' + subject_id + '.png', bbox_inches='tight',\n",
        "                            pad_inches=0.1, dpi=300)\n",
        "            plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvMaKW7EFRWx"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJDlmMExFRWx"
      },
      "source": [
        "# Setup Constant\n",
        "\n",
        "class Constants(object):\n",
        "\n",
        "    WAKE_THRESHOLD = 0.5  \n",
        "    REM_THRESHOLD = 0.35\n",
        "\n",
        "    EPOCH_DURATION_IN_SECONDS = 30\n",
        "    SECONDS_PER_MINUTE = 60\n",
        "    SECONDS_PER_DAY = 3600 * 24\n",
        "    SECONDS_PER_HOUR = 3600\n",
        "    VERBOSE = False\n",
        "    CROPPED_FILE_PATH = Utils.get_project_root().joinpath('outputs/cropped/')\n",
        "    FEATURE_FILE_PATH = Utils.get_project_root().joinpath('outputs/features/')\n",
        "    FIGURE_FILE_PATH = Utils.get_project_root().joinpath('outputs/figures/')\n",
        "    LOWER_BOUND = -0.2\n",
        "    MATLAB_PATH = '/Applications/MATLAB_R2019a.app/bin/matlab'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue7sPWEwFRWy"
      },
      "source": [
        "## Data Preparation (Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah3nEEhMFRWy"
      },
      "source": [
        "### Data Cleaning & Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLRmQBKAFRWy"
      },
      "source": [
        "#### Clock Proxy / Time-Based Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdKu7MCaX-M3"
      },
      "source": [
        "# Pre-Process: Build 'Clock Proxy' using MATLAB \n",
        "# Data will go in /data/circadian_predictions/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfkuj5LvYXo-"
      },
      "source": [
        "“Clock Proxy,” refer to a feature meant to approximate the changing drive of the circadian clock to sleep over the course of the night. \n",
        "\n",
        "2 ways of clock-proxy feature: \n",
        "1. Fixed cosine wave shifted relative to the time of recording start, which rose and fell over the course of the night. \n",
        "2. Circadian clocks: phase at the time of sleep onset. The step count data were used to estimate light input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26NaxyG_FRWy"
      },
      "source": [
        "class CircadianService(object):\n",
        "\n",
        "    def build_circadian_model():\n",
        "        os.system(Constants.MATLAB_PATH + ' -nodisplay -nosplash -nodesktop -r \\\"run(\\'' + str(\n",
        "            Utils.get_project_root()) + '/source/preprocessing/time/clock_proxy/runCircadianModel.m\\'); exit;\\\"')\n",
        "\n",
        "    def build_circadian_mesa():\n",
        "        os.system(Constants.MATLAB_PATH + ' -nodisplay -nosplash -nodesktop -r \\\"run(\\'' + str(\n",
        "            Utils.get_project_root()) + '/source/preprocessing/time/clock_proxy/runCircadianMESA.m\\'); exit;\\\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8uBg4xxFRWy"
      },
      "source": [
        "class TimeBasedFeatureService(object):\n",
        " \n",
        "    def load_time(subject_id):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_time(subject_id)\n",
        "        feature = pd.read_csv(str(feature_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path_for_time(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_time_feature.out')\n",
        "\n",
        "    def write_time(subject_id, feature):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_time(subject_id)\n",
        "        np.savetxt(feature_path, feature, fmt='%f')\n",
        "\n",
        "    def load_circadian_model(subject_id):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_circadian_model(subject_id)\n",
        "        feature = pd.read_csv(str(feature_path), delimiter=' ').values\n",
        "        return feature\n",
        "\n",
        "    def get_path_for_circadian_model(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_circadian_feature.out')\n",
        "\n",
        "    def write_circadian_model(subject_id, feature):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_circadian_model(subject_id)\n",
        "        np.savetxt(feature_path, feature, fmt='%f')\n",
        "\n",
        "    def load_cosine(subject_id):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_cosine(subject_id)\n",
        "        feature = pd.read_csv(str(feature_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path_for_cosine(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_cosine_feature.out')\n",
        "\n",
        "    def write_cosine(subject_id, feature):\n",
        "        feature_path = TimeBasedFeatureService.get_path_for_cosine(subject_id)\n",
        "        np.savetxt(feature_path, feature, fmt='%f')\n",
        "\n",
        "    def build_time(valid_epochs):\n",
        "        features = []\n",
        "        first_timestamp = valid_epochs[0].timestamp\n",
        "        for epoch in valid_epochs:\n",
        "            value = epoch.timestamp - first_timestamp\n",
        "\n",
        "            value = value / 3600.0  # Changing units to hours improves performance\n",
        "\n",
        "            features.append(value)\n",
        "        return np.array(features)\n",
        "\n",
        "    def build_circadian_model(subject_id, valid_epochs):\n",
        "        circadian_file = Utils.get_project_root().joinpath('data/circadian_predictions/' + subject_id +\n",
        "                                                           '_clock_proxy.txt')\n",
        "        if circadian_file.is_file():\n",
        "            circadian_model = pd.read_csv(str(circadian_file), delimiter=',').values\n",
        "\n",
        "            return TimeBasedFeatureService.build_circadian_model_from_raw(circadian_model, valid_epochs)\n",
        "\n",
        "    def cosine_proxy(time):\n",
        "        sleep_drive_cosine_shift = 5\n",
        "        return -1 * np.math.cos((time - sleep_drive_cosine_shift * Constants.SECONDS_PER_HOUR) *\n",
        "                                2 * np.math.pi / Constants.SECONDS_PER_DAY)\n",
        "\n",
        "    def build_cosine(valid_epochs):\n",
        "        features = []\n",
        "        first_value = TimeBasedFeatureService.cosine_proxy(0)\n",
        "        first_timestamp = valid_epochs[0].timestamp\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            value = TimeBasedFeatureService.cosine_proxy(epoch.timestamp - first_timestamp)\n",
        "            normalized_value = value\n",
        "            features.append(normalized_value)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def build_circadian_model_from_raw(circadian_model, valid_epochs):\n",
        "        features = []\n",
        "\n",
        "        first_inactive_epoch = valid_epochs[0]\n",
        "        first_value = np.interp(first_inactive_epoch.timestamp, circadian_model[:, 0], circadian_model[:, 1])\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            time = epoch.timestamp\n",
        "            value = np.interp(time, circadian_model[:, 0], circadian_model[:, 1])\n",
        "            normalized_value = (value - first_value) / (np.amin((circadian_model[:, 1] - first_value)))\n",
        "\n",
        "            if normalized_value < Constants.LOWER_BOUND:\n",
        "                normalized_value = Constants.LOWER_BOUND\n",
        "\n",
        "            features.append([normalized_value])\n",
        "\n",
        "        feature_array = np.array(features)\n",
        "\n",
        "        return feature_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLfan4-FRW1"
      },
      "source": [
        "#### Motion Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G1q9ttsd51U"
      },
      "source": [
        "Raw Acceleration from the Apple Watch represents in the x, y, and z directions; and a fourth, representing the UNIX timestamp.\n",
        "\n",
        "The final activity count feature was arrived at by convolving the window with a Gaussian (σ = 50 seconds)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rycpu2CkXke1"
      },
      "source": [
        "##### Motion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX20MI5oFRW1"
      },
      "source": [
        "class MotionCollection(object):\n",
        "    def __init__(self, subject_id, data):\n",
        "        self.subject_id = subject_id\n",
        "        self.data = data\n",
        "        self.timestamps = data[:, 0]\n",
        "        self.values = data[:, 1:]\n",
        "\n",
        "    def get_interval(self):\n",
        "        return Interval(start_time=np.amin(self.data[:, 0]),\n",
        "                        end_time=np.amax(self.data[:, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0425Yl7cFRW1"
      },
      "source": [
        "class MotionService(object):\n",
        "\n",
        "    def load_raw(subject_id):\n",
        "        raw_motion_path = MotionService.get_raw_file_path(subject_id)\n",
        "        motion_array = MotionService.load(raw_motion_path)\n",
        "        motion_array = Utils.remove_repeats(motion_array)\n",
        "        return MotionCollection(subject_id=subject_id, data=motion_array)\n",
        "\n",
        "    def load_cropped(subject_id):\n",
        "        cropped_motion_path = MotionService.get_cropped_file_path(subject_id)\n",
        "        motion_array = MotionService.load(cropped_motion_path)\n",
        "        return MotionCollection(subject_id=subject_id, data=motion_array)\n",
        "\n",
        "    def load(motion_file, delimiter=' '):\n",
        "        motion_array = pd.read_csv(str(motion_file), delimiter=delimiter).values\n",
        "        return motion_array\n",
        "\n",
        "    def write(motion_collection):\n",
        "        motion_output_path = MotionService.get_cropped_file_path(motion_collection.subject_id)\n",
        "        np.savetxt(motion_output_path, motion_collection.data, fmt='%f')\n",
        "\n",
        "    def crop(motion_collection, interval):\n",
        "        subject_id = motion_collection.subject_id\n",
        "        timestamps = motion_collection.timestamps\n",
        "        valid_indices = ((timestamps >= interval.start_time)\n",
        "                         & (timestamps < interval.end_time)).nonzero()[0]\n",
        "\n",
        "        cropped_data = motion_collection.data[valid_indices, :]\n",
        "        return MotionCollection(subject_id=subject_id, data=cropped_data)\n",
        "\n",
        "    def get_cropped_file_path(subject_id):\n",
        "        return Constants.CROPPED_FILE_PATH.joinpath(subject_id + \"_cleaned_motion.out\")\n",
        "\n",
        "    def get_raw_file_path(subject_id):\n",
        "        project_root = Utils.get_project_root()\n",
        "        return project_root.joinpath('data/motion/' + subject_id + '_acceleration.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeNutqa8FRW1"
      },
      "source": [
        "class MotionFeatureService(object):\n",
        "\n",
        "    def load(subject_id):\n",
        "        motion_feature_path = MotionFeatureService.get_path(subject_id)\n",
        "        feature = pd.read_csv(str(motion_feature_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_motion_feature.out')\n",
        "\n",
        "    def write(subject_id, feature):\n",
        "        motion_feature_path = MotionFeatureService.get_path(subject_id)\n",
        "        np.savetxt(motion_feature_path, feature, fmt='%f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gueuyZexFRW1"
      },
      "source": [
        "# TEST\n",
        "#motion_file = MotionService.get_raw_file_path('46343')\n",
        "#motion_array = MotionService.load(motion_file)\n",
        "#motion_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqFywwIiFRWy"
      },
      "source": [
        "##### Activity Count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gIqmSPaSeOW"
      },
      "source": [
        "Activity Count converted from raw acceleration in m/s^2 using the method from Lindert BH, et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xDexSF6FRWy"
      },
      "source": [
        "class ActivityCountCollection(object):\n",
        "    def __init__(self, subject_id, data):\n",
        "        self.subject_id = subject_id\n",
        "        self.data = data\n",
        "        self.timestamps = data[:, 0]\n",
        "        self.values = data[:, 1:]\n",
        "\n",
        "    def get_interval(self):\n",
        "        return Interval(start_time=np.amin(self.data[:, 0]),\n",
        "                        end_time=np.amax(self.data[:, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8DfRFFFRWy"
      },
      "source": [
        "class ActivityCountService(object):\n",
        "\n",
        "    def load_cropped(subject_id):\n",
        "        activity_counts_path = ActivityCountService.get_cropped_file_path(subject_id)\n",
        "        counts_array = ActivityCountService.load(activity_counts_path)\n",
        "        return ActivityCountCollection(subject_id=subject_id, data=counts_array)\n",
        "\n",
        "    def load(counts_file):\n",
        "        counts_array = pd.read_csv(str(counts_file)).values\n",
        "        return counts_array\n",
        "\n",
        "    def get_cropped_file_path(subject_id):\n",
        "        return Constants.CROPPED_FILE_PATH.joinpath(subject_id + \"_cleaned_counts.out\")\n",
        "    \n",
        "    def build_activity_counts():\n",
        "        os.system(MATLAB_PATH + ' -nodisplay -nosplash -nodesktop -r \\\"run(\\'' + str(\n",
        "            Utils.get_project_root()) + '/source/make_counts.m\\'); exit;\\\"')\n",
        "\n",
        "    def build_activity_counts_without_matlab(subject_id, data):\n",
        "        fs = 50\n",
        "        time = np.arange(np.amin(data[:, 0]), np.amax(data[:, 0]), 1.0 / fs)\n",
        "        z_data = np.interp(time, data[:, 0], data[:, 3])\n",
        "\n",
        "        cf_low = 3\n",
        "        cf_hi = 11\n",
        "        order = 5\n",
        "        w1 = cf_low / (fs / 2)\n",
        "        w2 = cf_hi / (fs / 2)\n",
        "        pass_band = [w1, w2]\n",
        "        b, a = butter(order, pass_band, 'bandpass')\n",
        "\n",
        "        z_filt = filtfilt(b, a, z_data)\n",
        "        z_filt = np.abs(z_filt)\n",
        "        top_edge = 5\n",
        "        bottom_edge = 0\n",
        "        number_of_bins = 128\n",
        "\n",
        "        bin_edges = np.linspace(bottom_edge, top_edge, number_of_bins + 1)\n",
        "        binned = np.digitize(z_filt, bin_edges)\n",
        "        epoch = 15\n",
        "        counts = ActivityCountService.max2epochs(binned, fs, epoch)\n",
        "        counts = (counts - 18) * 3.07\n",
        "        counts[counts < 0] = 0\n",
        "\n",
        "        time_counts = np.linspace(np.min(data[:, 0]), max(data[:, 0]), np.shape(counts)[0])\n",
        "        time_counts = np.expand_dims(time_counts, axis=1)\n",
        "        counts = np.expand_dims(counts, axis=1)\n",
        "        output = np.hstack((time_counts, counts))\n",
        "\n",
        "        activity_count_output_path = ActivityCountService.get_cropped_file_path(subject_id)\n",
        "        np.savetxt(activity_count_output_path, output, fmt='%f', delimiter=',')\n",
        "\n",
        "    def max2epochs(data, fs, epoch):\n",
        "        data = data.flatten()\n",
        "        seconds = int(np.floor(np.shape(data)[0] / fs))\n",
        "        data = np.abs(data)\n",
        "        data = data[0:int(seconds * fs)]\n",
        "\n",
        "        data = data.reshape(fs, seconds, order='F').copy()\n",
        "\n",
        "        data = data.max(0)\n",
        "        data = data.flatten()\n",
        "        N = np.shape(data)[0]\n",
        "        num_epochs = int(np.floor(N / epoch))\n",
        "        data = data[0:(num_epochs * epoch)]\n",
        "\n",
        "        data = data.reshape(epoch, num_epochs, order='F').copy()\n",
        "        epoch_data = np.sum(data, axis=0)\n",
        "        epoch_data = epoch_data.flatten()\n",
        "\n",
        "        return epoch_data\n",
        "\n",
        "    def crop(activity_count_collection, interval):\n",
        "        subject_id = activity_count_collection.subject_id\n",
        "        timestamps = activity_count_collection.timestamps\n",
        "        valid_indices = ((timestamps >= interval.start_time)\n",
        "                         & (timestamps < interval.end_time)).nonzero()[0]\n",
        "\n",
        "        cropped_data = activity_count_collection.data[valid_indices, :]\n",
        "        return ActivityCountCollection(subject_id=subject_id, data=cropped_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rQPuciZFRWz"
      },
      "source": [
        "class ActivityCountFeatureService(object):\n",
        "    WINDOW_SIZE = 10 * 30 - 15\n",
        "\n",
        "    def load(subject_id):\n",
        "        activity_count_feature_path = ActivityCountFeatureService.get_path(subject_id)\n",
        "        feature = pd.read_csv(str(activity_count_feature_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_count_feature.out')\n",
        "\n",
        "    def write(subject_id, feature):\n",
        "        activity_counts_feature_path = ActivityCountFeatureService.get_path(subject_id)\n",
        "        np.savetxt(activity_counts_feature_path, feature, fmt='%f')\n",
        "\n",
        "    def get_window(timestamps, epoch):\n",
        "        start_time = epoch.timestamp - ActivityCountFeatureService.WINDOW_SIZE\n",
        "        end_time = epoch.timestamp + Epoch.DURATION + ActivityCountFeatureService.WINDOW_SIZE\n",
        "        timestamps_ravel = timestamps.ravel()\n",
        "        indices_in_range = np.unravel_index(np.where((timestamps_ravel > start_time) & (timestamps_ravel < end_time)),\n",
        "                                            timestamps.shape)\n",
        "        return indices_in_range[0][0]\n",
        "\n",
        "    def build(subject_id, valid_epochs):\n",
        "        activity_count_collection = ActivityCountService.load_cropped(subject_id)\n",
        "        return ActivityCountFeatureService.build_from_collection(activity_count_collection, valid_epochs)\n",
        "\n",
        "    def build_from_collection(activity_count_collection, valid_epochs):\n",
        "        count_features = []\n",
        "\n",
        "        interpolated_timestamps, interpolated_counts = ActivityCountFeatureService.interpolate(\n",
        "            activity_count_collection)\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            indices_in_range = ActivityCountFeatureService.get_window(interpolated_timestamps, epoch)\n",
        "            activity_counts_in_range = interpolated_counts[indices_in_range]\n",
        "\n",
        "            feature = ActivityCountFeatureService.get_feature(activity_counts_in_range)\n",
        "            count_features.append(feature)\n",
        "\n",
        "        return np.array(count_features)\n",
        "\n",
        "    def get_feature(count_values):\n",
        "        convolution = Utils.smooth_gauss(count_values.flatten(), np.shape(count_values.flatten())[0])\n",
        "        return np.array([convolution])\n",
        "\n",
        "    def interpolate(activity_count_collection):\n",
        "        timestamps = activity_count_collection.timestamps.flatten()\n",
        "        activity_count_values = activity_count_collection.values.flatten()\n",
        "        interpolated_timestamps = np.arange(np.amin(timestamps),\n",
        "                                            np.amax(timestamps), 1)\n",
        "        interpolated_counts = np.interp(interpolated_timestamps, timestamps, activity_count_values)\n",
        "        return interpolated_timestamps, interpolated_counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORxeiA61FRWz"
      },
      "source": [
        "# TEST \n",
        "#act_file = ActivityCountService.get_cropped_file_path('46343')\n",
        "#act_array = ActivityCountService.load(act_file)\n",
        "#act_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxMJNXGVFRWz"
      },
      "source": [
        "class ActivityCountFeatureService(object):\n",
        "    WINDOW_SIZE = 10 * 30 - 15\n",
        "\n",
        "    def load(subject_id):\n",
        "        activity_count_feature_path = ActivityCountFeatureService.get_path(subject_id)\n",
        "        feature = pd.read_csv(str(activity_count_feature_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_count_feature.out')\n",
        "\n",
        "    def write(subject_id, feature):\n",
        "        activity_counts_feature_path = ActivityCountFeatureService.get_path(subject_id)\n",
        "        np.savetxt(activity_counts_feature_path, feature, fmt='%f')\n",
        "\n",
        "    def get_window(timestamps, epoch):\n",
        "        start_time = epoch.timestamp - ActivityCountFeatureService.WINDOW_SIZE\n",
        "        end_time = epoch.timestamp + Epoch.DURATION + ActivityCountFeatureService.WINDOW_SIZE\n",
        "        timestamps_ravel = timestamps.ravel()\n",
        "        indices_in_range = np.unravel_index(np.where((timestamps_ravel > start_time) & (timestamps_ravel < end_time)),\n",
        "                                            timestamps.shape)\n",
        "        return indices_in_range[0][0]\n",
        "\n",
        "    def build(subject_id, valid_epochs):\n",
        "        activity_count_collection = ActivityCountService.load_cropped(subject_id)\n",
        "        return ActivityCountFeatureService.build_from_collection(activity_count_collection, valid_epochs)\n",
        "\n",
        "    def build_from_collection(activity_count_collection, valid_epochs):\n",
        "        count_features = []\n",
        "\n",
        "        interpolated_timestamps, interpolated_counts = ActivityCountFeatureService.interpolate(\n",
        "            activity_count_collection)\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            indices_in_range = ActivityCountFeatureService.get_window(interpolated_timestamps, epoch)\n",
        "            activity_counts_in_range = interpolated_counts[indices_in_range]\n",
        "\n",
        "            feature = ActivityCountFeatureService.get_feature(activity_counts_in_range)\n",
        "            count_features.append(feature)\n",
        "\n",
        "        return np.array(count_features)\n",
        "\n",
        "    def get_feature(count_values):\n",
        "        convolution = Utils.smooth_gauss(count_values.flatten(), np.shape(count_values.flatten())[0])\n",
        "        return np.array([convolution])\n",
        "\n",
        "    def interpolate(activity_count_collection):\n",
        "        timestamps = activity_count_collection.timestamps.flatten()\n",
        "        activity_count_values = activity_count_collection.values.flatten()\n",
        "        interpolated_timestamps = np.arange(np.amin(timestamps),\n",
        "                                            np.amax(timestamps), 1)\n",
        "        interpolated_counts = np.interp(interpolated_timestamps, timestamps, activity_count_values)\n",
        "        return interpolated_timestamps, interpolated_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYkV1wNrFRWz"
      },
      "source": [
        "#### Heart Rate Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYoKDZ_WWRfU"
      },
      "source": [
        "Heart rate (bpm) was measured by PPG from the Apple Watch. \n",
        "\n",
        "This signal was interpolated to have a value for every 1 second,smoothed and filtered to amplify periods of high change by convolving with a difference of Gaussians filter (σ 1 = 120 seconds, σ 2 = 600 seconds). \n",
        "\n",
        "Each individual was normalized by dividing by the 90th percentile in the absolute difference between each heart rate measurement and the mean heart rate over the sleep period. \n",
        "\n",
        "The standard deviation in the window around the scored epoch was used as the representative feature for HRV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0b58jW-FRWz"
      },
      "source": [
        "class HeartRateCollection(object):\n",
        "    def __init__(self, subject_id, data):\n",
        "        self.subject_id = subject_id\n",
        "        self.data = data\n",
        "        self.timestamps = data[:, 0]\n",
        "        self.values = data[:, 1:]\n",
        "\n",
        "    def get_interval(self):\n",
        "        return Interval(start_time=np.amin(self.data[:, 0]),\n",
        "                        end_time=np.amax(self.data[:, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sifPZ_KUFRWz"
      },
      "source": [
        "class HeartRateService(object):\n",
        "\n",
        "    def load_raw(subject_id):\n",
        "        raw_hr_path = HeartRateService.get_raw_file_path(subject_id)\n",
        "        heart_rate_array = HeartRateService.load(raw_hr_path, \",\")\n",
        "        heart_rate_array = Utils.remove_repeats(heart_rate_array)\n",
        "        return HeartRateCollection(subject_id=subject_id, data=heart_rate_array)\n",
        "\n",
        "    def load_cropped(subject_id):\n",
        "        cropped_hr_path = HeartRateService.get_cropped_file_path(subject_id)\n",
        "        heart_rate_array = HeartRateService.load(cropped_hr_path)\n",
        "        return HeartRateCollection(subject_id=subject_id, data=heart_rate_array)\n",
        "\n",
        "    def load(hr_file, delimiter=\" \"):\n",
        "        heart_rate_array = pd.read_csv(str(hr_file), delimiter=delimiter).values\n",
        "        return heart_rate_array\n",
        "\n",
        "    def write(heart_rate_collection):\n",
        "        hr_output_path = HeartRateService.get_cropped_file_path(heart_rate_collection.subject_id)\n",
        "        np.savetxt(hr_output_path, heart_rate_collection.data, fmt='%f')\n",
        "\n",
        "    def crop(heart_rate_collection, interval):\n",
        "        subject_id = heart_rate_collection.subject_id\n",
        "        timestamps = heart_rate_collection.timestamps\n",
        "        valid_indices = ((timestamps >= interval.start_time)\n",
        "                         & (timestamps < interval.end_time)).nonzero()[0]\n",
        "\n",
        "        cropped_data = heart_rate_collection.data[valid_indices, :]\n",
        "        return HeartRateCollection(subject_id=subject_id, data=cropped_data)\n",
        "\n",
        "    def get_cropped_file_path(subject_id):\n",
        "        return Constants.CROPPED_FILE_PATH.joinpath(subject_id + \"_cleaned_hr.out\")\n",
        "\n",
        "    def get_raw_file_path(subject_id):\n",
        "        heart_rate_dir = Utils.get_project_root().joinpath('data/heart_rate/')\n",
        "        return heart_rate_dir.joinpath(subject_id + '_heartrate.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYso0oktFRWz"
      },
      "source": [
        "class HeartRateFeatureService(object):\n",
        "    WINDOW_SIZE = 10 * 30 - 15\n",
        "\n",
        "    def load(subject_id):\n",
        "        heart_rate_feature_path = HeartRateFeatureService.get_path(subject_id)\n",
        "        feature = pd.read_csv(str(heart_rate_feature_path), delimiter=' ').values\n",
        "        return feature\n",
        "\n",
        "    def get_path(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_hr_feature.out')\n",
        "\n",
        "    def write(subject_id, feature):\n",
        "        heart_rate_feature_path = HeartRateFeatureService.get_path(subject_id)\n",
        "        np.savetxt(heart_rate_feature_path, feature, fmt='%f')\n",
        "\n",
        "    def build(subject_id, valid_epochs):\n",
        "        heart_rate_collection = HeartRateService.load_cropped(subject_id)\n",
        "        return HeartRateFeatureService.build_from_collection(heart_rate_collection, valid_epochs)\n",
        "\n",
        "    def build_from_collection(heart_rate_collection, valid_epochs):\n",
        "        heart_rate_features = []\n",
        "\n",
        "        interpolated_timestamps, interpolated_hr = HeartRateFeatureService.interpolate_and_normalize(\n",
        "            heart_rate_collection)\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            indices_in_range = HeartRateFeatureService.get_window(interpolated_timestamps, epoch)\n",
        "            heart_rate_values_in_range = interpolated_hr[indices_in_range]\n",
        "\n",
        "            feature = HeartRateFeatureService.get_feature(heart_rate_values_in_range)\n",
        "\n",
        "            heart_rate_features.append(feature)\n",
        "\n",
        "        return np.array(heart_rate_features)\n",
        "\n",
        "    def get_window(timestamps, epoch):\n",
        "        start_time = epoch.timestamp - HeartRateFeatureService.WINDOW_SIZE\n",
        "        end_time = epoch.timestamp + Epoch.DURATION + HeartRateFeatureService.WINDOW_SIZE\n",
        "        timestamps_ravel = timestamps.ravel()\n",
        "        indices_in_range = np.unravel_index(np.where((timestamps_ravel > start_time) & (timestamps_ravel < end_time)),\n",
        "                                            timestamps.shape)\n",
        "        return indices_in_range[0][0]\n",
        "\n",
        "    def get_feature(heart_rate_values):\n",
        "        return [np.std(heart_rate_values)]\n",
        "\n",
        "    def interpolate_and_normalize(heart_rate_collection):\n",
        "        timestamps = heart_rate_collection.timestamps.flatten()\n",
        "        heart_rate_values = heart_rate_collection.values.flatten()\n",
        "        interpolated_timestamps = np.arange(np.amin(timestamps),\n",
        "                                            np.amax(timestamps), 1)\n",
        "        interpolated_hr = np.interp(interpolated_timestamps, timestamps, heart_rate_values)\n",
        "\n",
        "        interpolated_hr = Utils.convolve_with_dog(interpolated_hr, HeartRateFeatureService.WINDOW_SIZE)\n",
        "\n",
        "        scalar = np.percentile(np.abs(interpolated_hr), 90)\n",
        "        interpolated_hr = interpolated_hr / scalar\n",
        "\n",
        "        return interpolated_timestamps, interpolated_hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYTTL0ZFRW1"
      },
      "source": [
        "# TEST\n",
        "#hr_file = HeartRateService.get_raw_file_path('46343')\n",
        "#hr_array = HeartRateService.load(hr_file)\n",
        "#hr_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUn6UJwFRW1"
      },
      "source": [
        "#### PSG (Ground Truth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-bZjyXXFRW1"
      },
      "source": [
        "class PSGFileType(Enum):\n",
        "    Vitaport = 0\n",
        "    Compumedics = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRP-cVJ2FRW1"
      },
      "source": [
        "class StageItem(object):\n",
        "    def __init__(self, epoch, stage: SleepStage):\n",
        "        self.epoch = epoch\n",
        "        self.stage = stage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRo9awnCFRW2"
      },
      "source": [
        "class ReportSummary(object):\n",
        "    def __init__(self, study_date, start_epoch, start_time, file_type):\n",
        "        self.study_date = study_date\n",
        "        self.start_epoch = start_epoch\n",
        "        self.start_time = start_time\n",
        "        self.file_type = file_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9aISxfFRW2"
      },
      "source": [
        "class PSGConverter(object):\n",
        "    strings_to_labels = {\n",
        "        \"?\": SleepStage.unscored,\n",
        "        \"W\": SleepStage.wake,\n",
        "        \"1\": SleepStage.n1,\n",
        "        \"N1\": SleepStage.n1,\n",
        "        \"2\": SleepStage.n2,\n",
        "        \"N2\": SleepStage.n2,\n",
        "        \"3\": SleepStage.n3,\n",
        "        \"N3\": SleepStage.n3,\n",
        "        \"4\": SleepStage.n4,\n",
        "        \"N4\": SleepStage.n4,\n",
        "        \"R\": SleepStage.rem,\n",
        "        \"M\": SleepStage.wake}\n",
        "\n",
        "    ints_to_labels = {\n",
        "        -1: SleepStage.unscored,\n",
        "        0: SleepStage.wake,\n",
        "        1: SleepStage.n1,\n",
        "        2: SleepStage.n2,\n",
        "        3: SleepStage.n3,\n",
        "        4: SleepStage.n4,\n",
        "        5: SleepStage.rem,\n",
        "        6: SleepStage.unscored}\n",
        "\n",
        "    def get_label_from_string(stage_string):\n",
        "        if stage_string in PSGConverter.strings_to_labels:\n",
        "            return PSGConverter.strings_to_labels[stage_string]\n",
        "\n",
        "    def get_label_from_int(stage_int):\n",
        "        if stage_int in PSGConverter.ints_to_labels:\n",
        "            return PSGConverter.ints_to_labels[stage_int]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lWK7EchFRW2"
      },
      "source": [
        "class PSGRawDataCollection(object):\n",
        "    def __init__(self, subject_id, data: [SleepStage]):\n",
        "        self.subject_id = subject_id\n",
        "        self.data = data\n",
        "\n",
        "    def get_np_array(self):\n",
        "        number_of_epochs = len(self.data)\n",
        "        array = np.zeros((number_of_epochs, 2))\n",
        "\n",
        "        for index in range(number_of_epochs):\n",
        "            stage_item = self.data[index]\n",
        "            array[index, 0] = stage_item.epoch.timestamp\n",
        "            array[index, 1] = stage_item.stage.value\n",
        "\n",
        "        return array\n",
        "\n",
        "    def get_interval(self):\n",
        "        number_of_epochs = len(self.data)\n",
        "        min_timestamp = 1e15\n",
        "        max_timestamp = -1\n",
        "\n",
        "        for index in range(number_of_epochs):\n",
        "            stage_item = self.data[index]\n",
        "            if stage_item.epoch.timestamp < min_timestamp:\n",
        "                min_timestamp = stage_item.epoch.timestamp\n",
        "            if stage_item.epoch.timestamp > max_timestamp:\n",
        "                max_timestamp = stage_item.epoch.timestamp\n",
        "\n",
        "        return Interval(start_time=min_timestamp, end_time=max_timestamp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkGzUH_EFRW2"
      },
      "source": [
        "class PSGReportProcessor(object):\n",
        "\n",
        "    def get_start_epoch_for_subject(subject_id):\n",
        "        if int(subject_id) < 38:\n",
        "            return 1\n",
        "        if int(subject_id) == 40:\n",
        "            return 35\n",
        "        if int(subject_id) == 39:\n",
        "            return 32\n",
        "        if int(subject_id) == 38:\n",
        "            return 37\n",
        "        if int(subject_id) == 42:\n",
        "            return 27\n",
        "        if int(subject_id) == 41:\n",
        "            return 21\n",
        "\n",
        "    def get_summary_from_pdf(report_file_path):\n",
        "        report_raw_text = Utils.convert_pdf_to_txt(str(report_file_path), True)\n",
        "        raw_text_split_at_epoch = report_raw_text.split('Epoch')\n",
        "\n",
        "        split_at_study_date = (raw_text_split_at_epoch[0]).split('Study Date:  ')\n",
        "        study_date = (split_at_study_date[1]).split(' \\n')[0]\n",
        "\n",
        "        split_at_zero = (raw_text_split_at_epoch[1]).split('\\n\\n0')\n",
        "\n",
        "        split_at_newline = (split_at_zero[1]).split('\\n')\n",
        "        split_at_colon = (raw_text_split_at_epoch[1]).split(':')\n",
        "\n",
        "        start_epoch = split_at_newline[1]\n",
        "        start_time = split_at_colon[0][-2:] + ':' + split_at_colon[1] + ':' + split_at_colon[2][:5]\n",
        "\n",
        "        return ReportSummary(study_date=study_date, start_time=start_time, start_epoch=start_epoch,\n",
        "                             file_type=PSGFileType.Compumedics)\n",
        "\n",
        "    def get_summary_from_docx(report_file_path):\n",
        "        report_text = docx2txt.process(report_file_path)\n",
        "        report_split = report_text.split('DATE: ')\n",
        "        date = report_split[1].split('\\n')[0]\n",
        "\n",
        "        return ReportSummary(study_date=date, start_time=None, start_epoch=1, file_type=PSGFileType.Vitaport)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YoaOunUFRW2"
      },
      "source": [
        "class TimeService(object):\n",
        "\n",
        "    def get_start_epoch_timestamp(report_summary: ReportSummary):\n",
        "        if report_summary.file_type == PSGFileType.Compumedics:\n",
        "            study_date = dt.datetime.strptime(report_summary.study_date + ' ' + report_summary.start_time,\n",
        "                                              '%m/%d/%Y %I:%M:%S %p')\n",
        "            if study_date.strftime('%p') == 'AM':\n",
        "                study_date += dt.timedelta(days=1)\n",
        "            return study_date.timestamp()\n",
        "\n",
        "        if report_summary.file_type == PSGFileType.Vitaport:\n",
        "            study_date = dt.datetime.strptime(report_summary.study_date + ' ' + report_summary.start_time,\n",
        "                                              '%m/%d/%y %H:%M:%S')\n",
        "            if int(study_date.strftime('%H')) < 12:\n",
        "                study_date += dt.timedelta(days=1)\n",
        "            return study_date.timestamp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNl4wl0JFRW2"
      },
      "source": [
        "class CompumedicsProcessor(object):\n",
        "    DT_COMPUMEDICS_PSG = 30\n",
        "\n",
        "    def parse(report_summary, psg_stage_path):\n",
        "        data = []\n",
        "        score_strings = []\n",
        "        with open(psg_stage_path, 'rt') as csv_file:\n",
        "            file_reader = csv.reader(csv_file, delimiter=',', quotechar='|')\n",
        "\n",
        "            for row in file_reader:\n",
        "                score_strings.append(row[0])\n",
        "\n",
        "        start_epoch = report_summary.start_epoch\n",
        "        start_time_seconds = TimeService.get_start_epoch_timestamp(\n",
        "            report_summary)\n",
        "\n",
        "        for epoch_index in range(start_epoch - 1, len(score_strings)):\n",
        "            timestamp = start_time_seconds + \\\n",
        "                (epoch_index - start_epoch + 1) * \\\n",
        "                CompumedicsProcessor.DT_COMPUMEDICS_PSG\n",
        "            epoch = Epoch(timestamp=timestamp, index=epoch_index + 1)\n",
        "\n",
        "            stage = PSGConverter.get_label_from_string(\n",
        "                score_strings[epoch_index])\n",
        "            data.append(StageItem(epoch=epoch, stage=stage))\n",
        "\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpoxH1FPFRW2"
      },
      "source": [
        "class VitaportProcessor(object):\n",
        "    DT_TXT_PSG = 10\n",
        "\n",
        "    def parse(report_summary, psg_stage_path):\n",
        "        data = []\n",
        "        with open(psg_stage_path, 'rt') as csv_file:\n",
        "            file_reader = csv.reader(csv_file, delimiter=',', quotechar='|')\n",
        "            count = 0\n",
        "            rows_per_epoch = Epoch.DURATION / VitaportProcessor.DT_TXT_PSG\n",
        "\n",
        "            for row in file_reader:\n",
        "                if count == 0:\n",
        "                    start_time = row[1]\n",
        "                    start_score = int(row[0])\n",
        "                    report_summary.start_time = start_time\n",
        "                    start_time_seconds = TimeService.get_start_epoch_timestamp(\n",
        "                        report_summary)\n",
        "                    epoch = Epoch(timestamp=start_time_seconds, index=1)\n",
        "                    data.append(\n",
        "                        StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(start_score)))\n",
        "\n",
        "                if np.mod(count, rows_per_epoch) == 0 and count != 0:\n",
        "                    timestamp = start_time_seconds + count * VitaportProcessor.DT_TXT_PSG\n",
        "                    score = int(row[0])\n",
        "                    epoch = Epoch(timestamp=timestamp,\n",
        "                                  index=(1 + int(np.floor(count / rows_per_epoch))))\n",
        "\n",
        "                    data.append(\n",
        "                        StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(score)))\n",
        "                count = count + 1\n",
        "\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuWFcxAvFRW2"
      },
      "source": [
        "class PSGService(object):\n",
        "\n",
        "    def get_path_to_file(subject_id):\n",
        "        psg_dir = Utils.get_project_root().joinpath('data/psg')\n",
        "        compumedics_file = psg_dir.joinpath('compumedics/AW0' + subject_id.zfill(2) + '.TXT')\n",
        "        if compumedics_file.is_file():\n",
        "            return compumedics_file\n",
        "\n",
        "        txt_file = psg_dir.joinpath('vitaport/AW0' + subject_id.zfill(2) + '011.txt')\n",
        "        if txt_file.is_file():\n",
        "            return txt_file\n",
        "\n",
        "    def get_type_and_report(subject_id):\n",
        "        report_dir = Utils.get_project_root().joinpath('data/reports')\n",
        "\n",
        "        pdf_file = report_dir.joinpath('AW0' + subject_id.zfill(2) + '011_REPORT.pdf')\n",
        "        if pdf_file.is_file():\n",
        "            return pdf_file, PSGFileType.Compumedics\n",
        "\n",
        "        docx_file = report_dir.joinpath('AW00' + subject_id + '011 Study Sleep Log.docx')\n",
        "        if docx_file.is_file():\n",
        "            return docx_file, PSGFileType.Vitaport\n",
        "\n",
        "    def read_raw(subject_id):\n",
        "        psg_stage_path = PSGService.get_path_to_file(subject_id)\n",
        "        psg_report_path, psg_type = PSGService.get_type_and_report(subject_id)\n",
        "\n",
        "        if psg_type == PSGFileType.Compumedics:\n",
        "            report_summary = PSGReportProcessor.get_summary_from_pdf(psg_report_path)\n",
        "            report_summary.start_epoch = PSGReportProcessor.get_start_epoch_for_subject(subject_id)\n",
        "\n",
        "            data = CompumedicsProcessor.parse(report_summary, psg_stage_path)\n",
        "            return PSGRawDataCollection(subject_id=subject_id, data=data)\n",
        "\n",
        "        if psg_type == PSGFileType.Vitaport:\n",
        "            report_summary = PSGReportProcessor.get_summary_from_docx(psg_report_path)\n",
        "            data = VitaportProcessor.parse(report_summary, psg_stage_path)\n",
        "            return PSGRawDataCollection(subject_id=subject_id, data=data)\n",
        "\n",
        "    def read_precleaned(subject_id):\n",
        "        psg_path = str(Utils.get_project_root().joinpath('data/labels/' + subject_id + '_labeled_sleep.txt'))\n",
        "        data = []\n",
        "\n",
        "        with open(psg_path, 'rt') as csv_file:\n",
        "            file_reader = csv.reader(csv_file, delimiter=' ', quotechar='|')\n",
        "            count = 0\n",
        "            rows_per_epoch = 1\n",
        "            for row in file_reader:\n",
        "                if count == 0:\n",
        "                    start_time = float(row[0])\n",
        "                    start_score = int(row[1])\n",
        "                    epoch = Epoch(timestamp=start_time, index=1)\n",
        "                    data.append(StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(start_score)))\n",
        "                else:\n",
        "                    timestamp = start_time + count * 30\n",
        "                    score = int(row[1])\n",
        "                    epoch = Epoch(timestamp=timestamp,\n",
        "                                  index=(1 + int(np.floor(count / rows_per_epoch))))\n",
        "\n",
        "                    data.append(StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(score)))\n",
        "                count = count + 1\n",
        "        return PSGRawDataCollection(subject_id=subject_id, data=data)\n",
        "\n",
        "    def crop(psg_raw_collection, interval):\n",
        "        subject_id = psg_raw_collection.subject_id\n",
        "\n",
        "        stage_items = []\n",
        "        for stage_item in psg_raw_collection.data:\n",
        "            timestamp = stage_item.epoch.timestamp\n",
        "            if interval.start_time <= timestamp < interval.end_time:\n",
        "                stage_items.append(stage_item)\n",
        "\n",
        "        return PSGRawDataCollection(subject_id=subject_id, data=stage_items)\n",
        "\n",
        "    def write(psg_raw_data_collection):\n",
        "        data_array = []\n",
        "\n",
        "        for index in range(len(psg_raw_data_collection.data)):\n",
        "            stage_item = psg_raw_data_collection.data[index]\n",
        "            data_array.append([stage_item.epoch.timestamp, stage_item.stage.value])\n",
        "\n",
        "        np_psg_array = np.array(data_array)\n",
        "        psg_output_path = Constants.CROPPED_FILE_PATH.joinpath(psg_raw_data_collection.subject_id + \"_cleaned_psg.out\")\n",
        "\n",
        "        np.savetxt(psg_output_path, np_psg_array, fmt='%f')\n",
        "\n",
        "    def load_cropped_array(subject_id):\n",
        "        cropped_psg_path = Constants.CROPPED_FILE_PATH.joinpath(subject_id + \"_cleaned_psg.out\")\n",
        "        return pd.read_csv(str(cropped_psg_path), delimiter=' ').values\n",
        "\n",
        "    def load_cropped(subject_id):\n",
        "        cropped_array = PSGService.load_cropped_array(subject_id)\n",
        "        stage_items = []\n",
        "\n",
        "        for row in range(np.shape(cropped_array)[0]):\n",
        "            value = cropped_array[row, 1]\n",
        "            stage_items.append(StageItem(epoch=Epoch(timestamp=cropped_array[row, 0], index=row),\n",
        "                                         stage=PSGConverter.get_label_from_int(value)))\n",
        "\n",
        "        return PSGRawDataCollection(subject_id=subject_id, data=stage_items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCDsHdOcFRW3"
      },
      "source": [
        "class PSGLabelService(object):\n",
        "\n",
        "    def load(subject_id):\n",
        "        psg_label_path = PSGLabelService.get_path(subject_id)\n",
        "        feature = pd.read_csv(str(psg_label_path)).values\n",
        "        return feature\n",
        "\n",
        "    def get_path(subject_id):\n",
        "        return Constants.FEATURE_FILE_PATH.joinpath(subject_id + '_psg_labels.out')\n",
        "\n",
        "    def build(subject_id, valid_epochs):\n",
        "        psg_array = PSGService.load_cropped_array(subject_id)\n",
        "        labels = []\n",
        "        for epoch in valid_epochs:\n",
        "            value = np.interp(\n",
        "                epoch.timestamp, psg_array[:, 0], psg_array[:, 1])\n",
        "            labels.append(value)\n",
        "        return np.array(labels)\n",
        "\n",
        "    def write(subject_id, labels):\n",
        "        psg_labels_path = PSGLabelService.get_path(subject_id)\n",
        "        np.savetxt(psg_labels_path, labels, fmt='%f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOmTvaCmvq93"
      },
      "source": [
        "#### MESA Data Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMpFVgidi0Ye"
      },
      "source": [
        "The model were tested on Multi-ethnic Study of Atherosclerosis (MESA) cohort, which consists of motion data from actigraphy-derived activity counts and heart rate via pulse oximetry from co-recorded PSG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTc7oS1ivq93"
      },
      "source": [
        "class MetadataService(object):\n",
        "\n",
        "    def get_all_files():\n",
        "        project_root = str(Utils.get_project_root())\n",
        "        return glob.glob(project_root + \"/data/mesa/polysomnography/edfs/*edf\")\n",
        "\n",
        "    def get_metadata_dictionary():\n",
        "        metadata_dictionary = {}\n",
        "        ref_dict = dict()\n",
        "        ref_dict['ahi'] = 'ahiu35'\n",
        "        ref_dict['age'] = 'sleepage5c'\n",
        "        ref_dict['gender'] = 'gender1'  # 0 Female, 1 Male\n",
        "        ref_dict['tst'] = 'slpprdp5'\n",
        "        ref_dict['tib'] = 'time_bed5'\n",
        "        ref_dict['waso'] = 'waso5'\n",
        "        ref_dict['slp_eff'] = 'slp_eff5'\n",
        "        ref_dict['time_rem'] = 'timerem5'\n",
        "        ref_dict['time_n1'] = 'timest15'\n",
        "        ref_dict['time_n2'] = 'timest25'\n",
        "        ref_dict['time_n34'] = 'timest345'\n",
        "\n",
        "        with open('../mesa/mesa-sleep-dataset-0.4.csv') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            is_first_row = True\n",
        "            for row in csv_reader:\n",
        "                if is_first_row:\n",
        "                    is_first_row = False\n",
        "                    count = 0\n",
        "                    for col in row:\n",
        "                        if col in ref_dict.values():\n",
        "                            keys = list(ref_dict.keys())\n",
        "                            key = keys[list(ref_dict.values()).index(col)]\n",
        "                            ref_dict[key] = count  # Replace with index number where data is\n",
        "                        count = count + 1\n",
        "                else:\n",
        "                    subject_dict = {}\n",
        "                    for key in ref_dict:\n",
        "                        val = row[ref_dict[key]]\n",
        "                        if len(val) > 0:\n",
        "                            subject_dict[key] = float(val)\n",
        "                        else:\n",
        "                            subject_dict[key] = np.nan\n",
        "                    metadata_dictionary[int(row[0])] = subject_dict\n",
        "\n",
        "        return metadata_dictionary\n",
        "\n",
        "    def print_table(subject_ids):\n",
        "        all_files = MetadataService.get_all_files()\n",
        "        metadata_dictionary = MetadataService.get_metadata_dictionary()\n",
        "        ahis = []\n",
        "        ages = []\n",
        "        genders = []\n",
        "        tst = []\n",
        "        tib = []\n",
        "        waso = []\n",
        "        slp_eff = []\n",
        "        time_rem = []\n",
        "        time_nrem = []\n",
        "\n",
        "        for subject_index in subject_ids:  # Get all metadata for subjects\n",
        "            subject = int(all_files[subject_index][-8:-4])\n",
        "            subject_dict = metadata_dictionary[subject]\n",
        "            ahis.append(subject_dict['ahi'])\n",
        "            ages.append(subject_dict['age'])\n",
        "            genders.append(subject_dict['gender'])\n",
        "            tst.append(subject_dict['tst'])\n",
        "            tib.append(subject_dict['tib'])\n",
        "            waso.append(subject_dict['waso'])\n",
        "            slp_eff.append(subject_dict['slp_eff'])\n",
        "            time_rem.append(subject_dict['time_rem'])\n",
        "            time_nrem.append(\n",
        "                float(subject_dict['time_n1']) + float(subject_dict['time_n2']) + float(subject_dict['time_n34']))\n",
        "\n",
        "        ahis = np.array(ahis)\n",
        "        ages = np.array(ages)\n",
        "        genders = np.array(genders)\n",
        "        tst = np.array(tst)\n",
        "        tib = np.array(tib)\n",
        "        waso = np.array(waso)\n",
        "        slp_eff = np.array(slp_eff)\n",
        "\n",
        "        print('N women: ' + str(len(genders) - np.count_nonzero(genders)))\n",
        "\n",
        "        latex = True\n",
        "        if latex:\n",
        "            print(\n",
        "                '\\\\begin{table} \\\\caption{Sleep summary statistics - MESA subcohort}  \\\\label{tab:mesa} \\\\small  \\\\begin{tabularx}{\\\\columnwidth}{X | X | X  }\\\\hline Parameter & Mean (SD) & Range \\\\\\\\ \\\\hline')\n",
        "        else:\n",
        "            print('Parameter, Mean (SD), Range')\n",
        "\n",
        "        print(MetadataService.data_to_line('Age (years)', ages, latex))\n",
        "        print(MetadataService.data_to_line('TST (minutes)', tst, latex))\n",
        "        print(MetadataService.data_to_line('TIB (minutes)', tib, latex))\n",
        "        print(MetadataService.data_to_line('WASO (minutes)', waso, latex))\n",
        "        print(MetadataService.data_to_line('SE (\\%)', slp_eff, latex))\n",
        "        print(MetadataService.data_to_line('AHI', ahis, latex))\n",
        "\n",
        "        if latex:\n",
        "            print('\\\\end{tabularx} \\\\end{table}')\n",
        "\n",
        "    def data_to_line(name, data, latex=False):\n",
        "        if latex:\n",
        "            return name + ' & ' + str(round(np.mean(data), 2)) + ' (' + str(round(np.std(data), 2)) + ') & ' \\\n",
        "                   + str(round(np.min(data), 2)) + '-' + str(round(np.max(data), 2)) + '\\\\\\\\'\n",
        "        else:\n",
        "            return name + ',' + str(round(np.mean(data), 2)) + ' (' + str(round(np.std(data), 2)) + '),' \\\n",
        "                   + str(round(np.min(data), 2)) + '-' + str(round(np.max(data), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvOxUXuVvq93"
      },
      "source": [
        "class MesaActigraphyService(object):\n",
        "\n",
        "    def load_raw(file_id):\n",
        "        line_align = -1  # Find alignment line between PSG and actigraphy\n",
        "        project_root = str(Utils.get_project_root())\n",
        "\n",
        "        with open(project_root + '/data/mesa/overlap/mesa-actigraphy-psg-overlap.csv') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            next(csv_file)\n",
        "            for row in csv_reader:\n",
        "                if int(row[0]) == int(file_id):\n",
        "                    line_align = int(row[1])\n",
        "\n",
        "        activity = []\n",
        "        elapsed_time_counter = 0\n",
        "\n",
        "        if line_align == -1:  # If there was no alignment found\n",
        "            return ActivityCountCollection(subject_id=file_id, data=np.array([[-1], [-1]]))\n",
        "\n",
        "        with open(project_root + '/data/mesa/actigraphy/mesa-sleep-' + file_id + '.csv') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "            next(csv_file)\n",
        "            for row in csv_reader:\n",
        "                if int(row[1]) >= line_align:\n",
        "                    if row[4] == '':\n",
        "                        activity.append([elapsed_time_counter, np.nan])\n",
        "                    else:\n",
        "                        activity.append([elapsed_time_counter, float(row[4])])\n",
        "                    elapsed_time_counter = elapsed_time_counter + 30\n",
        "\n",
        "        data = np.array(activity)\n",
        "        data = Utils.remove_nans(data)\n",
        "\n",
        "        return ActivityCountCollection(subject_id=file_id, data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC2KHhFzvq93"
      },
      "source": [
        "class MesaHeartRateService(object):\n",
        "\n",
        "    def load_raw(file_id):\n",
        "        project_root = str(Utils.get_project_root())\n",
        "\n",
        "        edf_file = pyedflib.EdfReader(\n",
        "            project_root + '/data/mesa/polysomnography/edfs/mesa-sleep-' + file_id + '.edf')\n",
        "        signal_labels = edf_file.getSignalLabels()\n",
        "\n",
        "        hr_column = len(signal_labels) - 2\n",
        "\n",
        "        sample_frequencies = edf_file.getSampleFrequencies()\n",
        "\n",
        "        heart_rate = edf_file.readSignal(hr_column)\n",
        "        sf = sample_frequencies[hr_column]\n",
        "\n",
        "        # Get timestamps for heart rate data\n",
        "        time_hr = np.array(range(0, len(heart_rate)))\n",
        "        time_hr = time_hr / sf\n",
        "\n",
        "        data = np.transpose(np.vstack((time_hr, heart_rate)))\n",
        "        data = Utils.remove_nans(data)\n",
        "        return HeartRateCollection(subject_id=file_id, data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWHeRG80vq94"
      },
      "source": [
        "class MesaPSGService(object):\n",
        "\n",
        "    def load_raw(file_id):\n",
        "        stage_to_num = {'Stage 4 sleep|4': 4, 'Stage 3 sleep|3': 3, 'Stage 2 sleep|2': 2, 'Stage 1 sleep|1': 1,\n",
        "                        'Wake|0': 0, 'REM sleep|5': 5}\n",
        "        project_root = str(Utils.get_project_root())\n",
        "\n",
        "        xml_document = minidom.parse(\n",
        "            project_root + '/data/mesa/polysomnography/annotations-events-nsrr/mesa-sleep-' + file_id + '-nsrr.xml')\n",
        "        list_of_scored_events = xml_document.getElementsByTagName('ScoredEvent')\n",
        "\n",
        "        stage_data = []\n",
        "\n",
        "        for scored_event in list_of_scored_events:  # 3 is stage, 5 is start, 7 is duration\n",
        "            duration = scored_event.childNodes[7].childNodes[0].nodeValue\n",
        "            start = scored_event.childNodes[5].childNodes[0].nodeValue\n",
        "            stage = scored_event.childNodes[3].childNodes[0].nodeValue\n",
        "\n",
        "            if stage in stage_to_num:\n",
        "                # # For debugging: print(file_id + ' ' + str(stage) + ' ' + str(start) + ' ' + str(duration))\n",
        "                stage_data.append([stage_to_num[stage], float(start), float(duration)])\n",
        "\n",
        "        stages = []\n",
        "        for staged_window in stage_data[:]:  # Ignore last PSG overflow entry: it's long & doesn't have valid HR\n",
        "            elapsed_time_counter = 0\n",
        "            stage_value = staged_window[0]\n",
        "            duration = staged_window[2]\n",
        "\n",
        "            while elapsed_time_counter < duration:\n",
        "                stages.append(stage_value)\n",
        "                elapsed_time_counter = elapsed_time_counter + 1\n",
        "\n",
        "        return np.array(stages)\n",
        "\n",
        "    def crop(psg_labels, valid_epochs):\n",
        "        cropped_psg_labels = []\n",
        "\n",
        "        for epoch in valid_epochs:\n",
        "            index = int(epoch.timestamp)\n",
        "\n",
        "            cropped_psg_labels.append(psg_labels[index])\n",
        "        return np.array(cropped_psg_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fau8tc_Uvq94"
      },
      "source": [
        "# build_activity_count will run into the error; so\n",
        "# Use pre-processed files => copy 'clock_proxy' folder\n",
        "# Place to /data/mesa/...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RvZgAzOvq94"
      },
      "source": [
        "class MesaTimeBasedService(object):\n",
        "\n",
        "    def load_circadian_model(file_id):\n",
        "\n",
        "        path = Utils.get_project_root().joinpath(\n",
        "            'data/mesa/clock_proxy/' + file_id + '_clock_proxy.out')\n",
        "\n",
        "        if path.is_file():\n",
        "            array = pd.read_csv(str(path), delimiter=',').values\n",
        "            if np.shape(array)[0] > 0:\n",
        "                array = Utils.remove_nans(array)\n",
        "            if np.shape(array)[0] > 0:\n",
        "                return array\n",
        "\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZuLOs9pvq94"
      },
      "source": [
        "class MesaSubjectBuilder(object):\n",
        "\n",
        "    def build(file_id):\n",
        "        if Constants.VERBOSE:\n",
        "            print('Building MESA subject ' + file_id + '...')\n",
        "\n",
        "        raw_labeled_sleep = MesaPSGService.load_raw(file_id)\n",
        "        heart_rate_collection = MesaHeartRateService.load_raw(file_id)\n",
        "        activity_count_collection = MesaActigraphyService.load_raw(file_id)\n",
        "        circadian_model = MesaTimeBasedService.load_circadian_model(file_id)\n",
        "\n",
        "        if activity_count_collection.data[0][0] != -1 and circadian_model is not None:\n",
        "\n",
        "            circadian_model = Utils.remove_nans(circadian_model)\n",
        "\n",
        "            interval = Interval(start_time=0, end_time=np.shape(raw_labeled_sleep)[0])\n",
        "\n",
        "            activity_count_collection = ActivityCountService.crop(activity_count_collection, interval)\n",
        "            heart_rate_collection = HeartRateService.crop(heart_rate_collection, interval)\n",
        "\n",
        "            valid_epochs = []\n",
        "\n",
        "            for timestamp in range(interval.start_time, interval.end_time, Epoch.DURATION):\n",
        "                epoch = Epoch(timestamp=timestamp, index=len(valid_epochs))\n",
        "                activity_count_indices = ActivityCountFeatureService.get_window(activity_count_collection.timestamps,\n",
        "                                                                                epoch)\n",
        "                heart_rate_indices = HeartRateFeatureService.get_window(heart_rate_collection.timestamps, epoch)\n",
        "\n",
        "                if len(activity_count_indices) > 0 and 0 not in heart_rate_collection.values[heart_rate_indices]:\n",
        "                    valid_epochs.append(epoch)\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            labeled_sleep = np.expand_dims(\n",
        "                MesaPSGService.crop(psg_labels=raw_labeled_sleep, valid_epochs=valid_epochs),\n",
        "                axis=1)\n",
        "\n",
        "            feature_count = ActivityCountFeatureService.build_from_collection(activity_count_collection,\n",
        "                                                                              valid_epochs)\n",
        "            feature_hr = HeartRateFeatureService.build_from_collection(heart_rate_collection, valid_epochs)\n",
        "            feature_time = np.expand_dims(TimeBasedFeatureService.build_time(valid_epochs), axis=1)\n",
        "            feature_cosine = np.expand_dims(TimeBasedFeatureService.build_cosine(valid_epochs), axis=1)\n",
        "\n",
        "            feature_circadian = TimeBasedFeatureService.build_circadian_model_from_raw(circadian_model,\n",
        "                                                                                       valid_epochs)\n",
        "            feature_dictionary = {FeatureType.count: feature_count,\n",
        "                                  FeatureType.heart_rate: feature_hr,\n",
        "                                  FeatureType.time: feature_time,\n",
        "                                  FeatureType.circadian_model: feature_circadian,\n",
        "                                  FeatureType.cosine: feature_cosine}\n",
        "\n",
        "            subject = Subject(subject_id=file_id,\n",
        "                              labeled_sleep=labeled_sleep,\n",
        "                              feature_dictionary=feature_dictionary)\n",
        "\n",
        "            # Uncomment to save files for all subjects\n",
        "            #ax = plt.subplot(5, 1, 1)\n",
        "            #ax.plot(range(len(feature_hr)), feature_hr)\n",
        "            #ax = plt.subplot(5, 1, 2)\n",
        "            #ax.plot(range(len(feature_count)), feature_count)\n",
        "            #ax = plt.subplot(5, 1, 3)\n",
        "            #ax.plot(range(len(feature_cosine)), feature_cosine)\n",
        "            #ax = plt.subplot(5, 1, 4)\n",
        "            #ax.plot(range(len(feature_circadian)), feature_circadian)\n",
        "            #ax = plt.subplot(5, 1, 5)\n",
        "            #ax.plot(range(len(labeled_sleep)), labeled_sleep)\n",
        "            #\n",
        "            #plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(file_id + '.png')))\n",
        "            #plt.close()\n",
        "\n",
        "            return subject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MWDcmuevq94"
      },
      "source": [
        "class MesaDataService(object):\n",
        "\n",
        "    def get_all_subjects():\n",
        "        all_files = MetadataService.get_all_files()\n",
        "        all_subjects = []\n",
        "        for file in all_files:\n",
        "            file_id = file[-8:-4]\n",
        "            subject = MesaSubjectBuilder.build(file_id)\n",
        "            if subject is not None:\n",
        "                all_subjects.append(subject)\n",
        "\n",
        "        return all_subjects\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcR3G7Ivq94"
      },
      "source": [
        "#### Cleaning & Raw Data Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOCkv7eFRW3"
      },
      "source": [
        "class RawDataProcessor:\n",
        "    BASE_FILE_PATH = Utils.get_project_root().joinpath('outputs/cropped/')\n",
        "\n",
        "    def crop_all(subject_id):\n",
        "        \n",
        "        # psg_raw_collection = PSGService.read_raw(subject_id)  # Used to extract PSG details from the reports\n",
        "        psg_raw_collection = PSGService.read_precleaned(subject_id)  # Loads already extracted PSG data\n",
        "        motion_collection = MotionService.load_raw(subject_id)\n",
        "        heart_rate_collection = HeartRateService.load_raw(subject_id)\n",
        "\n",
        "        valid_interval = RawDataProcessor.get_intersecting_interval([psg_raw_collection,\n",
        "                                                                     motion_collection,\n",
        "                                                                     heart_rate_collection])\n",
        "\n",
        "        psg_raw_collection = PSGService.crop(psg_raw_collection, valid_interval)\n",
        "        motion_collection = MotionService.crop(motion_collection, valid_interval)\n",
        "        heart_rate_collection = HeartRateService.crop(heart_rate_collection, valid_interval)\n",
        "\n",
        "        PSGService.write(psg_raw_collection)\n",
        "        MotionService.write(motion_collection)\n",
        "        HeartRateService.write(heart_rate_collection)\n",
        "        \n",
        "        # Builds activity counts with python, not MATLAB\n",
        "        ActivityCountService.build_activity_counts_without_matlab(subject_id, motion_collection.data)\n",
        "        \n",
        "    def get_intersecting_interval(collection_list):\n",
        "        start_times = []\n",
        "        end_times = []\n",
        "        for collection in collection_list:\n",
        "            interval = collection.get_interval()\n",
        "            start_times.append(interval.start_time)\n",
        "            end_times.append(interval.end_time)\n",
        "\n",
        "        return Interval(start_time=max(start_times), end_time=min(end_times))\n",
        "\n",
        "    def get_valid_epochs(subject_id):\n",
        "\n",
        "        psg_collection = PSGService.load_cropped(subject_id)\n",
        "        motion_collection = MotionService.load_cropped(subject_id)\n",
        "        heart_rate_collection = HeartRateService.load_cropped(subject_id)\n",
        "\n",
        "        start_time = psg_collection.data[0].epoch.timestamp\n",
        "        motion_epoch_dictionary = RawDataProcessor.get_valid_epoch_dictionary(motion_collection.timestamps,\n",
        "                                                                              start_time)\n",
        "        hr_epoch_dictionary = RawDataProcessor.get_valid_epoch_dictionary(heart_rate_collection.timestamps,\n",
        "                                                                          start_time)\n",
        "\n",
        "        valid_epochs = []\n",
        "        for stage_item in psg_collection.data:\n",
        "            epoch = stage_item.epoch\n",
        "\n",
        "            if epoch.timestamp in motion_epoch_dictionary and epoch.timestamp in hr_epoch_dictionary \\\n",
        "                    and stage_item.stage != SleepStage.unscored:\n",
        "                valid_epochs.append(epoch)\n",
        "\n",
        "        return valid_epochs\n",
        "\n",
        "    def get_valid_epoch_dictionary(timestamps, start_time):\n",
        "        epoch_dictionary = {}\n",
        "\n",
        "        for ind in range(np.shape(timestamps)[0]):\n",
        "            time = timestamps[ind]\n",
        "            floored_timestamp = time - np.mod(time - start_time, Epoch.DURATION)\n",
        "\n",
        "            epoch_dictionary[floored_timestamp] = True\n",
        "\n",
        "        return epoch_dictionary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHTwtMpFRW3"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp46VuZmFRW3"
      },
      "source": [
        "class FeatureBuilder(object):\n",
        "\n",
        "    def build(subject_id):\n",
        "        if Constants.VERBOSE:\n",
        "            print(\"Getting valid epochs...\")\n",
        "        valid_epochs = RawDataProcessor.get_valid_epochs(subject_id)\n",
        "\n",
        "        if Constants.VERBOSE:\n",
        "            print(\"Building features...\")\n",
        "        FeatureBuilder.build_labels(subject_id, valid_epochs)\n",
        "        FeatureBuilder.build_from_wearables(subject_id, valid_epochs)\n",
        "        FeatureBuilder.build_from_time(subject_id, valid_epochs)\n",
        "\n",
        "    def build_labels(subject_id, valid_epochs):\n",
        "        psg_labels = PSGLabelService.build(subject_id, valid_epochs)\n",
        "        PSGLabelService.write(subject_id, psg_labels)\n",
        "\n",
        "    def build_from_wearables(subject_id, valid_epochs):\n",
        "        count_feature = ActivityCountFeatureService.build(subject_id, valid_epochs)\n",
        "        heart_rate_feature = HeartRateFeatureService.build(subject_id, valid_epochs)\n",
        "        ActivityCountFeatureService.write(subject_id, count_feature)\n",
        "        HeartRateFeatureService.write(subject_id, heart_rate_feature)\n",
        "\n",
        "    def build_from_time(subject_id, valid_epochs):\n",
        "        circadian_feature = TimeBasedFeatureService.build_circadian_model(subject_id, valid_epochs)\n",
        "        cosine_feature = TimeBasedFeatureService.build_cosine(valid_epochs)\n",
        "        time_feature = TimeBasedFeatureService.build_time(valid_epochs)\n",
        "\n",
        "        TimeBasedFeatureService.write_circadian_model(subject_id, circadian_feature)\n",
        "        TimeBasedFeatureService.write_cosine(subject_id, cosine_feature)\n",
        "        TimeBasedFeatureService.write_time(subject_id, time_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PayCtfARFRW3"
      },
      "source": [
        "class SubjectBuilder(object):\n",
        "\n",
        "    def get_all_subject_ids():\n",
        "      \n",
        "        ### Comment/Uncomment for selecting set of data\n",
        "\n",
        "        \"\"\"\n",
        "        ### All Dataset (31 subjects)\n",
        "        subjects_as_ints = [3509524, 5132496, 1066528, 5498603, 2638030, 2598705, 5383425, 1455390, 4018081, 9961348,\n",
        "                            1449548, 8258170, 781756, 9106476, 8686948, 8530312, 3997827, 4314139, 1818471, 4426783,\n",
        "                            8173033, 7749105, 5797046, 759667, 8000685, 6220552, 844359, 9618981, 1360686, 46343,\n",
        "                            8692923]\n",
        "        \"\"\"\n",
        "\n",
        "        ### Small-Set (10 subjects)\n",
        "        subjects_as_ints = [3509524, 1066528, 2598705, 9961348, 759667, 4426783,\n",
        "                            5797046, 8000685, 6220552, 46343]\n",
        "\n",
        "        subjects_as_strings = []\n",
        "\n",
        "        for subject in subjects_as_ints:\n",
        "            subjects_as_strings.append(str(subject))\n",
        "        return subjects_as_strings\n",
        "\n",
        "    def get_subject_dictionary():\n",
        "        subject_dictionary = {}\n",
        "        all_subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "        for subject_id in all_subject_ids:\n",
        "            subject_dictionary[subject_id] = SubjectBuilder.build(subject_id)\n",
        "\n",
        "        return subject_dictionary\n",
        "\n",
        "    def build(subject_id):\n",
        "        feature_count = ActivityCountFeatureService.load(subject_id)\n",
        "        feature_hr = HeartRateFeatureService.load(subject_id)\n",
        "        feature_time = TimeBasedFeatureService.load_time(subject_id)\n",
        "        feature_circadian = TimeBasedFeatureService.load_circadian_model(subject_id)\n",
        "        feature_cosine = TimeBasedFeatureService.load_cosine(subject_id)\n",
        "        labeled_sleep = PSGLabelService.load(subject_id)\n",
        "\n",
        "        feature_dictionary = {FeatureType.count: feature_count,\n",
        "                              FeatureType.heart_rate: feature_hr,\n",
        "                              FeatureType.time: feature_time,\n",
        "                              FeatureType.circadian_model: feature_circadian,\n",
        "                              FeatureType.cosine: feature_cosine}\n",
        "\n",
        "        subject = Subject(subject_id=subject_id,\n",
        "                          labeled_sleep=labeled_sleep,\n",
        "                          feature_dictionary=feature_dictionary)\n",
        "\n",
        "        # Uncomment to save plots of every subject's data:\n",
        "        #ax = plt.subplot(5, 1, 1)\n",
        "        #ax.plot(range(len(feature_hr)), feature_hr)\n",
        "        #ax = plt.subplot(5, 1, 2)\n",
        "        #ax.plot(range(len(feature_count)), feature_count)\n",
        "        #ax = plt.subplot(5, 1, 3)\n",
        "        #ax.plot(range(len(feature_cosine)), feature_cosine)\n",
        "        #ax = plt.subplot(5, 1, 4)\n",
        "        #ax.plot(range(len(feature_circadian)), feature_circadian)\n",
        "        #ax = plt.subplot(5, 1, 5)\n",
        "        #ax.plot(range(len(labeled_sleep)), labeled_sleep)\n",
        "        #\n",
        "        #plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(subject_id + '_applewatch.png')))\n",
        "        #plt.close()\n",
        "        \n",
        "        return subject\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7qxlJg1FRW3"
      },
      "source": [
        "def run_feature_builder(subject_set):\n",
        "    \n",
        "    print(\"+++ Start FeatureBuilder +++\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for subject in subject_set:\n",
        "        print(\"Cropping data from subject \" + str(subject) + \"...\")\n",
        "        RawDataProcessor.crop_all(str(subject))\n",
        "\n",
        "    # This uses MATLAB, but has been replaced with a python implementation\n",
        "    #ActivityCountService.build_activity_counts()  \n",
        "    \n",
        "    # Both of the circadian lines require MATLAB to run\n",
        "    # CircadianService.build_circadian_model() \n",
        "    # CircadianService.build_circadian_mesa()\n",
        "\n",
        "    for subject in subject_set:\n",
        "        FeatureBuilder.build(str(subject))\n",
        "\n",
        "    print(\"________________\")\n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"___ Finished FeatureBuilder ___\")\n",
        "\n",
        "\n",
        "def run_preprocessing():\n",
        "    subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "\n",
        "    run_feature_builder(subject_ids)\n",
        "\n",
        "    print(\"+++ Start DataPlotBuilder +++\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for subject in subject_ids:\n",
        "        DataPlotBuilder.make_data_demo(subject, False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"___ Finished DataPlotBuilder ___\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM7Zwy0KIHmh"
      },
      "source": [
        "### Run Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SveI8H3IGhT"
      },
      "source": [
        "# Uncomment to run preprocessing\n",
        "run_preprocessing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg0MmASOFRW3"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXqFSRV0aEKV"
      },
      "source": [
        "<h2> Algorithm training and selection </h2>\n",
        "\n",
        "1. Logistic Regression\n",
        "2. K-Nearest Neighbors \n",
        "3. Random Forest\n",
        "4. Neural Net (multilayer perceptron, MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhaWzwQFRW3"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtmlYSyZ5Pa"
      },
      "source": [
        "Fine tuning parameter using GridSearchCV: tries different parameters for training model to find the best, evaluates the model for each combination using the Cross-Validation method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpI_oc3Avq96"
      },
      "source": [
        "class ParameterSearch(object):\n",
        "    \n",
        "    parameter_dictionary = {\n",
        "        'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']},\n",
        "        'Random Forest': {'max_depth': [10, 50, 100]},\n",
        "        'k-Nearest Neighbors': {'n_neighbors': [500, 1000]},\n",
        "        'Neural Net': {'alpha': [0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
        "    }\n",
        "\n",
        "    def run_search(attributed_classifier, training_x, training_y, scoring):\n",
        "        parameter_range = ParameterSearch.parameter_dictionary[attributed_classifier.name]\n",
        "        grid_search = GridSearchCV(\n",
        "            attributed_classifier.classifier, parameter_range, scoring=scoring, cv=3, iid=False)\n",
        "        grid_search.fit(training_x, training_y)\n",
        "        return grid_search.best_params_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCLB-DdZFRW4"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkDD68RfmSQJ"
      },
      "source": [
        "Models were trained and tested using: \n",
        "- Monte Carlo cross-validation \n",
        "- Leave-one-out cross-validation \n",
        "\n",
        "<h4>Monte Carlo cross-validation </h4>\n",
        "\n",
        "*   sleep/wake classification: the dataset was randomly split 50 times into a training set (70%) and a testing set (30%)\n",
        "*   wake/NREM/REM classification: dataset was randomly split 20 times at the same training and testing proportions\n",
        "\n",
        "<h4>Leave-one-out cross-validation </h4>\n",
        "Test with every single element in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6xBonnYvq97"
      },
      "source": [
        "#### Training Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxB13VT9vq97"
      },
      "source": [
        "class ClassifierInputBuilder(object):\n",
        "\n",
        "    def get_array(subject_ids, subject_dictionary, feature_set):\n",
        "\n",
        "        all_subjects_features = np.array([])\n",
        "        all_subjects_labels = np.array([])\n",
        "\n",
        "        for subject_id in subject_ids:\n",
        "            subject_features = np.array([])\n",
        "            subject = subject_dictionary[subject_id]\n",
        "            feature_dictionary = subject.feature_dictionary\n",
        "\n",
        "            for feature in feature_set:\n",
        "                feature_data = feature_dictionary[feature]\n",
        "                subject_features = ClassifierInputBuilder.__append_feature(subject_features, feature_data)\n",
        "\n",
        "            all_subjects_features = ClassifierInputBuilder.__stack(all_subjects_features, subject_features)\n",
        "            all_subjects_labels = ClassifierInputBuilder.__stack(all_subjects_labels, subject.labeled_sleep)\n",
        "\n",
        "        return all_subjects_features, all_subjects_labels\n",
        "\n",
        "    def get_sleep_wake_inputs(subject_ids, subject_dictionary, feature_set):\n",
        "        values, raw_labels = ClassifierInputBuilder.get_array(subject_ids, subject_dictionary, feature_set)\n",
        "        processed_labels = SleepLabeler.label_sleep_wake(raw_labels)\n",
        "        return values, processed_labels\n",
        "\n",
        "    def get_three_class_inputs(subject_ids, subject_dictionary, feature_set):\n",
        "        values, raw_labels = ClassifierInputBuilder.get_array(subject_ids, subject_dictionary, feature_set)\n",
        "        processed_labels = SleepLabeler.label_three_class(raw_labels)\n",
        "        return values, processed_labels\n",
        "\n",
        "    def __append_feature(array, feature):\n",
        "        if len(np.shape(feature)) < 2:\n",
        "            feature = np.transpose([feature])\n",
        "        if np.shape(array)[0] == 0:\n",
        "            array = feature\n",
        "        else:\n",
        "            array = np.hstack((array, feature))\n",
        "\n",
        "        return array\n",
        "\n",
        "    def __stack(combined_array, new_array):\n",
        "        if np.shape(combined_array)[0] == 0:\n",
        "            combined_array = new_array\n",
        "        else:\n",
        "            combined_array = np.vstack((combined_array, new_array))\n",
        "        return combined_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbmfuP47vq97"
      },
      "source": [
        "#### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkWb28Wkvq97"
      },
      "source": [
        "class ClassifierService(object):\n",
        "\n",
        "    def run_sw(data_splits, classifier, subject_dictionary, feature_set):\n",
        "        return ClassifierService.run_in_parallel(ClassifierService.run_single_data_split_sw,\n",
        "                                                 data_splits, classifier,\n",
        "                                                 subject_dictionary, feature_set)\n",
        "\n",
        "    def run_three_class(data_splits, classifier, subject_dictionary, feature_set):\n",
        "        return ClassifierService.run_in_parallel(ClassifierService.run_single_data_split_three_class,\n",
        "                                                 data_splits, classifier,\n",
        "                                                 subject_dictionary, feature_set)\n",
        "\n",
        "    def run_three_class_with_loaded_model(data_splits, classifier, subject_dictionary, feature_set):\n",
        "\n",
        "        raw_performances = []\n",
        "        for ind in range(len(data_splits)):\n",
        "            data_split = data_splits[ind]\n",
        "            if ind == 0:\n",
        "                training_x, training_y = ClassifierInputBuilder.get_three_class_inputs(data_split.training_set,\n",
        "                                                                                       subject_dictionary=subject_dictionary,\n",
        "                                                                                       feature_set=feature_set)\n",
        "                classifier = ClassifierService.train_classifier(training_x, training_y, classifier, 'neg_log_loss')\n",
        "\n",
        "            testing_x, testing_y = ClassifierInputBuilder.get_three_class_inputs(data_split.testing_set,\n",
        "                                                                                 subject_dictionary=subject_dictionary,\n",
        "                                                                                 feature_set=feature_set)\n",
        "            class_probabilities = classifier.predict_proba(testing_x)\n",
        "\n",
        "            raw_performance = RawPerformance(true_labels=testing_y, class_probabilities=class_probabilities)\n",
        "            raw_performances.append(raw_performance)\n",
        "\n",
        "        return raw_performances\n",
        "\n",
        "    def run_in_parallel(function, data_splits, classifier, subject_dictionary, feature_set):\n",
        "        pool = Pool(cpu_count())\n",
        "\n",
        "        single_run_wrapper = partial(function,\n",
        "                                     attributed_classifier=classifier,\n",
        "                                     subject_dictionary=subject_dictionary,\n",
        "                                     feature_set=feature_set)\n",
        "\n",
        "        results = pool.map(single_run_wrapper, data_splits)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def run_single_data_split_sw(data_split, attributed_classifier, subject_dictionary, feature_set):\n",
        "\n",
        "        training_x, training_y = ClassifierInputBuilder.get_sleep_wake_inputs(data_split.training_set,\n",
        "                                                                              subject_dictionary=subject_dictionary,\n",
        "                                                                              feature_set=feature_set)\n",
        "        testing_x, testing_y = ClassifierInputBuilder.get_sleep_wake_inputs(data_split.testing_set,\n",
        "                                                                            subject_dictionary=subject_dictionary,\n",
        "                                                                            feature_set=feature_set)\n",
        "\n",
        "        return ClassifierService.run_single_data_split(training_x, training_y, testing_x, testing_y,\n",
        "                                                       attributed_classifier)\n",
        "\n",
        "    def run_single_data_split_three_class(data_split, attributed_classifier, subject_dictionary, feature_set):\n",
        "\n",
        "        training_x, training_y = ClassifierInputBuilder.get_three_class_inputs(data_split.training_set,\n",
        "                                                                               subject_dictionary=subject_dictionary,\n",
        "                                                                               feature_set=feature_set)\n",
        "        testing_x, testing_y = ClassifierInputBuilder.get_three_class_inputs(data_split.testing_set,\n",
        "                                                                             subject_dictionary=subject_dictionary,\n",
        "                                                                             feature_set=feature_set)\n",
        "        return ClassifierService.run_single_data_split(training_x, training_y, testing_x, testing_y,\n",
        "                                                       attributed_classifier, 'neg_log_loss')\n",
        "\n",
        "    def run_single_data_split(training_x, training_y, testing_x, testing_y, attributed_classifier, scoring='roc_auc'):\n",
        "        start_time = time.time()\n",
        "\n",
        "        classifier = ClassifierService.train_classifier(training_x, training_y, attributed_classifier, scoring)\n",
        "        class_probabilities = classifier.predict_proba(testing_x)\n",
        "\n",
        "        raw_performance = RawPerformance(true_labels=testing_y, class_probabilities=class_probabilities)\n",
        "\n",
        "        if Constants.VERBOSE:\n",
        "            print('Completed data split in ' + str(time.time() - start_time))\n",
        "\n",
        "        return raw_performance\n",
        "\n",
        "    def train_classifier(training_x, training_y, attributed_classifier, scoring='roc_auc'):\n",
        "        classifier = attributed_classifier.classifier\n",
        "\n",
        "        classifier.class_weight = ClassifierService.get_class_weights(training_y)\n",
        "        parameters = ParameterSearch.run_search(attributed_classifier, training_x, training_y, scoring=scoring)\n",
        "        classifier.set_params(**parameters)\n",
        "        classifier.fit(training_x, training_y)\n",
        "        \n",
        "        return classifier\n",
        "\n",
        "    def get_class_weights(training_y):\n",
        "        class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                          np.unique(training_y),\n",
        "                                                          training_y)\n",
        "        class_weight_dict = {}\n",
        "\n",
        "        if len(class_weights) == 2:\n",
        "            class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "        if len(class_weights) == 3:\n",
        "            class_weight_dict = {0: class_weights[0], 1: class_weights[1], 2: class_weights[2]}\n",
        "\n",
        "        return class_weight_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA3ekcr8vq98"
      },
      "source": [
        "#### Training Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKMfmX0vq98"
      },
      "source": [
        "class SleepWakeClassifierSummaryBuilder(object):\n",
        "\n",
        "    def build_monte_carlo(attributed_classifier: AttributedClassifier, feature_sets: [[FeatureType]],\n",
        "                          number_of_splits: int) -> ClassifierSummary:\n",
        "        subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "        subject_dictionary = SubjectBuilder.get_subject_dictionary()\n",
        "\n",
        "        data_splits = TrainTestSplitter.by_fraction(subject_ids, test_fraction=0.3, number_of_splits=number_of_splits)\n",
        "\n",
        "        return SleepWakeClassifierSummaryBuilder.run_feature_sets(data_splits, subject_dictionary,\n",
        "                                                                  attributed_classifier,\n",
        "                                                                  feature_sets)\n",
        "\n",
        "    def build_leave_one_out(attributed_classifier: AttributedClassifier,\n",
        "                            feature_sets: [[FeatureType]]) -> ClassifierSummary:\n",
        "        subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "        subject_dictionary = SubjectBuilder.get_subject_dictionary()\n",
        "\n",
        "        data_splits = TrainTestSplitter.leave_one_out(subject_ids)\n",
        "\n",
        "        return SleepWakeClassifierSummaryBuilder.run_feature_sets(data_splits, subject_dictionary,\n",
        "                                                                  attributed_classifier,\n",
        "                                                                  feature_sets)\n",
        "\n",
        "    def run_feature_sets(data_splits: [DataSplit], subject_dictionary, attributed_classifier: AttributedClassifier,\n",
        "                         feature_sets: [[FeatureType]]):\n",
        "        performance_dictionary = {}\n",
        "        for feature_set in feature_sets:\n",
        "            raw_performance_results = ClassifierService.run_sw(data_splits, attributed_classifier,\n",
        "                                                               subject_dictionary, feature_set)\n",
        "            performance_dictionary[tuple(feature_set)] = raw_performance_results\n",
        "\n",
        "        return ClassifierSummary(attributed_classifier, performance_dictionary)\n",
        "\n",
        "    def build_mesa(attributed_classifier: AttributedClassifier, feature_sets: [[FeatureType]]):\n",
        "        apple_watch_subjects = SubjectBuilder.get_subject_dictionary()\n",
        "        mesa_subjects = MesaDataService.get_all_subjects()\n",
        "        training_set = []\n",
        "        testing_set = []\n",
        "        mesa_dictionary = {}\n",
        "\n",
        "        for subject_key in apple_watch_subjects:\n",
        "            training_set.append(subject_key)\n",
        "\n",
        "        for mesa_subject in mesa_subjects:\n",
        "            mesa_subject.subject_id = 'mesa' + mesa_subject.subject_id\n",
        "            testing_set.append(mesa_subject.subject_id)\n",
        "            mesa_dictionary[mesa_subject.subject_id] = mesa_subject\n",
        "\n",
        "        data_split = DataSplit(training_set=training_set, testing_set=testing_set)\n",
        "        apple_watch_subjects.update(mesa_dictionary)\n",
        "\n",
        "        return SleepWakeClassifierSummaryBuilder.run_feature_sets([data_split], apple_watch_subjects,\n",
        "                                                                  attributed_classifier,\n",
        "                                                                  feature_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr3hsRMjvq98"
      },
      "source": [
        "class ThreeClassClassifierSummaryBuilder(object):\n",
        "\n",
        "    def build_monte_carlo(attributed_classifier: AttributedClassifier, feature_sets: [[FeatureType]],\n",
        "                          number_of_splits: int) -> ClassifierSummary:\n",
        "        subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "        subject_dictionary = SubjectBuilder.get_subject_dictionary()\n",
        "\n",
        "        data_splits = TrainTestSplitter.by_fraction(subject_ids, test_fraction=0.3, number_of_splits=number_of_splits)\n",
        "\n",
        "        return ThreeClassClassifierSummaryBuilder.run_feature_sets(data_splits, subject_dictionary,\n",
        "                                                                   attributed_classifier,\n",
        "                                                                   feature_sets)\n",
        "\n",
        "    def build_leave_one_out(attributed_classifier: AttributedClassifier,\n",
        "                            feature_sets: [[FeatureType]]) -> ClassifierSummary:\n",
        "        subject_ids = SubjectBuilder.get_all_subject_ids()\n",
        "        subject_dictionary = SubjectBuilder.get_subject_dictionary()\n",
        "\n",
        "        data_splits = TrainTestSplitter.leave_one_out(subject_ids)\n",
        "\n",
        "        return ThreeClassClassifierSummaryBuilder.run_feature_sets(data_splits, subject_dictionary,\n",
        "                                                                   attributed_classifier,\n",
        "                                                                   feature_sets)\n",
        "\n",
        "    def run_feature_sets(data_splits: [DataSplit], subject_dictionary, attributed_classifier: AttributedClassifier,\n",
        "                         feature_sets: [[FeatureType]], use_preloaded=False):\n",
        "        performance_dictionary = {}\n",
        "        for feature_set in feature_sets:\n",
        "            if use_preloaded:\n",
        "                raw_performance_results = ClassifierService.run_three_class_with_loaded_model(data_splits,\n",
        "                                                                                              attributed_classifier,\n",
        "                                                                                              subject_dictionary,\n",
        "                                                                                              feature_set)\n",
        "            else:\n",
        "                raw_performance_results = ClassifierService.run_three_class(data_splits, attributed_classifier,\n",
        "                                                                            subject_dictionary, feature_set)\n",
        "            performance_dictionary[tuple(feature_set)] = raw_performance_results\n",
        "\n",
        "        return ClassifierSummary(attributed_classifier, performance_dictionary)\n",
        "\n",
        "    def build_mesa_leave_one_out(attributed_classifier: AttributedClassifier, feature_sets: [[FeatureType]]):\n",
        "        apple_watch_subjects = SubjectBuilder.get_subject_dictionary()\n",
        "        mesa_subjects = MesaDataService.get_all_subjects()\n",
        "        training_set = []\n",
        "        mesa_dictionary = {}\n",
        "        data_splits = []\n",
        "\n",
        "        for subject_key in apple_watch_subjects:\n",
        "            training_set.append(subject_key)\n",
        "\n",
        "        for mesa_subject in mesa_subjects:\n",
        "            mesa_subject.subject_id = 'mesa' + mesa_subject.subject_id\n",
        "            mesa_dictionary[mesa_subject.subject_id] = mesa_subject\n",
        "            testing_set = [mesa_subject.subject_id]\n",
        "            data_split = DataSplit(training_set=training_set, testing_set=testing_set)\n",
        "            data_splits.append(data_split)\n",
        "\n",
        "        apple_watch_subjects.update(mesa_dictionary)\n",
        "\n",
        "        return ThreeClassClassifierSummaryBuilder.run_feature_sets(data_splits, apple_watch_subjects,\n",
        "                                                                   attributed_classifier,\n",
        "                                                                   feature_sets, True)\n",
        "\n",
        "    def build_mesa_all_combined(attributed_classifier: AttributedClassifier, feature_sets: [[FeatureType]]):\n",
        "        apple_watch_subjects = SubjectBuilder.get_subject_dictionary()\n",
        "        mesa_subjects = MesaDataService.get_all_subjects()\n",
        "        training_set = []\n",
        "        testing_set = []\n",
        "        mesa_dictionary = {}\n",
        "\n",
        "        for subject_key in apple_watch_subjects:\n",
        "            training_set.append(subject_key)\n",
        "\n",
        "        for mesa_subject in mesa_subjects:\n",
        "            mesa_subject.subject_id = 'mesa' + mesa_subject.subject_id\n",
        "            mesa_dictionary[mesa_subject.subject_id] = mesa_subject\n",
        "            testing_set.append(mesa_subject.subject_id)\n",
        "\n",
        "        data_split = DataSplit(training_set=training_set, testing_set=testing_set)\n",
        "        apple_watch_subjects.update(mesa_dictionary)\n",
        "\n",
        "        return ThreeClassClassifierSummaryBuilder.run_feature_sets([data_split], apple_watch_subjects,\n",
        "                                                                   attributed_classifier,\n",
        "                                                                   feature_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92eO90a6FRW4"
      },
      "source": [
        "## Testing Model & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e49Fkigvq98"
      },
      "source": [
        "### Sleep Metrice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzp0I7LMvq98"
      },
      "source": [
        "class SleepMetricsCalculator(object):\n",
        "\n",
        "    def get_tst(labels):\n",
        "        sleep_epoch_indices = np.where(labels > 0)\n",
        "        tst = np.shape(sleep_epoch_indices)[1]\n",
        "        return tst * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "\n",
        "    def get_wake_after_sleep_onset(labels):\n",
        "        sleep_indices = np.argwhere(labels > 0)\n",
        "        if np.shape(sleep_indices)[0] > 0:\n",
        "            sol_index = np.amin(sleep_indices)\n",
        "            indices_where_wake_occurred = np.where(labels == 0)\n",
        "\n",
        "            waso_indices = np.where(indices_where_wake_occurred > sol_index)\n",
        "            waso_indices = waso_indices[1]\n",
        "            number_waso_indices = np.shape(waso_indices)[0]\n",
        "            return number_waso_indices * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "        else:\n",
        "            return len(labels) * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "\n",
        "    def get_sleep_efficiency(labels):\n",
        "        sleep_indices = np.where(labels > 0)\n",
        "        sleep_efficiency = float(np.shape(sleep_indices)[1]) / float(np.shape(labels)[0])\n",
        "        return sleep_efficiency\n",
        "\n",
        "    def get_sleep_onset_latency(labels):\n",
        "        sleep_indices = np.argwhere(labels > 0)\n",
        "        if np.shape(sleep_indices)[0] > 0:\n",
        "            return np.amin(sleep_indices) * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "        else:\n",
        "            return len(labels) * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "\n",
        "    def get_time_in_rem(labels):\n",
        "        rem_epoch_indices = np.where(labels == 2)\n",
        "        rem_time = np.shape(rem_epoch_indices)[1]\n",
        "        return rem_time * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE\n",
        "\n",
        "    def get_time_in_nrem(labels):\n",
        "        rem_epoch_indices = np.where(labels == 1)\n",
        "        rem_time = np.shape(rem_epoch_indices)[1]\n",
        "        return rem_time * Constants.EPOCH_DURATION_IN_SECONDS / Constants.SECONDS_PER_MINUTE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mehohzjJFRW4"
      },
      "source": [
        "### Evaluation / Performance Metrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBMmiykGoz8U"
      },
      "source": [
        "* Receiver Operating Characteristic (ROC) curves: varying a threshold parameter and plotting the true positive and false-positive rates at all thresholds against each other \n",
        "  \n",
        "  ***ROC => higher is better  ***\n",
        "\n",
        "* Precision-recall curves: recall (on the x-axis) is the fraction of wake epochs scored correctly, and the precision (on the y-axis) shows the fraction of all epochs labeled wake that were truly wake\n",
        "\n",
        "* AUC: area under curve of ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIbCNbQrvq99"
      },
      "source": [
        "class PerformanceBuilder(object):\n",
        "\n",
        "    def build_with_sleep_threshold(raw_performance, sleep_threshold):\n",
        "\n",
        "        if np.shape(raw_performance.class_probabilities)[1] > 2:\n",
        "            raw_performance = SleepLabeler.convert_three_class_to_two(raw_performance)\n",
        "\n",
        "        false_positive_rates, true_positive_rates, thresholds = roc_curve(raw_performance.true_labels,\n",
        "                                                                          raw_performance.class_probabilities[:,\n",
        "                                                                          SleepWakeLabel.sleep.value],\n",
        "                                                                          pos_label=SleepWakeLabel.sleep.value,\n",
        "                                                                          drop_intermediate=False)\n",
        "        auc_value = auc(false_positive_rates, true_positive_rates)\n",
        "\n",
        "        predicted_labels = PerformanceBuilder.apply_threshold_sleep_wake(raw_performance, sleep_threshold)\n",
        "\n",
        "        kappa = cohen_kappa_score(raw_performance.true_labels, predicted_labels)\n",
        "\n",
        "        accuracy = accuracy_score(raw_performance.true_labels, predicted_labels)\n",
        "        wake_correct = recall_score(raw_performance.true_labels, predicted_labels, pos_label=SleepWakeLabel.wake.value)\n",
        "        sleep_correct = recall_score(raw_performance.true_labels, predicted_labels,\n",
        "                                     pos_label=SleepWakeLabel.sleep.value)\n",
        "        sleep_predictive_value = precision_score(raw_performance.true_labels, predicted_labels,\n",
        "                                                 pos_label=SleepWakeLabel.sleep.value)\n",
        "        wake_predictive_value = precision_score(raw_performance.true_labels, predicted_labels,\n",
        "                                                pos_label=SleepWakeLabel.wake.value)\n",
        "\n",
        "        return SleepWakePerformance(accuracy=accuracy,\n",
        "                                    wake_correct=wake_correct,\n",
        "                                    sleep_correct=sleep_correct,\n",
        "                                    kappa=kappa,\n",
        "                                    auc=auc_value,\n",
        "                                    sleep_predictive_value=sleep_predictive_value,\n",
        "                                    wake_predictive_value=wake_predictive_value)\n",
        "\n",
        "    def build_with_true_positive_rate_threshold(raw_performance, true_positive_threshold):\n",
        "        false_positive_rates, true_positive_rates, thresholds = \\\n",
        "            roc_curve(raw_performance.true_labels,\n",
        "                      raw_performance.class_probabilities[:, SleepWakeLabel.sleep.value],\n",
        "                      pos_label=SleepWakeLabel.sleep.value,\n",
        "                      drop_intermediate=False)\n",
        "\n",
        "        sleep_threshold = np.interp(true_positive_threshold, true_positive_rates, thresholds)\n",
        "\n",
        "        performance = PerformanceBuilder.build_with_sleep_threshold(raw_performance, sleep_threshold)\n",
        "\n",
        "        return performance\n",
        "\n",
        "    def apply_threshold_sleep_wake(raw_performance, sleep_threshold):\n",
        "        predicted_labels = []\n",
        "\n",
        "        number_of_samples = np.shape(raw_performance.class_probabilities)[0]\n",
        "        for index in range(number_of_samples):\n",
        "            if raw_performance.class_probabilities[index, 1] >= sleep_threshold:\n",
        "                predicted_labels.append(SleepWakeLabel.sleep.value)\n",
        "            else:\n",
        "                predicted_labels.append(SleepWakeLabel.wake.value)\n",
        "\n",
        "        return np.array(predicted_labels)\n",
        "\n",
        "    def apply_threshold_three_class(raw_performance, wake_threshold, rem_threshold):\n",
        "        predicted_labels = []\n",
        "\n",
        "        number_of_samples = np.shape(raw_performance.class_probabilities)[0]\n",
        "        for index in range(number_of_samples):\n",
        "            if raw_performance.class_probabilities[index, 0] >= wake_threshold:\n",
        "                predicted_labels.append(ThreeClassLabel.wake.value)\n",
        "            else:\n",
        "                if raw_performance.class_probabilities[index, 2] >= rem_threshold:\n",
        "                    predicted_labels.append(ThreeClassLabel.rem.value)\n",
        "                else:\n",
        "                    predicted_labels.append(ThreeClassLabel.nrem.value)\n",
        "\n",
        "        return np.array(predicted_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJD12s2ivq99"
      },
      "source": [
        "class PerformanceSummarizer(object):\n",
        "\n",
        "    def average(sleep_wake_performance_array: [SleepWakePerformance]):\n",
        "        average_performance = SleepWakePerformance(\n",
        "            accuracy=0,\n",
        "            wake_correct=0,\n",
        "            sleep_correct=0,\n",
        "            auc=0,\n",
        "            kappa=0,\n",
        "            wake_predictive_value=0,\n",
        "            sleep_predictive_value=0\n",
        "        )\n",
        "\n",
        "        for sleep_wake_performance in sleep_wake_performance_array:\n",
        "            average_performance.accuracy += sleep_wake_performance.accuracy\n",
        "            average_performance.wake_correct += sleep_wake_performance.wake_correct\n",
        "            average_performance.sleep_correct += sleep_wake_performance.sleep_correct\n",
        "            average_performance.auc += sleep_wake_performance.auc\n",
        "            average_performance.kappa += sleep_wake_performance.kappa\n",
        "            average_performance.wake_predictive_value += sleep_wake_performance.wake_predictive_value\n",
        "            average_performance.sleep_predictive_value += sleep_wake_performance.sleep_predictive_value\n",
        "\n",
        "        number_of_performance_models = len(sleep_wake_performance_array)\n",
        "\n",
        "        average_performance.accuracy = PerformanceSummarizer.__calculate_average(average_performance.accuracy,\n",
        "                                                                                 number_of_performance_models)\n",
        "\n",
        "        average_performance.wake_correct = PerformanceSummarizer.__calculate_average(average_performance.wake_correct,\n",
        "                                                                                     number_of_performance_models)\n",
        "\n",
        "        average_performance.sleep_correct = PerformanceSummarizer.__calculate_average(\n",
        "            average_performance.sleep_correct,\n",
        "            number_of_performance_models)\n",
        "\n",
        "        average_performance.auc = PerformanceSummarizer.__calculate_average(average_performance.auc,\n",
        "                                                                            number_of_performance_models)\n",
        "\n",
        "        average_performance.kappa = PerformanceSummarizer.__calculate_average(average_performance.kappa,\n",
        "                                                                              number_of_performance_models)\n",
        "\n",
        "        average_performance.wake_predictive_value = PerformanceSummarizer.__calculate_average(\n",
        "            average_performance.wake_predictive_value,\n",
        "            number_of_performance_models)\n",
        "\n",
        "        average_performance.sleep_predictive_value = PerformanceSummarizer.__calculate_average(\n",
        "            average_performance.sleep_predictive_value,\n",
        "            number_of_performance_models)\n",
        "\n",
        "        return average_performance\n",
        "\n",
        "    def average_three_class(three_class_performance_array: [ThreeClassPerformance]):\n",
        "        average_performance = ThreeClassPerformance(accuracy=0,\n",
        "                                                    wake_correct=0,\n",
        "                                                    rem_correct=0,\n",
        "                                                    nrem_correct=0,\n",
        "                                                    kappa=0)\n",
        "\n",
        "        for three_class_performance in three_class_performance_array:\n",
        "            average_performance.accuracy += three_class_performance.accuracy\n",
        "            average_performance.wake_correct += three_class_performance.wake_correct\n",
        "            average_performance.rem_correct += three_class_performance.rem_correct\n",
        "            average_performance.nrem_correct += three_class_performance.nrem_correct\n",
        "            average_performance.kappa += three_class_performance.kappa\n",
        "\n",
        "        number_of_performance_models = len(three_class_performance_array)\n",
        "\n",
        "        average_performance.accuracy = PerformanceSummarizer.__calculate_average(average_performance.accuracy,\n",
        "                                                                                 number_of_performance_models)\n",
        "\n",
        "        average_performance.wake_correct = PerformanceSummarizer.__calculate_average(average_performance.wake_correct,\n",
        "                                                                                     number_of_performance_models)\n",
        "\n",
        "        average_performance.rem_correct = PerformanceSummarizer.__calculate_average(\n",
        "            average_performance.rem_correct,\n",
        "            number_of_performance_models)\n",
        "\n",
        "        average_performance.nrem_correct = PerformanceSummarizer.__calculate_average(average_performance.nrem_correct,\n",
        "                                                                                     number_of_performance_models)\n",
        "\n",
        "        average_performance.kappa = PerformanceSummarizer.__calculate_average(average_performance.kappa,\n",
        "                                                                              number_of_performance_models)\n",
        "\n",
        "        return average_performance\n",
        "\n",
        "    def summarize_thresholds(raw_performances: [RawPerformance]):\n",
        "        performance_summaries = []\n",
        "\n",
        "        true_positive_thresholds = [0.8, 0.9, 0.93, 0.95]\n",
        "        for fixed_true_positive_rate in true_positive_thresholds:\n",
        "\n",
        "            performances = []\n",
        "            for raw_performance in raw_performances:\n",
        "                performance = PerformanceBuilder.build_with_true_positive_rate_threshold(raw_performance,\n",
        "                                                                                         fixed_true_positive_rate)\n",
        "                performances.append(performance)\n",
        "\n",
        "            performance_summary = PerformanceSummarizer.average(performances)\n",
        "            performance_summaries.append(performance_summary)\n",
        "\n",
        "        return true_positive_thresholds, performance_summaries\n",
        "\n",
        "    def apply_single_threshold(raw_performances: [RawPerformance], sleep_threshold):\n",
        "        performances = []\n",
        "\n",
        "        for raw_performance in raw_performances:\n",
        "            performance = PerformanceBuilder.build_with_sleep_threshold(raw_performance, sleep_threshold)\n",
        "            performances.append(performance)\n",
        "\n",
        "        return performances\n",
        "\n",
        "    def __calculate_average(value, count):\n",
        "        return value / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHrKGStOvq99"
      },
      "source": [
        "class CurvePerformanceBuilder(object):\n",
        "    NUMBER_OF_INTERPOLATION_POINT = 100\n",
        "\n",
        "    def get_axes_bins():\n",
        "        x_axis = []\n",
        "\n",
        "        for i in range(0, CurvePerformanceBuilder.NUMBER_OF_INTERPOLATION_POINT):\n",
        "            x_axis.append((i + 1) / (CurvePerformanceBuilder.NUMBER_OF_INTERPOLATION_POINT * 1.0))\n",
        "\n",
        "        x_axis = np.array(x_axis)\n",
        "        y_axis = np.zeros(np.shape(x_axis))\n",
        "\n",
        "        return x_axis, y_axis\n",
        "\n",
        "    def build_roc_from_raw(raw_performances: [RawPerformance], positive_class):\n",
        "        false_positive_spread, true_positive_spread = CurvePerformanceBuilder.get_axes_bins()\n",
        "        count = 0\n",
        "\n",
        "        for raw_performance in raw_performances:\n",
        "            true_labels = SleepLabeler.label_one_vs_rest(raw_performance.true_labels, positive_class)\n",
        "\n",
        "            false_positive_rates, true_positive_rates, thresholds = roc_curve(\n",
        "                true_labels,\n",
        "                raw_performance.class_probabilities[:, positive_class],\n",
        "                pos_label=SleepWakeLabel.sleep.value,\n",
        "                drop_intermediate=False)\n",
        "            count = count + 1\n",
        "            true_positive_spread += np.interp(false_positive_spread, false_positive_rates,\n",
        "                                              true_positive_rates)\n",
        "\n",
        "        true_positive_spread = true_positive_spread / count\n",
        "\n",
        "        false_positive_spread = np.insert(false_positive_spread, 0, 0)\n",
        "        true_positive_spread = np.insert(true_positive_spread, 0, 0)\n",
        "\n",
        "        return ROCPerformance(false_positive_rates=false_positive_spread, true_positive_rates=true_positive_spread)\n",
        "\n",
        "    def build_precision_recall_from_raw(raw_performances: [RawPerformance]):\n",
        "        recall_spread, precision_spread = CurvePerformanceBuilder.get_axes_bins()\n",
        "        count = 0\n",
        "\n",
        "        for raw_performance in raw_performances:\n",
        "            count = count + 1\n",
        "            precisions, recalls, thresholds = precision_recall_curve(raw_performance.true_labels,\n",
        "                                                                     raw_performance.class_probabilities[:, 0],\n",
        "                                                                     pos_label=SleepWakeLabel.wake.value)\n",
        "            precision_spread += np.interp(recall_spread, np.flip(recalls), np.flip(precisions))\n",
        "\n",
        "        precision_spread = precision_spread / count\n",
        "\n",
        "        recall_spread = np.array(np.insert(recall_spread, 0, 0))\n",
        "        precision_spread = np.array(np.insert(precision_spread, 0, 1))\n",
        "\n",
        "        return PrecisionRecallPerformance(recalls=recall_spread, precisions=precision_spread)\n",
        "\n",
        "    def build_three_class_roc_with_binary_search(raw_performances: [RawPerformance]):\n",
        "        ''' This function runs two binary searches-- first, to match a given wake accuracy value,\n",
        "        and second, the try to find a threshold that balances the NREM and REM class accuracies '''\n",
        "        three_class_performances = []\n",
        "\n",
        "        number_of_wake_scored_as_sleep_bins = 20  # Bin size for the x-dimension of this plot\n",
        "        false_positive_buffer = 0.001  # How close to the target wake false positive rate we need to be before stopping\n",
        "        max_attempts_binary_search_wake = 50  # Max number of times the search will try to find a given wake accuracy\n",
        "        rem_nrem_accuracy_tolerance = 1e-2  # How close the NREM and REM class accuracies need to be before stopping\n",
        "        max_attempts_binary_search_rem_nrem = 15  # Max times the search will try to balance REM and NREM accuracies\n",
        "        wake_scored_as_sleep_interpolation_point = 0.4  # Point to report values in the table for\n",
        "        goal_fraction_wake_scored_as_sleep_spread = []  # Wake false positive rate spread (x-axis of plot)\n",
        "\n",
        "        #  Initialize holders for wake, NREM, REM accuracies\n",
        "        for i in range(0, number_of_wake_scored_as_sleep_bins):\n",
        "            goal_fraction_wake_scored_as_sleep_spread.append(i / (number_of_wake_scored_as_sleep_bins * 1.0))\n",
        "\n",
        "        goal_fraction_wake_scored_as_sleep_spread = np.array(goal_fraction_wake_scored_as_sleep_spread)\n",
        "        cumulative_nrem_accuracies = np.zeros(np.shape(goal_fraction_wake_scored_as_sleep_spread))\n",
        "        cumulative_rem_accuracies = np.zeros(np.shape(goal_fraction_wake_scored_as_sleep_spread))\n",
        "        cumulative_accuracies = np.zeros(np.shape(goal_fraction_wake_scored_as_sleep_spread))\n",
        "\n",
        "        cumulative_counter = 0\n",
        "\n",
        "        # Loop over all training/testing splits\n",
        "        for raw_performance in raw_performances:\n",
        "\n",
        "            true_labels = raw_performance.true_labels\n",
        "            class_probabilities = raw_performance.class_probabilities\n",
        "\n",
        "            wake_scored_as_sleep_spread = []\n",
        "            sleep_accuracy_spread = []\n",
        "            accuracies = []\n",
        "            kappas = []\n",
        "            nrem_class_accuracies = []\n",
        "            rem_class_accuracies = []\n",
        "\n",
        "            true_wake_indices = np.where(true_labels == 0)[0]\n",
        "            true_nrem_indices = np.where(true_labels == 1)[0]\n",
        "            true_rem_indices = np.where(true_labels == 2)[0]\n",
        "\n",
        "            #  Try to find a threshold that matches a target fraction wake scored as sleep\n",
        "            for goal_fraction_wake_scored_as_sleep in goal_fraction_wake_scored_as_sleep_spread:\n",
        "\n",
        "                fraction_wake_scored_as_sleep = -1\n",
        "                binary_search_counter = 0\n",
        "\n",
        "                # While we haven't found the target wake false positive rate\n",
        "                # (and haven't exceeded the number of allowable searches), keep searching:\n",
        "                while (fraction_wake_scored_as_sleep < goal_fraction_wake_scored_as_sleep - false_positive_buffer\n",
        "                       or fraction_wake_scored_as_sleep >= goal_fraction_wake_scored_as_sleep + false_positive_buffer) \\\n",
        "                        and binary_search_counter < max_attempts_binary_search_wake:\n",
        "\n",
        "                    # If this is the first iteration on the binary search, initialize.\n",
        "                    if binary_search_counter == 0:\n",
        "                        threshold_for_sleep = 0.5\n",
        "                        threshold_delta = 0.25\n",
        "                    else:\n",
        "                        if fraction_wake_scored_as_sleep < goal_fraction_wake_scored_as_sleep - false_positive_buffer:\n",
        "                            threshold_for_sleep = threshold_for_sleep - threshold_delta\n",
        "                            threshold_delta = threshold_delta / 2\n",
        "                        if fraction_wake_scored_as_sleep >= goal_fraction_wake_scored_as_sleep + false_positive_buffer:\n",
        "                            threshold_for_sleep = threshold_for_sleep + threshold_delta\n",
        "                            threshold_delta = threshold_delta / 2\n",
        "\n",
        "                    if goal_fraction_wake_scored_as_sleep == 1:  # Edge cases\n",
        "                        threshold_for_sleep = 0.0\n",
        "                    if goal_fraction_wake_scored_as_sleep == 0:\n",
        "                        threshold_for_sleep = 1.0\n",
        "\n",
        "                    predicted_sleep_indices = np.where(1 - np.array(class_probabilities[:, 0]) >= threshold_for_sleep)[0]\n",
        "\n",
        "                    predicted_labels = np.zeros(np.shape(true_labels))\n",
        "                    predicted_labels[predicted_sleep_indices] = 1\n",
        "                    predicted_labels_at_true_wake_indices = predicted_labels[true_wake_indices]\n",
        "\n",
        "                    number_wake_correct = len(true_wake_indices) - np.count_nonzero(\n",
        "                        predicted_labels_at_true_wake_indices)\n",
        "                    fraction_wake_correct = number_wake_correct / (len(true_wake_indices) * 1.0)\n",
        "                    fraction_wake_scored_as_sleep = 1.0 - fraction_wake_correct\n",
        "\n",
        "                    binary_search_counter = binary_search_counter + 1\n",
        "\n",
        "                # Next, try to find a threshold that balances the REM and NREM class accuracies\n",
        "                if binary_search_counter < max_attempts_binary_search_wake:\n",
        "\n",
        "                    smallest_accuracy_difference = 2\n",
        "                    sleep_accuracy = 0\n",
        "                    rem_accuracy = 0\n",
        "                    nrem_accuracy = 0\n",
        "                    best_accuracy = -1\n",
        "                    kappa_at_best_accuracy = -1\n",
        "\n",
        "                    count_thresh = 0\n",
        "                    threshold_for_rem = 0.5\n",
        "                    threshold_delta_rem = 0.5\n",
        "\n",
        "                    # While we're outside the tolerance window for equalizing REM/NREM accuracies and\n",
        "                    # we haven't exceed the maximum number of search attempts, keep hunting\n",
        "                    while count_thresh < max_attempts_binary_search_rem_nrem and \\\n",
        "                            smallest_accuracy_difference > rem_nrem_accuracy_tolerance:\n",
        "\n",
        "                        count_thresh = count_thresh + 1\n",
        "\n",
        "                        for predicted_sleep_index in range(len(predicted_sleep_indices)):\n",
        "                            predicted_sleep_epoch = predicted_sleep_indices[predicted_sleep_index]\n",
        "\n",
        "                            # Apply the threshold to split REM and NREM sleep\n",
        "                            if class_probabilities[predicted_sleep_epoch, 2] > threshold_for_rem:\n",
        "                                predicted_labels[predicted_sleep_epoch] = 2  # Set to REM sleep\n",
        "                            else:\n",
        "                                predicted_labels[predicted_sleep_epoch] = 1  # Set to NREM sleep\n",
        "\n",
        "                        accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "                        kappa = cohen_kappa_score(predicted_labels, true_labels)\n",
        "\n",
        "                        if accuracy > best_accuracy:\n",
        "                            best_accuracy = accuracy\n",
        "                            kappa_at_best_accuracy = kappa\n",
        "\n",
        "                        # Assess accuracy at the current threshold\n",
        "                        predicted_nrem_indices = np.where(predicted_labels == 1)[0]\n",
        "                        predicted_rem_indices = np.where(predicted_labels == 2)[0]\n",
        "\n",
        "                        correct_nrem_indices = np.intersect1d(predicted_nrem_indices, true_nrem_indices)\n",
        "                        correct_rem_indices = np.intersect1d(predicted_rem_indices, true_rem_indices)\n",
        "\n",
        "                        nrem_accuracy = len(correct_nrem_indices) / (1.0 * len(true_nrem_indices))\n",
        "\n",
        "                        if len(true_rem_indices) > 0:\n",
        "                            rem_accuracy = len(correct_rem_indices) / (1.0 * len(true_rem_indices))\n",
        "                        else:\n",
        "                            rem_accuracy = 0\n",
        "\n",
        "                        # Update current accuracy difference and adjust the threshold for REM sleep\n",
        "                        sleep_accuracy = (len(correct_nrem_indices) + len(correct_rem_indices)) / (\n",
        "                                1.0 * len(true_nrem_indices) + 1.0 * len(true_rem_indices))\n",
        "\n",
        "                        smallest_accuracy_difference = np.abs(nrem_accuracy - rem_accuracy)\n",
        "\n",
        "                        if rem_accuracy < nrem_accuracy:\n",
        "                            threshold_for_rem = threshold_for_rem - threshold_delta_rem / 2.0\n",
        "                        else:\n",
        "                            threshold_for_rem = threshold_for_rem + threshold_delta_rem / 2.0\n",
        "\n",
        "                        threshold_delta_rem = threshold_delta_rem / 2.0\n",
        "\n",
        "                    wake_scored_as_sleep_spread.append(fraction_wake_scored_as_sleep)\n",
        "                    sleep_accuracy_spread.append(sleep_accuracy)\n",
        "                    nrem_class_accuracies.append(nrem_accuracy)\n",
        "                    rem_class_accuracies.append(rem_accuracy)\n",
        "                    accuracies.append(best_accuracy)\n",
        "                    kappas.append(kappa_at_best_accuracy)\n",
        "\n",
        "            wake_scored_as_sleep_spread = np.array(wake_scored_as_sleep_spread)\n",
        "            sleep_accuracy_spread = np.array(sleep_accuracy_spread)\n",
        "            nrem_class_accuracies = np.array(nrem_class_accuracies)\n",
        "            rem_class_accuracies = np.array(rem_class_accuracies)\n",
        "\n",
        "            wake_scored_as_sleep_spread = np.insert(wake_scored_as_sleep_spread, 0, 0)\n",
        "            sleep_accuracy_spread = np.insert(sleep_accuracy_spread, 0, 0)\n",
        "            nrem_class_accuracies = np.insert(nrem_class_accuracies, 0, 0)\n",
        "            rem_class_accuracies = np.insert(rem_class_accuracies, 0, 0)\n",
        "\n",
        "            index_of_best_accuracy = np.argmax(accuracies)\n",
        "\n",
        "            accuracy = accuracies[index_of_best_accuracy]\n",
        "            kappa = kappas[index_of_best_accuracy]\n",
        "\n",
        "            # Interpolate and add the vectors to our running totals\n",
        "            cumulative_accuracies = cumulative_accuracies + np.interp(goal_fraction_wake_scored_as_sleep_spread,\n",
        "                                                                      wake_scored_as_sleep_spread,\n",
        "                                                                      sleep_accuracy_spread)\n",
        "\n",
        "            cumulative_nrem_accuracies = cumulative_nrem_accuracies + np.interp(\n",
        "                goal_fraction_wake_scored_as_sleep_spread,\n",
        "                wake_scored_as_sleep_spread,\n",
        "                nrem_class_accuracies)\n",
        "\n",
        "            cumulative_rem_accuracies = cumulative_rem_accuracies + np.interp(goal_fraction_wake_scored_as_sleep_spread,\n",
        "                                                                              wake_scored_as_sleep_spread,\n",
        "                                                                              rem_class_accuracies)\n",
        "\n",
        "            cumulative_counter = cumulative_counter + 1\n",
        "\n",
        "            rem_correct = np.interp(wake_scored_as_sleep_interpolation_point,\n",
        "                                    wake_scored_as_sleep_spread,\n",
        "                                    rem_class_accuracies)\n",
        "            nrem_correct = np.interp(wake_scored_as_sleep_interpolation_point,\n",
        "                                     wake_scored_as_sleep_spread,\n",
        "                                     nrem_class_accuracies)\n",
        "\n",
        "            three_class_performance = ThreeClassPerformance(accuracy=accuracy,\n",
        "                                                            wake_correct=1 - wake_scored_as_sleep_interpolation_point,\n",
        "                                                            rem_correct=rem_correct,\n",
        "                                                            nrem_correct=nrem_correct,\n",
        "                                                            kappa=kappa)\n",
        "            three_class_performances.append(three_class_performance)\n",
        "\n",
        "        # Average over all the train/test splits\n",
        "        cumulative_accuracies = cumulative_accuracies / cumulative_counter\n",
        "        cumulative_nrem_accuracies = cumulative_nrem_accuracies / cumulative_counter\n",
        "        cumulative_rem_accuracies = cumulative_rem_accuracies / cumulative_counter\n",
        "\n",
        "        sleep_wake_roc_performance = ROCPerformance(false_positive_rates=goal_fraction_wake_scored_as_sleep_spread,\n",
        "                                                    true_positive_rates=cumulative_accuracies)\n",
        "\n",
        "        rem_roc_performance = ROCPerformance(false_positive_rates=goal_fraction_wake_scored_as_sleep_spread,\n",
        "                                             true_positive_rates=cumulative_rem_accuracies)\n",
        "\n",
        "        nrem_roc_performance = ROCPerformance(false_positive_rates=goal_fraction_wake_scored_as_sleep_spread,\n",
        "                                              true_positive_rates=cumulative_nrem_accuracies)\n",
        "\n",
        "        return sleep_wake_roc_performance, rem_roc_performance, nrem_roc_performance, three_class_performances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyxK_kEFFRW4"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYRr_hafvq9-"
      },
      "source": [
        "#### Table Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIez2F8nvq9-"
      },
      "source": [
        "class TableBuilder(object):\n",
        "    pass\n",
        "\n",
        "    def print_table_sw(classifier_summary):\n",
        "        performance_dictionary = classifier_summary.performance_dictionary\n",
        "        attributed_classifier = classifier_summary.attributed_classifier\n",
        "\n",
        "        frontmatter = '\\\\begin{table}  \\\\caption{Sleep/wake differentiation performance by ' \\\n",
        "                      + attributed_classifier.name \\\n",
        "                      + ' across different feature inputs in the Apple Watch (PPG, MEMS) dataset} ' \\\n",
        "                        '\\\\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) ' \\\n",
        "                        '& Sleep correct (sensitivity) & $\\\\kappa$ & AUC \\\\\\\\ '\n",
        "        print(frontmatter)\n",
        "\n",
        "        for feature_set in performance_dictionary:\n",
        "            raw_performances: [RawPerformance] = performance_dictionary[feature_set]\n",
        "            \n",
        "            print('\\\\hline ' + FeatureSetService.get_label(list(feature_set)) + ' &')\n",
        "\n",
        "            true_positive_thresholds, performance_summaries = PerformanceSummarizer.summarize_thresholds(\n",
        "                raw_performances)\n",
        "\n",
        "            for true_positive_threshold, performance in zip(true_positive_thresholds, performance_summaries):\n",
        "                if true_positive_threshold == 0.8:\n",
        "                    print(\n",
        "                        str(round(performance.accuracy, 3)) + ' & ' +\n",
        "                        str(round(performance.wake_correct, 3)) + ' & ' +\n",
        "                        str(round(performance.sleep_correct, 3)) + ' & ' +\n",
        "                        str(round(performance.kappa, 3)) + ' & ' +\n",
        "                        str(round(performance.auc, 3)) + '  \\\\\\\\')\n",
        "                else:\n",
        "                    print('& ' +\n",
        "                          str(round(performance.accuracy, 3)) + ' & ' +\n",
        "                          str(round(performance.wake_correct, 3)) + ' & ' +\n",
        "                          str(round(performance.sleep_correct, 3))\n",
        "                          + ' & ' + str(round(performance.kappa, 3)) + ' &   \\\\\\\\')\n",
        "\n",
        "        backmatter = '\\\\hline \\\\end{tabular}  \\\\label{tab:' + \\\n",
        "                     attributed_classifier.name[0:4] + \\\n",
        "                     'params} \\\\small \\\\vspace{.2cm} ' \\\n",
        "                     '\\\\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, ' \\\n",
        "                     '$\\\\kappa$, and AUC for sleep-wake predictions of ' + attributed_classifier.name + \\\n",
        "                     ' with use of motion, HR, clock proxy, or combination of features. PPG, ' \\\n",
        "                     'photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; ' \\\n",
        "                     'AUC, area under the curve.} \\\\end{table}'\n",
        "\n",
        "        print(backmatter)\n",
        "\n",
        "    def print_table_three_class(classifier_summaries):\n",
        "\n",
        "        frontmatter = '\\\\begin{tabular}{l | l | c | c | c | c | c } & & Wake Correct & NREM' \\\n",
        "                      ' Correct & REM Correct & Best accuracy & $\\\\kappa$ \\\\'\n",
        "\n",
        "        print(frontmatter)\n",
        "\n",
        "        for classifier_summary in classifier_summaries:\n",
        "            performance_dictionary = classifier_summary.performance_dictionary\n",
        "            attributed_classifier = classifier_summary.attributed_classifier\n",
        "\n",
        "            for feature_set in performance_dictionary:\n",
        "\n",
        "                if list(feature_set) == [FeatureType.count]:\n",
        "                    print('\\\\hline ' + attributed_classifier.name + ' & ' + FeatureSetService.get_label(\n",
        "                        list(feature_set)) + ' & ')\n",
        "\n",
        "                else:\n",
        "                    print(' & ' + FeatureSetService.get_label(list(feature_set)) + ' & ')\n",
        "\n",
        "                three_class_performance_summary = performance_dictionary[feature_set]\n",
        "                wake_correct = three_class_performance_summary.wake_correct\n",
        "                nrem_correct = three_class_performance_summary.nrem_correct\n",
        "                rem_correct = three_class_performance_summary.rem_correct\n",
        "                accuracy = three_class_performance_summary.accuracy\n",
        "                kappa = three_class_performance_summary.kappa\n",
        "\n",
        "                print(str(round(wake_correct, 3)) + ' & ' +\n",
        "                      str(round(nrem_correct, 3)) + ' & ' +\n",
        "                      str(round(rem_correct, 3)) + ' & ' +\n",
        "                      str(round(accuracy, 3)) + ' & ' +\n",
        "                      str(round(kappa, 3)) + '\\\\\\\\')\n",
        "\n",
        "        backmatter = '\\\\end{tabular}  \\\\label{tab:REM_params} \\\\small \\\\vspace{.2cm}'\n",
        "\n",
        "        print(backmatter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2wWlGzKvq9-"
      },
      "source": [
        "#### Figure Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K93NHDk3vq9-"
      },
      "source": [
        "class PerformancePlotBuilder(object):\n",
        "\n",
        "    def make_histogram_with_thresholds(classifier_summary: ClassifierSummary):\n",
        "\n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "\n",
        "            number_of_thresholds = 4\n",
        "            number_of_subjects = len(raw_performances)\n",
        "            fig, ax = plt.subplots(nrows=number_of_thresholds, ncols=2, figsize=(8, 8), sharex=True, sharey=True)\n",
        "\n",
        "            all_accuracies = np.zeros((number_of_subjects, number_of_thresholds))\n",
        "            all_wake_correct_fractions = np.zeros((number_of_subjects, number_of_thresholds))\n",
        "\n",
        "            for subject_index in range(number_of_subjects):\n",
        "                true_positive_thresholds, performance_summary = PerformanceSummarizer.summarize_thresholds(\n",
        "                    [raw_performances[subject_index]])\n",
        "\n",
        "                for threshold_index in range(number_of_thresholds):\n",
        "                    all_accuracies[subject_index, threshold_index] = performance_summary[threshold_index].accuracy\n",
        "                    all_wake_correct_fractions[subject_index, threshold_index] = performance_summary[\n",
        "                        threshold_index].wake_correct\n",
        "\n",
        "            dt = 0.02\n",
        "            row_count = 0\n",
        "            font_size = 16\n",
        "            font_name = 'Arial'\n",
        "\n",
        "            for row in ax:\n",
        "                row[0].hist(all_accuracies[:, row_count].tolist(),\n",
        "                            bins=np.arange(0, 1 + dt, dt),\n",
        "                            color=\"skyblue\",\n",
        "                            ec=\"skyblue\")\n",
        "                row[1].hist(all_wake_correct_fractions[:, row_count].tolist(),\n",
        "                            bins=np.arange(0, 1 + dt, dt),\n",
        "                            color=\"lightsalmon\",\n",
        "                            ec=\"lightsalmon\")\n",
        "\n",
        "                if row_count == number_of_thresholds - 1:\n",
        "                    row[0].set_xlabel('Accuracy', fontsize=font_size, fontname=font_name)\n",
        "                    row[1].set_xlabel('Wake correct', fontsize=font_size, fontname=font_name)\n",
        "                    row[0].set_ylabel('Count', fontsize=font_size, fontname=font_name)\n",
        "                row[0].set_xlim((0, 1))\n",
        "                row[1].set_xlim((0, 1))\n",
        "\n",
        "                row_count = row_count + 1\n",
        "\n",
        "            file_save_name = str(Constants.FIGURE_FILE_PATH) + '/' + FeatureSetService.get_label(feature_set) + '_' \\\n",
        "                             + classifier_summary.attributed_classifier.name + '_histograms_with_thresholds.png'\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.savefig(file_save_name, dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            image = Image.open(file_save_name)\n",
        "            width, height = image.size\n",
        "            scale_factor = 0.3\n",
        "            new_image = Image.new('RGB', (int((1 + scale_factor) * width), height), \"white\")\n",
        "            new_image.paste(image, (int(scale_factor * width), 0))\n",
        "            draw = ImageDraw.Draw(new_image)\n",
        "            font = ImageFont.truetype('/Library/Fonts/Arial Unicode.ttf', 75)\n",
        "\n",
        "            draw.text((int(scale_factor * width / 3), int((height * 0.9) * 0.125)), \"TPR = 0.8\", (0, 0, 0), font=font)\n",
        "            draw.text((int(scale_factor * width / 3), int(height * 0.9 * 0.375)), \"TPR = 0.9\", (0, 0, 0), font=font)\n",
        "            draw.text((int(scale_factor * width / 3), int(height * 0.9 * 0.625)), \"TPR = 0.93\", (0, 0, 0), font=font)\n",
        "            draw.text((int(scale_factor * width / 3), int(height * 0.9 * 0.875)), \"TPR = 0.95\", (0, 0, 0), font=font)\n",
        "\n",
        "            new_image.save(str(Constants.FIGURE_FILE_PATH) + '/' + 'figure_threshold_histogram.png')\n",
        "\n",
        "    def make_single_threshold_histograms(classifier_summary: ClassifierSummary, description=''):\n",
        "        font_name = \"Arial\"\n",
        "        font_size = 14\n",
        "\n",
        "        sleep_threshold = 1 - Constants.WAKE_THRESHOLD\n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "            performances = PerformanceSummarizer.apply_single_threshold(raw_performances,\n",
        "                                                                        sleep_threshold=sleep_threshold)\n",
        "            number_of_subjects = len(performances)\n",
        "\n",
        "            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
        "            all_accuracies = np.zeros((number_of_subjects, 1))\n",
        "            all_fraction_wake_correct = np.zeros((number_of_subjects, 1))\n",
        "            all_fraction_sleep_correct = np.zeros((number_of_subjects, 1))\n",
        "            all_kappas = np.zeros((number_of_subjects, 1))\n",
        "\n",
        "            for subject_index in range(number_of_subjects):\n",
        "                all_accuracies[subject_index, 0] = performances[subject_index].accuracy\n",
        "                all_fraction_wake_correct[subject_index, 0] = performances[subject_index].wake_correct\n",
        "                all_fraction_sleep_correct[subject_index, 0] = performances[subject_index].sleep_correct\n",
        "                all_kappas[subject_index, 0] = performances[subject_index].kappa\n",
        "\n",
        "            dt = 0.02\n",
        "            ax[0, 0].hist(all_accuracies, bins=np.arange(0, 1 + dt, dt), color=\"skyblue\", ec=\"skyblue\")\n",
        "            ax[0, 0].set_xlabel('Accuracy', fontsize=font_size, fontname=font_name)\n",
        "            ax[0, 0].set_ylabel('Count', fontsize=font_size, fontname=font_name)\n",
        "            ax[0, 0].set_xlim((0, 1))\n",
        "\n",
        "            ax[0, 1].hist(all_kappas, bins=np.arange(0, 1 + dt, dt), color=\"skyblue\", ec=\"skyblue\")\n",
        "            ax[0, 1].set_xlabel('Cohen\\'s Kappa', fontsize=font_size, fontname=font_name)\n",
        "            ax[0, 1].set_xlim((0, 1))\n",
        "\n",
        "            ax[1, 0].hist(all_fraction_wake_correct, bins=np.arange(0, 1 + dt, dt), color=\"skyblue\", ec=\"skyblue\")\n",
        "            ax[1, 0].set_xlabel('Fraction wake correct (specificity)', fontsize=font_size, fontname=font_name)\n",
        "            ax[1, 0].set_ylabel('Count', fontsize=font_size, fontname=font_name)\n",
        "            ax[1, 0].set_xlim((0, 1))\n",
        "\n",
        "            ax[1, 1].hist(all_fraction_sleep_correct, bins=np.arange(0, 1 + dt, dt), color=\"skyblue\", ec=\"skyblue\")\n",
        "            ax[1, 1].set_xlabel('Fraction sleep correct (sensitivity)', fontsize=font_size, fontname=font_name)\n",
        "            ax[1, 1].set_xlim((0, 1))\n",
        "            plt.tight_layout()\n",
        "            file_save_name = str(\n",
        "                Constants.FIGURE_FILE_PATH) + '/figure_' + classifier_summary.attributed_classifier.name + '_' + \\\n",
        "                             description + '_single_threshold_histograms.png'\n",
        "\n",
        "            plt.savefig(file_save_name, dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "    def make_bland_altman(classifier_summary: ClassifierSummary, description=''):\n",
        "        fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(8, 8))\n",
        "\n",
        "        wake_threshold = Constants.WAKE_THRESHOLD\n",
        "        rem_threshold = Constants.REM_THRESHOLD\n",
        "        \n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "\n",
        "            number_of_subjects = len(raw_performances)\n",
        "\n",
        "            plot_color = FeatureSetService.get_color(feature_set)\n",
        "\n",
        "            for subject_index in range(number_of_subjects):\n",
        "                raw_performance = raw_performances[subject_index]\n",
        "                true_labels = raw_performance.true_labels\n",
        "                predicted_labels = PerformanceBuilder.apply_threshold_three_class(raw_performance, wake_threshold,\n",
        "                                                                                  rem_threshold)\n",
        "\n",
        "                actual_sol = SleepMetricsCalculator.get_sleep_onset_latency(true_labels)\n",
        "                predicted_sol = SleepMetricsCalculator.get_sleep_onset_latency(predicted_labels)\n",
        "\n",
        "                sol_diff = (actual_sol - predicted_sol)\n",
        "                ax[0, 0].scatter(actual_sol, sol_diff, c=plot_color)\n",
        "\n",
        "                ax[0, 0].set_xlabel(\"SOL\")\n",
        "                ax[0, 0].set_ylabel(\"Difference in SOL\")\n",
        "\n",
        "                actual_waso = SleepMetricsCalculator.get_wake_after_sleep_onset(true_labels)\n",
        "                predicted_waso = SleepMetricsCalculator.get_wake_after_sleep_onset(predicted_labels)\n",
        "\n",
        "                waso_diff = (actual_waso - predicted_waso)\n",
        "\n",
        "                ax[0, 1].scatter(actual_waso, waso_diff, c=plot_color)\n",
        "                ax[0, 1].set_xlabel(\"WASO\")\n",
        "\n",
        "                ax[0, 1].set_ylabel(\"Difference in WASO\")\n",
        "\n",
        "                actual_tst = SleepMetricsCalculator.get_tst(true_labels)\n",
        "                predicted_tst = SleepMetricsCalculator.get_tst(predicted_labels)\n",
        "\n",
        "                tst_diff = (actual_tst - predicted_tst)\n",
        "                ax[1, 0].scatter(actual_tst, tst_diff, c=plot_color)\n",
        "\n",
        "                ax[1, 0].set_xlabel(\"TST\")\n",
        "                ax[1, 0].set_ylabel(\"Difference in TST\")\n",
        "\n",
        "                actual_sleep_efficiency = SleepMetricsCalculator.get_sleep_efficiency(true_labels)\n",
        "                predicted_sleep_efficiency = SleepMetricsCalculator.get_sleep_efficiency(predicted_labels)\n",
        "\n",
        "                sleep_efficiency_diff = (actual_sleep_efficiency - predicted_sleep_efficiency)\n",
        "                ax[1, 1].scatter(actual_sleep_efficiency, sleep_efficiency_diff, c=plot_color)\n",
        "                ax[1, 1].set_xlabel(\"Sleep efficiency\")\n",
        "\n",
        "                ax[1, 1].set_ylabel(\"Difference in sleep efficiency\")\n",
        "\n",
        "                actual_time_in_rem = SleepMetricsCalculator.get_time_in_rem(true_labels)\n",
        "                predicted_time_in_rem = SleepMetricsCalculator.get_time_in_rem(predicted_labels)\n",
        "\n",
        "                time_in_rem_diff = (actual_time_in_rem - predicted_time_in_rem)\n",
        "                ax[2, 0].scatter(actual_time_in_rem, time_in_rem_diff, c=plot_color)\n",
        "                ax[2, 0].set_xlabel(\"Time in REM\")\n",
        "                ax[2, 0].set_ylabel(\"Difference in time in REM\")\n",
        "\n",
        "                actual_time_in_nrem = SleepMetricsCalculator.get_time_in_nrem(true_labels)\n",
        "                predicted_time_in_nrem = SleepMetricsCalculator.get_time_in_nrem(predicted_labels)\n",
        "\n",
        "                time_in_nrem_diff = (actual_time_in_nrem - predicted_time_in_nrem)\n",
        "\n",
        "                ax[2, 1].set_xlabel(\"Time in NREM\")\n",
        "                ax[2, 1].set_ylabel(\"Difference in time in NREM\")\n",
        "                font = font_manager.FontProperties(family='Arial', style='normal', size=10)\n",
        "                if subject_index == 0:\n",
        "                    ax[2, 1].scatter(actual_time_in_nrem, time_in_nrem_diff, c=plot_color,\n",
        "                                     label=FeatureSetService.get_label(feature_set))\n",
        "                    ax[2, 1].legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\", borderaxespad=0, prop=font)\n",
        "                else:\n",
        "                    ax[2, 1].scatter(actual_time_in_nrem, time_in_nrem_diff, c=plot_color)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        file_save_name = str(\n",
        "            Constants.FIGURE_FILE_PATH) + '/figure_' + classifier_summary.attributed_classifier.name + '_' + \\\n",
        "                         description + '_bland_altman.png'\n",
        "\n",
        "        plt.savefig(file_save_name, dpi=300)\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOi-rcCgvq9_"
      },
      "source": [
        "class CurvePlotBuilder(object):\n",
        "\n",
        "    def tidy_plot():\n",
        "        ax = plt.subplot(111)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(True)\n",
        "        ax.spines['left'].set_visible(True)\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "\n",
        "    def build_roc_plot(classifier_summary: ClassifierSummary, positive_class=1):\n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "            roc_performance = CurvePerformanceBuilder.build_roc_from_raw(raw_performances, positive_class)\n",
        "\n",
        "            legend_text = FeatureSetService.get_label(list(feature_set))\n",
        "            plot_color = FeatureSetService.get_color(list(feature_set))\n",
        "\n",
        "            plt.plot(roc_performance.false_positive_rates, roc_performance.true_positive_rates,\n",
        "                     label=legend_text, color=plot_color)\n",
        "\n",
        "    def make_roc_sw(classifier_summary: ClassifierSummary, description=''):\n",
        "        CurvePlotBuilder.build_roc_plot(classifier_summary)\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of wake scored as sleep',\n",
        "                                    'Fraction of sleep scored as sleep', (1.0, 0.4))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_sw_roc.png')))\n",
        "        plt.close()\n",
        "\n",
        "    def make_roc_one_vs_rest(classifier_summary: ClassifierSummary, description=''):\n",
        "        CurvePlotBuilder.build_roc_plot(classifier_summary, 0)\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of REM or NREM scored as wake',\n",
        "                                    'Fraction of wake scored as wake', (1.0, 0.4))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_ovr_wake_roc.png')))\n",
        "        plt.close()\n",
        "\n",
        "        CurvePlotBuilder.build_roc_plot(classifier_summary, 1)\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of wake or REM scored as NREM',\n",
        "                                    'Fraction of NREM scored as NREM', (1.0, 0.4))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_ovr_nrem_roc.png')))\n",
        "        plt.close()\n",
        "\n",
        "        CurvePlotBuilder.build_roc_plot(classifier_summary, 2)\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of wake or NREM scored as REM',\n",
        "                                    'Fraction of REM scored as REM', (1.0, 0.4))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_ovr_rem_roc.png')))\n",
        "        plt.close()\n",
        "\n",
        "    def build_pr_plot(classifier_summary: ClassifierSummary):\n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "            roc_performance = CurvePerformanceBuilder.build_precision_recall_from_raw(raw_performances)\n",
        "\n",
        "            legend_text = FeatureSetService.get_label(list(feature_set))\n",
        "            plot_color = FeatureSetService.get_color(list(feature_set))\n",
        "\n",
        "            plt.plot(roc_performance.recalls, roc_performance.precisions,\n",
        "                     label=legend_text, color=plot_color)\n",
        "\n",
        "    def make_pr_sw(classifier_summary: ClassifierSummary, description=''):\n",
        "        CurvePlotBuilder.build_pr_plot(classifier_summary)\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of wake scored as wake',\n",
        "                                    'Fraction of predicted wake correct', (0.5, 1.0))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_sw_pr.png')))\n",
        "        plt.close()\n",
        "\n",
        "    def make_three_class_roc(classifier_summary: ClassifierSummary, description=''):\n",
        "\n",
        "        performance_dictionary = {}\n",
        "\n",
        "        for feature_set in classifier_summary.performance_dictionary:\n",
        "            raw_performances = classifier_summary.performance_dictionary[feature_set]\n",
        "            sleep_wake_roc_performance, rem_roc_performance, nrem_roc_performance, three_class_performances = \\\n",
        "                CurvePerformanceBuilder.build_three_class_roc_with_binary_search(\n",
        "                    raw_performances)\n",
        "\n",
        "            performance_dictionary[feature_set] = PerformanceSummarizer.average_three_class(three_class_performances)\n",
        "\n",
        "            legend_text = FeatureSetService.get_label(list(feature_set))\n",
        "            plot_color = FeatureSetService.get_color(list(feature_set))\n",
        "\n",
        "            plt.plot(sleep_wake_roc_performance.false_positive_rates, sleep_wake_roc_performance.true_positive_rates,\n",
        "                     label=legend_text, color=plot_color)\n",
        "            plt.plot(nrem_roc_performance.false_positive_rates, nrem_roc_performance.true_positive_rates,\n",
        "                     color=plot_color, linestyle=':')\n",
        "            plt.plot(rem_roc_performance.false_positive_rates, rem_roc_performance.true_positive_rates,\n",
        "                     color=plot_color, linestyle='--')\n",
        "\n",
        "        CurvePlotBuilder.tidy_plot()\n",
        "        CurvePlotBuilder.set_labels(classifier_summary.attributed_classifier,\n",
        "                                    'Fraction of wake scored as REM or NREM',\n",
        "                                    'Fraction of REM, NREM scored correctly', (1.0, 0.4))\n",
        "\n",
        "        number_of_trials = len(next(iter(classifier_summary.performance_dictionary.values())))\n",
        "        plt.savefig(str(Constants.FIGURE_FILE_PATH.joinpath(\n",
        "            classifier_summary.attributed_classifier.name + '_' + str(\n",
        "                number_of_trials) + '_' + description + '_three_class_roc.png')))\n",
        "        plt.close()\n",
        "\n",
        "        return performance_dictionary\n",
        "\n",
        "    def set_labels(attributed_classifier, x_label_text, y_label_text, legend_location):\n",
        "        font_name = \"Arial\"\n",
        "        font_size = 14\n",
        "        font = font_manager.FontProperties(family=font_name, style='normal', size=font_size)\n",
        "\n",
        "        if attributed_classifier.name == 'Neural Net':\n",
        "            plt.legend(bbox_to_anchor=legend_location, borderaxespad=0., prop=font)\n",
        "\n",
        "        plt.xlabel(x_label_text, fontsize=font_size, fontname=font_name)\n",
        "        plt.ylabel(y_label_text, fontsize=font_size, fontname=font_name)\n",
        "\n",
        "        plt.title(attributed_classifier.name, fontsize=18, fontname=font_name, fontweight='bold')\n",
        "\n",
        "    def combine_plots_as_grid(classifiers, number_of_trials, plot_extension):\n",
        "        combined_filenames = []\n",
        "        for attributed_classifier in classifiers:\n",
        "            combined_filenames.append(str(Constants.FIGURE_FILE_PATH) + '/' +\n",
        "                                      attributed_classifier.name + '_' +\n",
        "                                      str(number_of_trials) + '_' + plot_extension + '.png')\n",
        "\n",
        "        images = list(map(Image.open, combined_filenames))\n",
        "        widths, heights = zip(*(i.size for i in images))\n",
        "        max_width = max(widths)\n",
        "        max_height = max(heights)\n",
        "\n",
        "        new_image = Image.new('RGB', (2 * max_width, 2 * max_height))\n",
        "\n",
        "        count = 0\n",
        "        for im in images:\n",
        "            x_offset = int((count % 2) * max_width)\n",
        "            y_offset = int(math.floor(count / 2) * max_height)\n",
        "\n",
        "            new_image.paste(im, (x_offset, y_offset))\n",
        "            count = count + 1\n",
        "\n",
        "        new_image.save(str(Constants.FIGURE_FILE_PATH) + '/figure_' + str(number_of_trials) + plot_extension + '.png')\n",
        "\n",
        "    def combine_sw_and_three_class_plots(attributed_classifier, number_of_trials, plot_extension):\n",
        "        combined_filenames = [str(Constants.FIGURE_FILE_PATH) + '/' +\n",
        "                              attributed_classifier.name + '_' +\n",
        "                              str(number_of_trials) + '__' + plot_extension + '_sw_roc.png',\n",
        "                              str(Constants.FIGURE_FILE_PATH) + '/' +\n",
        "                              attributed_classifier.name + '_' +\n",
        "                              str(number_of_trials) + '__' + plot_extension + '_three_class_roc.png']\n",
        "\n",
        "        images = list(map(Image.open, combined_filenames))\n",
        "        widths, heights = zip(*(i.size for i in images))\n",
        "        max_width = max(widths)\n",
        "        max_height = max(heights)\n",
        "\n",
        "        new_image = Image.new('RGB', (2 * max_width, 1 * max_height))\n",
        "\n",
        "        count = 0\n",
        "        for im in images:\n",
        "            x_offset = int((count % 2) * max_width)\n",
        "            y_offset = int(math.floor(count / 2) * max_height)\n",
        "\n",
        "            new_image.paste(im, (x_offset, y_offset))\n",
        "            count = count + 1\n",
        "\n",
        "        new_image.save(\n",
        "            str(Constants.FIGURE_FILE_PATH) + '/figure_' + attributed_classifier.name + str(\n",
        "                number_of_trials) + plot_extension + '_combined.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoAMAjapFRW4"
      },
      "source": [
        "# Results & Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS6XwRmVvq9_"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42GveCgHvq9_"
      },
      "source": [
        "def figures_leave_one_out_sleep_wake_performance():\n",
        "    print(\"+++ [Start] figures_leave_one_out_sleep_wake_performance() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    attributed_classifier = AttributedClassifier(name='Neural Net',\n",
        "                                                 classifier=MLPClassifier(activation='relu',\n",
        "                                                                          hidden_layer_sizes=(15, 15, 15),\n",
        "                                                                          max_iter=1000, alpha=0.01, solver='lbfgs'))\n",
        "\n",
        "    feature_sets = [[FeatureType.count, FeatureType.heart_rate, FeatureType.circadian_model]]\n",
        "\n",
        "    print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        \n",
        "    if Constants.VERBOSE:\n",
        "        print('Running ' + attributed_classifier.name + '...')\n",
        "        \n",
        "    classifier_summary = SleepWakeClassifierSummaryBuilder.build_leave_one_out(attributed_classifier, feature_sets)\n",
        "    PerformancePlotBuilder.make_histogram_with_thresholds(classifier_summary)\n",
        "    PerformancePlotBuilder.make_single_threshold_histograms(classifier_summary)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_leave_one_out_sleep_wake_performance() ---\")\n",
        "\n",
        "def figures_leave_one_out_three_class_performance():\n",
        "    print(\"+++ [Start] figures_leave_one_out_three_class_performance() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    attributed_classifier = AttributedClassifier(name='Neural Net',\n",
        "                                                 classifier=MLPClassifier(activation='relu',\n",
        "                                                                          hidden_layer_sizes=(15, 15, 15),\n",
        "                                                                          max_iter=1000, alpha=0.01, solver='lbfgs'))\n",
        "\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "\n",
        "    print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        \n",
        "    if Constants.VERBOSE:\n",
        "        print('Running ' + attributed_classifier.name + '...')\n",
        "        \n",
        "    classifier_summary = ThreeClassClassifierSummaryBuilder.build_leave_one_out(attributed_classifier, feature_sets)\n",
        "    PerformancePlotBuilder.make_bland_altman(classifier_summary)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_leave_one_out_three_class_performance() ---\")\n",
        "\n",
        "def figure_leave_one_out_roc_and_pr():\n",
        "    print(\"+++ [Start] figure_leave_one_out_roc_and_pr() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = SleepWakeClassifierSummaryBuilder.build_leave_one_out(attributed_classifier, feature_sets)\n",
        "\n",
        "        CurvePlotBuilder.make_roc_sw(classifier_summary)\n",
        "        CurvePlotBuilder.make_pr_sw(classifier_summary)\n",
        "        TableBuilder.print_table_sw(classifier_summary)\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, len(SubjectBuilder.get_all_subject_ids()), '_sw_pr')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, len(SubjectBuilder.get_all_subject_ids()), '_sw_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figure_leave_one_out_roc_and_pr() ---\")\n",
        "\n",
        "def figures_mc_sleep_wake():\n",
        "    \n",
        "    print(\"+++ [Start] figures_mc_sleep_wake() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "    trial_count = 20\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = SleepWakeClassifierSummaryBuilder.build_monte_carlo(attributed_classifier, feature_sets,\n",
        "                                                                                 trial_count)\n",
        "\n",
        "        CurvePlotBuilder.make_roc_sw(classifier_summary)\n",
        "        CurvePlotBuilder.make_pr_sw(classifier_summary)\n",
        "        TableBuilder.print_table_sw(classifier_summary)\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_sw_pr')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_sw_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_mc_sleep_wake() ---\")\n",
        "\n",
        "def figures_mc_three_class():\n",
        "    \n",
        "    print(\"+++ [Start] figures_mc_three_class() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "    trial_count = 20\n",
        "\n",
        "    three_class_performance_summaries = []\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = ThreeClassClassifierSummaryBuilder.build_monte_carlo(attributed_classifier, feature_sets,\n",
        "                                                                                  trial_count)\n",
        "\n",
        "        CurvePlotBuilder.make_roc_one_vs_rest(classifier_summary)\n",
        "        three_class_performance_dictionary = CurvePlotBuilder.make_three_class_roc(classifier_summary)\n",
        "\n",
        "        classifier_summary.performance_dictionary = three_class_performance_dictionary\n",
        "        three_class_performance_summaries.append(classifier_summary)\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    TableBuilder.print_table_three_class(three_class_performance_summaries)\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_three_class_roc')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_ovr_rem_roc')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_ovr_nrem_roc')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_ovr_wake_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_mc_three_class() ---\")\n",
        "\n",
        "def figures_mesa_sleep_wake():\n",
        "    \n",
        "    print(\"+++ [Start] figures_mesa_sleep_wake() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "    # Uncomment to just use MLP:\n",
        "    # classifiers = [AttributedClassifier(name='Neural Net',\n",
        "    #                                     classifier=MLPClassifier(activation='relu', hidden_layer_sizes=(15, 15, 15),\n",
        "    #                                                              max_iter=1000, alpha=0.01, solver='lbfgs'))]\n",
        "\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = SleepWakeClassifierSummaryBuilder.build_mesa(attributed_classifier, feature_sets)\n",
        "        CurvePlotBuilder.make_roc_sw(classifier_summary, '_mesa')\n",
        "        CurvePlotBuilder.make_pr_sw(classifier_summary, '_mesa')\n",
        "        TableBuilder.print_table_sw(classifier_summary)\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, 1, '_mesa_sw_pr')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, 1, '_mesa_sw_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_mesa_sleep_wake() ---\")\n",
        "\n",
        "def figures_mesa_three_class():\n",
        "    \n",
        "    print(\"+++ [Start] figures_mesa_three_class() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "\n",
        "    # Uncomment to just use MLP:\n",
        "    # classifiers = [AttributedClassifier(name='Neural Net', classifier=MLPClassifier(activation='relu', hidden_layer_sizes=(15, 15, 15),\n",
        "    #                                                            max_iter=1000, alpha=0.01, solver='lbfgs'))]\n",
        "\n",
        "    feature_sets = Utils.get_base_feature_sets()\n",
        "    three_class_performance_summaries = []\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = ThreeClassClassifierSummaryBuilder.build_mesa_leave_one_out(attributed_classifier,\n",
        "                                                                                         feature_sets)\n",
        "        PerformancePlotBuilder.make_bland_altman(classifier_summary, '_mesa')\n",
        "        PerformancePlotBuilder.make_single_threshold_histograms(classifier_summary, '_mesa')\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = ThreeClassClassifierSummaryBuilder.build_mesa_all_combined(attributed_classifier,\n",
        "                                                                                        feature_sets)\n",
        "        three_class_performance_dictionary = CurvePlotBuilder.make_three_class_roc(classifier_summary, '_mesa')\n",
        "        classifier_summary.performance_dictionary = three_class_performance_dictionary\n",
        "        three_class_performance_summaries.append(classifier_summary)\n",
        "        CurvePlotBuilder.combine_sw_and_three_class_plots(attributed_classifier, 1, 'mesa')\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    TableBuilder.print_table_three_class(three_class_performance_summaries)\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, 1, '_mesa_three_class_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_mesa_three_class() ---\")\n",
        "\n",
        "def figures_compare_time_based_features():\n",
        "    \n",
        "    print(\"+++ [Start] figures_compare_time_based_features() +++\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    classifiers = Utils.get_classifiers()\n",
        "    feature_sets = [[FeatureType.count, FeatureType.heart_rate],\n",
        "                    [FeatureType.count, FeatureType.heart_rate, FeatureType.time],\n",
        "                    [FeatureType.count, FeatureType.heart_rate, FeatureType.cosine],\n",
        "                    [FeatureType.count, FeatureType.heart_rate, FeatureType.circadian_model]]\n",
        "\n",
        "    trial_count = 50\n",
        "\n",
        "    for attributed_classifier in classifiers:\n",
        "        print('|---- [ ' + attributed_classifier.name + ' ] ----|')\n",
        "        start_classifier_time = time.time()\n",
        "        \n",
        "        if Constants.VERBOSE:\n",
        "            print('Running ' + attributed_classifier.name + '...')\n",
        "        classifier_summary = SleepWakeClassifierSummaryBuilder.build_monte_carlo(attributed_classifier, feature_sets,\n",
        "                                                                                 trial_count)\n",
        "\n",
        "        CurvePlotBuilder.make_roc_sw(classifier_summary, '_time_only')\n",
        "        CurvePlotBuilder.make_pr_sw(classifier_summary, '_time_only')\n",
        "        TableBuilder.print_table_sw(classifier_summary)\n",
        "        \n",
        "        end_classifier_time = time.time()\n",
        "        took_time = str(round(((end_classifier_time - start_classifier_time) / 60), 2))\n",
        "        print('----------------------------------------------------------------------------')\n",
        "        print('____(: ' + attributed_classifier.name + ' :) took ' + took_time + \" minutes\")\n",
        "\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_time_only_sw_pr')\n",
        "    CurvePlotBuilder.combine_plots_as_grid(classifiers, trial_count, '_time_only_sw_roc')\n",
        "    \n",
        "    end_time = time.time()\n",
        "    execution_time = str(round(((end_time - start_time) / 60), 2))\n",
        "    print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "    print(\"--- [Done] figures_compare_time_based_features() ---\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmufAvCpvq-A"
      },
      "source": [
        "\n",
        "def run_all_analysis():\n",
        "\n",
        "    figures_mc_sleep_wake()\n",
        "    figures_mc_three_class()\n",
        "    \n",
        "    figure_leave_one_out_roc_and_pr()\n",
        "\n",
        "    figures_leave_one_out_sleep_wake_performance()\n",
        "    figures_leave_one_out_three_class_performance()\n",
        "    \n",
        "    figures_compare_time_based_features()\n",
        "\n",
        "    figures_mesa_sleep_wake()\n",
        "    figures_mesa_three_class()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eXYRUYXvq-A"
      },
      "source": [
        "## Analysis Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txpx-O3Dvq-A",
        "outputId": "cb09bb17-836f-42c5-a88a-fd5a98a39a96"
      },
      "source": [
        "### ---- START ANALYSIS ----\n",
        "print(\"+++ Start Analysis_Runner +++\")\n",
        "start_time_all = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ Start Analysis_Runner +++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BG_Jv_Hvq-A"
      },
      "source": [
        "### [Monte Carlo] Sleep-Wake Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tcHegT9vq-A",
        "outputId": "51fe1fbc-8498-463e-eeb0-8ca23b1d98dc"
      },
      "source": [
        "figures_mc_sleep_wake()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_mc_sleep_wake() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Logistic Regression across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.792 & 0.703 & 0.8 & 0.241 & 0.838  \\\\\n",
            "& 0.874 & 0.559 & 0.9 & 0.321 &   \\\\\n",
            "& 0.898 & 0.502 & 0.93 & 0.355 &   \\\\\n",
            "& 0.912 & 0.44 & 0.95 & 0.366 &   \\\\\n",
            "\\hline HR only &\n",
            "0.777 & 0.503 & 0.8 & 0.149 & 0.739  \\\\\n",
            "& 0.856 & 0.303 & 0.9 & 0.155 &   \\\\\n",
            "& 0.88 & 0.252 & 0.93 & 0.166 &   \\\\\n",
            "& 0.895 & 0.203 & 0.95 & 0.159 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.792 & 0.702 & 0.8 & 0.24 & 0.835  \\\\\n",
            "& 0.874 & 0.561 & 0.9 & 0.323 &   \\\\\n",
            "& 0.898 & 0.504 & 0.93 & 0.357 &   \\\\\n",
            "& 0.912 & 0.442 & 0.95 & 0.368 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.801 & 0.817 & 0.8 & 0.288 & 0.891  \\\\\n",
            "& 0.884 & 0.685 & 0.9 & 0.394 &   \\\\\n",
            "& 0.903 & 0.581 & 0.93 & 0.406 &   \\\\\n",
            "& 0.915 & 0.488 & 0.95 & 0.402 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Logiparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Logistic Regression with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.08 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Random Forest across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.789 & 0.68 & 0.799 & 0.25 & 0.821  \\\\\n",
            "& 0.868 & 0.519 & 0.899 & 0.317 &   \\\\\n",
            "& 0.89 & 0.442 & 0.929 & 0.33 &   \\\\\n",
            "& 0.903 & 0.381 & 0.949 & 0.332 &   \\\\\n",
            "\\hline HR only &\n",
            "0.766 & 0.388 & 0.799 & 0.107 & 0.683  \\\\\n",
            "& 0.849 & 0.282 & 0.899 & 0.151 &   \\\\\n",
            "& 0.874 & 0.244 & 0.929 & 0.168 &   \\\\\n",
            "& 0.889 & 0.205 & 0.949 & 0.169 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.788 & 0.654 & 0.8 & 0.241 & 0.812  \\\\\n",
            "& 0.869 & 0.518 & 0.9 & 0.319 &   \\\\\n",
            "& 0.892 & 0.46 & 0.93 & 0.345 &   \\\\\n",
            "& 0.906 & 0.404 & 0.95 & 0.354 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.799 & 0.794 & 0.8 & 0.304 & 0.888  \\\\\n",
            "& 0.883 & 0.687 & 0.9 & 0.422 &   \\\\\n",
            "& 0.905 & 0.623 & 0.93 & 0.459 &   \\\\\n",
            "& 0.919 & 0.572 & 0.95 & 0.484 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Randparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Random Forest with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.77 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by k-Nearest Neighbors across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.788 & 0.663 & 0.8 & 0.235 & 0.789  \\\\\n",
            "& 0.866 & 0.488 & 0.9 & 0.286 &   \\\\\n",
            "& 0.886 & 0.4 & 0.93 & 0.287 &   \\\\\n",
            "& 0.897 & 0.307 & 0.95 & 0.257 &   \\\\\n",
            "\\hline HR only &\n",
            "0.767 & 0.391 & 0.8 & 0.106 & 0.682  \\\\\n",
            "& 0.848 & 0.255 & 0.9 & 0.128 &   \\\\\n",
            "& 0.872 & 0.2 & 0.93 & 0.127 &   \\\\\n",
            "& 0.884 & 0.132 & 0.95 & 0.092 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.789 & 0.68 & 0.8 & 0.242 & 0.825  \\\\\n",
            "& 0.87 & 0.547 & 0.9 & 0.323 &   \\\\\n",
            "& 0.892 & 0.469 & 0.93 & 0.339 &   \\\\\n",
            "& 0.905 & 0.403 & 0.95 & 0.341 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.803 & 0.845 & 0.8 & 0.317 & 0.891  \\\\\n",
            "& 0.878 & 0.649 & 0.9 & 0.382 &   \\\\\n",
            "& 0.898 & 0.557 & 0.93 & 0.397 &   \\\\\n",
            "& 0.91 & 0.469 & 0.95 & 0.392 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:k-Neparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of k-Nearest Neighbors with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.48 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Neural Net across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.793 & 0.724 & 0.8 & 0.261 & 0.853  \\\\\n",
            "& 0.871 & 0.565 & 0.9 & 0.332 &   \\\\\n",
            "& 0.894 & 0.505 & 0.93 & 0.361 &   \\\\\n",
            "& 0.907 & 0.437 & 0.95 & 0.365 &   \\\\\n",
            "\\hline HR only &\n",
            "0.774 & 0.502 & 0.8 & 0.155 & 0.736  \\\\\n",
            "& 0.852 & 0.308 & 0.9 & 0.165 &   \\\\\n",
            "& 0.877 & 0.27 & 0.93 & 0.185 &   \\\\\n",
            "& 0.892 & 0.236 & 0.95 & 0.195 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.79 & 0.698 & 0.8 & 0.249 & 0.84  \\\\\n",
            "& 0.872 & 0.569 & 0.9 & 0.336 &   \\\\\n",
            "& 0.895 & 0.515 & 0.93 & 0.369 &   \\\\\n",
            "& 0.908 & 0.45 & 0.95 & 0.377 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.802 & 0.831 & 0.8 & 0.311 & 0.908  \\\\\n",
            "& 0.886 & 0.737 & 0.9 & 0.437 &   \\\\\n",
            "& 0.909 & 0.673 & 0.93 & 0.477 &   \\\\\n",
            "& 0.922 & 0.606 & 0.95 & 0.497 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Neurparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Neural Net with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 8.61 minutes\n",
            "    Execution time: 9.93 minutes\n",
            "--- [Done] figures_mc_sleep_wake() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0npCiYvq-A"
      },
      "source": [
        "### [Monte Carlo] Three-Class Performance (REM/NREM/Wake)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEJECJ3Hvq-A",
        "outputId": "08387310-01d2-4518-da8f-1e69a09c9c0e"
      },
      "source": [
        "figures_mc_three_class()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_mc_three_class() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.32 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.93 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.68 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 12.94 minutes\n",
            "\\begin{tabular}{l | l | c | c | c | c | c } & & Wake Correct & NREM Correct & REM Correct & Best accuracy & $\\kappa$ \\\n",
            "\\hline Logistic Regression & Motion only & \n",
            "0.6 & 0.48 & 0.336 & 0.668 & 0.082\\\\\n",
            " & HR only & \n",
            "0.6 & 0.412 & 0.412 & 0.657 & 0.131\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.536 & 0.536 & 0.674 & 0.198\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.576 & 0.575 & 0.671 & 0.186\\\\\n",
            "\\hline Random Forest & Motion only & \n",
            "0.6 & 0.482 & 0.307 & 0.656 & 0.08\\\\\n",
            " & HR only & \n",
            "0.6 & 0.37 & 0.37 & 0.595 & 0.15\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.536 & 0.535 & 0.644 & 0.214\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.581 & 0.581 & 0.651 & 0.24\\\\\n",
            "\\hline k-Nearest Neighbors & Motion only & \n",
            "0.6 & 0.285 & 0.534 & 0.651 & 0.039\\\\\n",
            " & HR only & \n",
            "0.6 & 0.39 & 0.39 & 0.627 & 0.08\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.544 & 0.544 & 0.656 & 0.12\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.605 & 0.606 & 0.676 & 0.193\\\\\n",
            "\\hline Neural Net & Motion only & \n",
            "0.6 & 0.297 & 0.601 & 0.693 & 0.086\\\\\n",
            " & HR only & \n",
            "0.6 & 0.435 & 0.436 & 0.675 & 0.094\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.601 & 0.601 & 0.687 & 0.13\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.63 & 0.629 & 0.684 & 0.221\\\\\n",
            "\\end{tabular}  \\label{tab:REM_params} \\small \\vspace{.2cm}\n",
            "    Execution time: 14.88 minutes\n",
            "--- [Done] figures_mc_three_class() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCZ_2XPvq-A"
      },
      "source": [
        "### [Leave One Out] ROC and Precision-recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bib2TTxzvq-B",
        "outputId": "3c778d53-8833-48bd-8d99-29e5915b0cf1"
      },
      "source": [
        "figure_leave_one_out_roc_and_pr()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figure_leave_one_out_roc_and_pr() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Logistic Regression across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.795 & 0.731 & 0.799 & 0.26 & 0.851  \\\\\n",
            "& 0.875 & 0.609 & 0.899 & 0.339 &   \\\\\n",
            "& 0.898 & 0.559 & 0.929 & 0.371 &   \\\\\n",
            "& 0.911 & 0.496 & 0.949 & 0.382 &   \\\\\n",
            "\\hline HR only &\n",
            "0.777 & 0.558 & 0.799 & 0.169 & 0.766  \\\\\n",
            "& 0.854 & 0.386 & 0.899 & 0.187 &   \\\\\n",
            "& 0.877 & 0.297 & 0.929 & 0.188 &   \\\\\n",
            "& 0.892 & 0.264 & 0.949 & 0.196 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.795 & 0.733 & 0.799 & 0.26 & 0.852  \\\\\n",
            "& 0.875 & 0.61 & 0.899 & 0.34 &   \\\\\n",
            "& 0.898 & 0.563 & 0.929 & 0.374 &   \\\\\n",
            "& 0.911 & 0.497 & 0.949 & 0.383 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.803 & 0.832 & 0.799 & 0.298 & 0.903  \\\\\n",
            "& 0.887 & 0.727 & 0.899 & 0.411 &   \\\\\n",
            "& 0.904 & 0.646 & 0.929 & 0.427 &   \\\\\n",
            "& 0.918 & 0.6 & 0.949 & 0.454 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Logiparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Logistic Regression with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.07 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Random Forest across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.793 & 0.709 & 0.799 & 0.25 & 0.833  \\\\\n",
            "& 0.871 & 0.569 & 0.899 & 0.315 &   \\\\\n",
            "& 0.894 & 0.519 & 0.929 & 0.347 &   \\\\\n",
            "& 0.908 & 0.452 & 0.949 & 0.356 &   \\\\\n",
            "\\hline HR only &\n",
            "0.77 & 0.43 & 0.799 & 0.122 & 0.694  \\\\\n",
            "& 0.849 & 0.291 & 0.899 & 0.138 &   \\\\\n",
            "& 0.874 & 0.263 & 0.929 & 0.162 &   \\\\\n",
            "& 0.888 & 0.212 & 0.948 & 0.154 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.794 & 0.711 & 0.799 & 0.253 & 0.831  \\\\\n",
            "& 0.871 & 0.562 & 0.899 & 0.313 &   \\\\\n",
            "& 0.893 & 0.492 & 0.929 & 0.332 &   \\\\\n",
            "& 0.907 & 0.425 & 0.949 & 0.335 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.802 & 0.846 & 0.799 & 0.302 & 0.91  \\\\\n",
            "& 0.887 & 0.765 & 0.899 & 0.425 &   \\\\\n",
            "& 0.909 & 0.669 & 0.929 & 0.448 &   \\\\\n",
            "& 0.923 & 0.624 & 0.949 & 0.481 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Randparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Random Forest with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.55 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by k-Nearest Neighbors across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.727 & 0.692 & 0.727 & 0.195 & 0.774  \\\\\n",
            "& 0.867 & 0.501 & 0.899 & 0.281 &   \\\\\n",
            "& 0.885 & 0.406 & 0.929 & 0.267 &   \\\\\n",
            "& 0.898 & 0.332 & 0.949 & 0.258 &   \\\\\n",
            "\\hline HR only &\n",
            "0.769 & 0.415 & 0.799 & 0.115 & 0.69  \\\\\n",
            "& 0.848 & 0.262 & 0.899 & 0.125 &   \\\\\n",
            "& 0.87 & 0.202 & 0.929 & 0.12 &   \\\\\n",
            "& 0.884 & 0.153 & 0.949 & 0.1 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.794 & 0.716 & 0.799 & 0.254 & 0.843  \\\\\n",
            "& 0.873 & 0.588 & 0.899 & 0.326 &   \\\\\n",
            "& 0.895 & 0.532 & 0.929 & 0.355 &   \\\\\n",
            "& 0.907 & 0.447 & 0.949 & 0.35 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.805 & 0.849 & 0.799 & 0.307 & 0.901  \\\\\n",
            "& 0.886 & 0.719 & 0.899 & 0.406 &   \\\\\n",
            "& 0.906 & 0.64 & 0.929 & 0.429 &   \\\\\n",
            "& 0.915 & 0.537 & 0.949 & 0.416 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:k-Neparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of k-Nearest Neighbors with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.31 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Neural Net across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.794 & 0.725 & 0.799 & 0.258 & 0.851  \\\\\n",
            "& 0.875 & 0.61 & 0.899 & 0.34 &   \\\\\n",
            "& 0.898 & 0.559 & 0.929 & 0.371 &   \\\\\n",
            "& 0.911 & 0.496 & 0.949 & 0.382 &   \\\\\n",
            "\\hline HR only &\n",
            "0.777 & 0.558 & 0.799 & 0.169 & 0.766  \\\\\n",
            "& 0.854 & 0.386 & 0.899 & 0.187 &   \\\\\n",
            "& 0.877 & 0.297 & 0.929 & 0.188 &   \\\\\n",
            "& 0.892 & 0.264 & 0.949 & 0.196 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.794 & 0.722 & 0.799 & 0.256 & 0.841  \\\\\n",
            "& 0.876 & 0.612 & 0.899 & 0.342 &   \\\\\n",
            "& 0.898 & 0.565 & 0.929 & 0.377 &   \\\\\n",
            "& 0.91 & 0.488 & 0.949 & 0.376 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.806 & 0.869 & 0.799 & 0.314 & 0.926  \\\\\n",
            "& 0.891 & 0.791 & 0.899 & 0.442 &   \\\\\n",
            "& 0.915 & 0.739 & 0.929 & 0.49 &   \\\\\n",
            "& 0.928 & 0.668 & 0.949 & 0.513 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Neurparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Neural Net with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 6.32 minutes\n",
            "    Execution time: 7.25 minutes\n",
            "--- [Done] figure_leave_one_out_roc_and_pr() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpRXmZzMvq-B"
      },
      "source": [
        "### [Leave One Out] Sleep-Wake Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSY_NeiAvq-B",
        "outputId": "06494914-15cf-4e05-ce01-ff192e66e403"
      },
      "source": [
        "figures_leave_one_out_sleep_wake_performance()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_leave_one_out_sleep_wake_performance() +++\n",
            "|---- [ Neural Net ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Execution time: 3.27 minutes\n",
            "--- [Done] figures_leave_one_out_sleep_wake_performance() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaXXwHSFvq-B"
      },
      "source": [
        "### [Leave One Out] Three-Class Performance (REM/NREM/Wake)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAbrlxAAvq-B",
        "outputId": "51a72a0f-91ee-4150-f9e7-c3f237ceca8a"
      },
      "source": [
        "figures_leave_one_out_three_class_performance()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_leave_one_out_three_class_performance() +++\n",
            "|---- [ Neural Net ] ----|\n",
            "    Execution time: 24.79 minutes\n",
            "--- [Done] figures_leave_one_out_three_class_performance() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm_7u7aQvq-B"
      },
      "source": [
        "### Compare Time Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjVstASTvq-B",
        "outputId": "db689f4f-055b-431f-de02-cb1a116c58e9"
      },
      "source": [
        "figures_compare_time_based_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_compare_time_based_features() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Logistic Regression across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion, HR &\n",
            "0.792 & 0.709 & 0.8 & 0.259 & 0.84  \\\\\n",
            "& 0.872 & 0.564 & 0.9 & 0.34 &   \\\\\n",
            "& 0.894 & 0.503 & 0.93 & 0.368 &   \\\\\n",
            "& 0.907 & 0.433 & 0.95 & 0.371 &   \\\\\n",
            "\\hline Motion, HR, and Time &\n",
            "0.799 & 0.793 & 0.8 & 0.299 & 0.848  \\\\\n",
            "& 0.876 & 0.609 & 0.9 & 0.367 &   \\\\\n",
            "& 0.897 & 0.532 & 0.93 & 0.388 &   \\\\\n",
            "& 0.91 & 0.469 & 0.95 & 0.398 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.802 & 0.83 & 0.8 & 0.315 & 0.892  \\\\\n",
            "& 0.879 & 0.652 & 0.9 & 0.393 &   \\\\\n",
            "& 0.899 & 0.567 & 0.93 & 0.413 &   \\\\\n",
            "& 0.911 & 0.489 & 0.95 & 0.414 &   \\\\\n",
            "\\hline Motion, HR, and Clock &\n",
            "0.802 & 0.826 & 0.8 & 0.313 & 0.886  \\\\\n",
            "& 0.88 & 0.664 & 0.9 & 0.399 &   \\\\\n",
            "& 0.899 & 0.565 & 0.93 & 0.41 &   \\\\\n",
            "& 0.911 & 0.489 & 0.95 & 0.413 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Logiparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Logistic Regression with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.22 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Random Forest across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion, HR &\n",
            "0.79 & 0.682 & 0.8 & 0.257 & 0.819  \\\\\n",
            "& 0.869 & 0.546 & 0.9 & 0.339 &   \\\\\n",
            "& 0.892 & 0.485 & 0.93 & 0.365 &   \\\\\n",
            "& 0.905 & 0.43 & 0.95 & 0.376 &   \\\\\n",
            "\\hline Motion, HR, and Time &\n",
            "0.8 & 0.796 & 0.8 & 0.311 & 0.883  \\\\\n",
            "& 0.883 & 0.701 & 0.9 & 0.434 &   \\\\\n",
            "& 0.905 & 0.637 & 0.93 & 0.471 &   \\\\\n",
            "& 0.918 & 0.574 & 0.95 & 0.487 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.8 & 0.809 & 0.8 & 0.316 & 0.893  \\\\\n",
            "& 0.883 & 0.702 & 0.9 & 0.435 &   \\\\\n",
            "& 0.905 & 0.636 & 0.93 & 0.47 &   \\\\\n",
            "& 0.918 & 0.575 & 0.95 & 0.487 &   \\\\\n",
            "\\hline Motion, HR, and Clock &\n",
            "0.799 & 0.795 & 0.8 & 0.309 & 0.886  \\\\\n",
            "& 0.882 & 0.69 & 0.9 & 0.428 &   \\\\\n",
            "& 0.905 & 0.633 & 0.93 & 0.468 &   \\\\\n",
            "& 0.918 & 0.577 & 0.95 & 0.489 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Randparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Random Forest with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 2.45 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by k-Nearest Neighbors across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion, HR &\n",
            "0.79 & 0.682 & 0.8 & 0.246 & 0.83  \\\\\n",
            "& 0.872 & 0.549 & 0.9 & 0.33 &   \\\\\n",
            "& 0.894 & 0.476 & 0.93 & 0.351 &   \\\\\n",
            "& 0.907 & 0.409 & 0.95 & 0.353 &   \\\\\n",
            "\\hline Motion, HR, and Time &\n",
            "0.801 & 0.81 & 0.8 & 0.303 & 0.859  \\\\\n",
            "& 0.885 & 0.704 & 0.9 & 0.425 &   \\\\\n",
            "& 0.904 & 0.608 & 0.93 & 0.442 &   \\\\\n",
            "& 0.916 & 0.52 & 0.95 & 0.441 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.804 & 0.847 & 0.8 & 0.319 & 0.887  \\\\\n",
            "& 0.879 & 0.641 & 0.9 & 0.386 &   \\\\\n",
            "& 0.899 & 0.546 & 0.93 & 0.399 &   \\\\\n",
            "& 0.911 & 0.457 & 0.95 & 0.391 &   \\\\\n",
            "\\hline Motion, HR, and Clock &\n",
            "0.804 & 0.85 & 0.8 & 0.32 & 0.889  \\\\\n",
            "& 0.88 & 0.654 & 0.9 & 0.394 &   \\\\\n",
            "& 0.9 & 0.553 & 0.93 & 0.404 &   \\\\\n",
            "& 0.911 & 0.464 & 0.95 & 0.397 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:k-Neparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of k-Nearest Neighbors with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 1.02 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Neural Net across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion, HR &\n",
            "0.791 & 0.705 & 0.8 & 0.262 & 0.84  \\\\\n",
            "& 0.872 & 0.576 & 0.9 & 0.352 &   \\\\\n",
            "& 0.894 & 0.514 & 0.93 & 0.38 &   \\\\\n",
            "& 0.908 & 0.452 & 0.95 & 0.388 &   \\\\\n",
            "\\hline Motion, HR, and Time &\n",
            "0.803 & 0.836 & 0.8 & 0.322 & 0.892  \\\\\n",
            "& 0.887 & 0.748 & 0.9 & 0.455 &   \\\\\n",
            "& 0.909 & 0.688 & 0.93 & 0.498 &   \\\\\n",
            "& 0.922 & 0.619 & 0.95 & 0.515 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.804 & 0.848 & 0.8 & 0.328 & 0.912  \\\\\n",
            "& 0.887 & 0.753 & 0.9 & 0.458 &   \\\\\n",
            "& 0.909 & 0.687 & 0.93 & 0.497 &   \\\\\n",
            "& 0.921 & 0.615 & 0.95 & 0.512 &   \\\\\n",
            "\\hline Motion, HR, and Clock &\n",
            "0.803 & 0.845 & 0.8 & 0.326 & 0.914  \\\\\n",
            "& 0.887 & 0.759 & 0.9 & 0.461 &   \\\\\n",
            "& 0.911 & 0.705 & 0.93 & 0.509 &   \\\\\n",
            "& 0.924 & 0.642 & 0.95 & 0.531 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Neurparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Neural Net with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 32.02 minutes\n",
            "    Execution time: 35.72 minutes\n",
            "--- [Done] figures_compare_time_based_features() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QwnKIAJvq-B"
      },
      "source": [
        "### [MESA] Sleep-Wake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC_G0rOZvq-B",
        "outputId": "408b6098-836d-405e-b536-915e039d2f3b"
      },
      "source": [
        "figures_mesa_sleep_wake()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_mesa_sleep_wake() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Logistic Regression across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.761 & 0.655 & 0.799 & 0.426 & 0.792  \\\\\n",
            "& 0.796 & 0.507 & 0.9 & 0.437 &   \\\\\n",
            "& 0.799 & 0.439 & 0.93 & 0.418 &   \\\\\n",
            "& 0.798 & 0.378 & 0.95 & 0.388 &   \\\\\n",
            "\\hline HR only &\n",
            "0.694 & 0.403 & 0.8 & 0.206 & 0.68  \\\\\n",
            "& 0.739 & 0.293 & 0.9 & 0.224 &   \\\\\n",
            "& 0.751 & 0.257 & 0.93 & 0.226 &   \\\\\n",
            "& 0.752 & 0.205 & 0.95 & 0.196 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.763 & 0.66 & 0.8 & 0.431 & 0.83  \\\\\n",
            "& 0.796 & 0.507 & 0.9 & 0.437 &   \\\\\n",
            "& 0.8 & 0.441 & 0.93 & 0.419 &   \\\\\n",
            "& 0.798 & 0.377 & 0.95 & 0.387 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.778 & 0.716 & 0.8 & 0.475 & 0.824  \\\\\n",
            "& 0.809 & 0.558 & 0.9 & 0.483 &   \\\\\n",
            "& 0.811 & 0.482 & 0.93 & 0.46 &   \\\\\n",
            "& 0.807 & 0.413 & 0.95 & 0.423 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Logiparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Logistic Regression with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.32 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Random Forest across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.752 & 0.622 & 0.8 & 0.399 & 0.779  \\\\\n",
            "& 0.792 & 0.495 & 0.9 & 0.426 &   \\\\\n",
            "& 0.798 & 0.433 & 0.93 & 0.411 &   \\\\\n",
            "& 0.795 & 0.369 & 0.95 & 0.378 &   \\\\\n",
            "\\hline HR only &\n",
            "0.691 & 0.389 & 0.8 & 0.192 & 0.62  \\\\\n",
            "& 0.73 & 0.263 & 0.898 & 0.189 &   \\\\\n",
            "& 0.736 & 0.202 & 0.929 & 0.162 &   \\\\\n",
            "& 0.739 & 0.159 & 0.95 & 0.139 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.753 & 0.628 & 0.799 & 0.403 & 0.808  \\\\\n",
            "& 0.79 & 0.485 & 0.9 & 0.417 &   \\\\\n",
            "& 0.8 & 0.442 & 0.929 & 0.42 &   \\\\\n",
            "& 0.8 & 0.387 & 0.95 & 0.397 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.768 & 0.681 & 0.8 & 0.448 & 0.82  \\\\\n",
            "& 0.804 & 0.539 & 0.9 & 0.467 &   \\\\\n",
            "& 0.78 & 0.368 & 0.93 & 0.346 &   \\\\\n",
            "& 0.767 & 0.262 & 0.95 & 0.262 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Randparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Random Forest with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.53 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by k-Nearest Neighbors across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.752 & 0.619 & 0.8 & 0.397 & 0.756  \\\\\n",
            "& 0.787 & 0.475 & 0.9 & 0.407 &   \\\\\n",
            "& 0.791 & 0.409 & 0.93 & 0.388 &   \\\\\n",
            "& 0.78 & 0.312 & 0.95 & 0.318 &   \\\\\n",
            "\\hline HR only &\n",
            "0.693 & 0.398 & 0.8 & 0.201 & 0.635  \\\\\n",
            "& 0.728 & 0.255 & 0.9 & 0.182 &   \\\\\n",
            "& 0.736 & 0.2 & 0.93 & 0.161 &   \\\\\n",
            "& 0.739 & 0.157 & 0.95 & 0.138 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.764 & 0.666 & 0.8 & 0.436 & 0.818  \\\\\n",
            "& 0.797 & 0.511 & 0.9 & 0.441 &   \\\\\n",
            "& 0.8 & 0.442 & 0.93 & 0.421 &   \\\\\n",
            "& 0.797 & 0.376 & 0.95 & 0.386 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.764 & 0.664 & 0.8 & 0.434 & 0.816  \\\\\n",
            "& 0.799 & 0.52 & 0.9 & 0.449 &   \\\\\n",
            "& 0.8 & 0.442 & 0.93 & 0.421 &   \\\\\n",
            "& 0.8 & 0.385 & 0.95 & 0.395 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:k-Neparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of k-Nearest Neighbors with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.48 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "\\begin{table}  \\caption{Sleep/wake differentiation performance by Neural Net across different feature inputs in the Apple Watch (PPG, MEMS) dataset} \\begin{tabular}{l*{5}{c}} & Accuracy & Wake correct (specificity) & Sleep correct (sensitivity) & $\\kappa$ & AUC \\\\ \n",
            "\\hline Motion only &\n",
            "0.761 & 0.655 & 0.799 & 0.426 & 0.792  \\\\\n",
            "& 0.796 & 0.507 & 0.9 & 0.437 &   \\\\\n",
            "& 0.799 & 0.439 & 0.93 & 0.418 &   \\\\\n",
            "& 0.798 & 0.378 & 0.95 & 0.388 &   \\\\\n",
            "\\hline HR only &\n",
            "0.694 & 0.403 & 0.8 & 0.206 & 0.68  \\\\\n",
            "& 0.739 & 0.293 & 0.9 & 0.224 &   \\\\\n",
            "& 0.751 & 0.257 & 0.93 & 0.226 &   \\\\\n",
            "& 0.752 & 0.205 & 0.95 & 0.196 &   \\\\\n",
            "\\hline Motion, HR &\n",
            "0.772 & 0.695 & 0.8 & 0.459 & 0.834  \\\\\n",
            "& 0.801 & 0.528 & 0.9 & 0.457 &   \\\\\n",
            "& 0.805 & 0.461 & 0.93 & 0.439 &   \\\\\n",
            "& 0.799 & 0.382 & 0.95 & 0.392 &   \\\\\n",
            "\\hline Motion, HR, and Cosine &\n",
            "0.762 & 0.658 & 0.8 & 0.429 & 0.821  \\\\\n",
            "& 0.787 & 0.475 & 0.9 & 0.407 &   \\\\\n",
            "& 0.79 & 0.403 & 0.93 & 0.382 &   \\\\\n",
            "& 0.776 & 0.295 & 0.95 & 0.299 &   \\\\\n",
            "\\hline \\end{tabular}  \\label{tab:Neurparams} \\small \\vspace{.2cm} \\caption*{Fraction of wake correct, fraction of sleep correct, accuracy, $\\kappa$, and AUC for sleep-wake predictions of Neural Net with use of motion, HR, clock proxy, or combination of features. PPG, photoplethysmography; MEMS, microelectromechanical systems; HR, heart rate; AUC, area under the curve.} \\end{table}\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 2.77 minutes\n",
            "    Execution time: 4.11 minutes\n",
            "--- [Done] figures_mesa_sleep_wake() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DlqONBFvq-C"
      },
      "source": [
        "### [MESA] Three-Class (REM/NREM/Wake)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAjTOC78vq-C",
        "outputId": "fe73f6b0-db90-4a69-8733-e168378f5f92"
      },
      "source": [
        "figures_mesa_three_class()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ [Start] figures_mesa_three_class() +++\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.41 minutes\n",
            "|---- [ Random Forest ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.66 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.57 minutes\n",
            "|---- [ Neural Net ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 4.32 minutes\n",
            "|---- [ Logistic Regression ] ----|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/nothing2say/Documents/Programme/Anaconda/anaconda3/envs/Sleep_Classifier/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "____(: Logistic Regression :) took 0.43 minutes\n",
            "|---- [ Random Forest ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: Random Forest :) took 0.64 minutes\n",
            "|---- [ k-Nearest Neighbors ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: k-Nearest Neighbors :) took 0.6 minutes\n",
            "|---- [ Neural Net ] ----|\n",
            "----------------------------------------------------------------------------\n",
            "____(: Neural Net :) took 4.05 minutes\n",
            "\\begin{tabular}{l | l | c | c | c | c | c } & & Wake Correct & NREM Correct & REM Correct & Best accuracy & $\\kappa$ \\\n",
            "\\hline Logistic Regression & Motion only & \n",
            "0.6 & 0.286 & 0.545 & 0.672 & 0.3\\\\\n",
            " & HR only & \n",
            "0.6 & 0.387 & 0.38 & 0.633 & 0.165\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.485 & 0.494 & 0.687 & 0.337\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.43 & 0.423 & 0.681 & 0.321\\\\\n",
            "\\hline Random Forest & Motion only & \n",
            "0.6 & 0.692 & 0.139 & 0.664 & 0.281\\\\\n",
            " & HR only & \n",
            "0.6 & 0.456 & 0.164 & 0.607 & 0.093\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.431 & 0.441 & 0.651 & 0.294\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.457 & 0.452 & 0.622 & 0.305\\\\\n",
            "\\hline k-Nearest Neighbors & Motion only & \n",
            "0.6 & 0.661 & 0.172 & 0.653 & 0.272\\\\\n",
            " & HR only & \n",
            "0.6 & 0.272 & 0.27 & 0.604 & 0.096\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.448 & 0.446 & 0.672 & 0.308\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.453 & 0.449 & 0.676 & 0.314\\\\\n",
            "\\hline Neural Net & Motion only & \n",
            "0.6 & 0.0 & 0.863 & 0.672 & 0.3\\\\\n",
            " & HR only & \n",
            "0.6 & 0.386 & 0.385 & 0.633 & 0.165\\\\\n",
            " & Motion, HR & \n",
            "0.6 & 0.428 & 0.421 & 0.676 & 0.295\\\\\n",
            " & Motion, HR, and Cosine & \n",
            "0.6 & 0.437 & 0.43 & 0.663 & 0.278\\\\\n",
            "\\end{tabular}  \\label{tab:REM_params} \\small \\vspace{.2cm}\n",
            "    Execution time: 11.68 minutes\n",
            "--- [Done] figures_mesa_three_class() ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWN1Yk6nvq-C"
      },
      "source": [
        "### Run ALL Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wyur_Wevq-C",
        "outputId": "a1d40ac2-e7ff-4b20-a27b-277e0361d6b8"
      },
      "source": [
        "#run_all_analysis()\n",
        "\n",
        "print(\"........ Well Done ........\")\n",
        "end_time_all = time.time()\n",
        "execution_time = str(round(((end_time_all - start_time_all) / 60), 2))\n",
        "print(\"    Execution time: \" + execution_time + \" minutes\")\n",
        "print(\"___ Finished Analysis_Runner ___\")\n",
        "### --- END START ANALYSIS ---"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "........ Well Done ........\n",
            "    Execution time: 111.64 minutes\n",
            "___ Finished Analysis_Runner ___\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}